#!/usr/bin/env python3
"""
Unified MacauSlot + Titan007 + HKJC matcher + AI analysis (patched with debug instrumentation).
Includes HKJC All Odds Collection.

Notes:
- Integrates debug_instrumentation (save rendered HTML, parsed JSON, mapping events).
- Merges Titan 'sections' into normalized data so AI sees scraped sections.
- Titan analysis pages wait longer before parsing stats to reduce false "no stats" cases.
- Scrapes detailed HKJC odds (All Odds) for matched games.

Usage:
  export DEEPSEEK_API_KEY="sk-..."
  export DEEPSEEK_API_URL="https://api.deepseek.com/chat/completions"
  python3 macau_titan_hkjc_matcher.py
"""

import asyncio
import os
import re
import json
import logging
from datetime import datetime
from difflib import SequenceMatcher
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any

# HTTP client (async)
import httpx

# Playwright (async)
from playwright.async_api import async_playwright

# HTML parsing
from bs4 import BeautifulSoup, Tag

# Data handling
import pandas as pd

# Debug instrumentation
try:
    from debug_instrumentation import (
        init_debug_session,
        save_rendered_html,
        save_parsed_json,
        log_mapping_decision,
        log_info
    )

    DEBUG_INSTRUMENTATION_AVAILABLE = True
except Exception:
    DEBUG_INSTRUMENTATION_AVAILABLE = False

# Color output
try:
    from colorama import init as _init_colorama, Fore, Style

    _init_colorama(autoreset=True)
    COLORS_AVAILABLE = True
except Exception:
    COLORS_AVAILABLE = False


    class _Dummy:
        GREEN = RED = YELLOW = BLUE = MAGENTA = CYAN = WHITE = ''
        BRIGHT = NORMAL = ''


    Fore = _Dummy()
    Style = _Dummy()

# Logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("matcher")

# --- Configuration ---
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_API_URL = os.getenv("DEEPSEEK_API_URL", "https://api.deepseek.com/chat/completions")
DEEPSEEK_TIMEOUT = float(os.getenv("DEEPSEEK_TIMEOUT", "30"))
DEEPSEEK_RETRIES = int(os.getenv("DEEPSEEK_RETRIES", "3"))

if not DEEPSEEK_API_KEY:
    logger.warning("DEEPSEEK_API_KEY not set. AI functionality will fail unless you set the env var.")


def cprint(text: str, color: str = '', style: str = ''):
    if COLORS_AVAILABLE:
        print(f"{style}{color}{text}{Style.RESET_ALL}")
    else:
        print(text)


# -----------------------
# HKJC Constants & Helpers
# -----------------------
DISPLAY_ZH = {
    "HAD": "ä¸»å®¢å’Œ", "FHA": "åŠå ´ä¸»å®¢å’Œ", "HHA": "è®“çƒä¸»å®¢å’Œ", "HHA_Extra": "è®“çƒä¸»å®¢å’Œ",
    "HDC": "è®“çƒ", "HIL": "å…¥çƒå¤§ç´°", "FHL": "åŠå ´å…¥çƒå¤§ç´°",
    "CHL": "é–‹å‡ºè§’çƒå¤§ç´°", "FCH": "åŠå ´é–‹å‡ºè§’çƒå¤§ç´°",
    "CHD": "é–‹å‡ºè§’çƒè®“çƒ", "FHC": "åŠå ´é–‹å‡ºè§’çƒè®“çƒ",
    "CRS": "æ³¢è†½", "FCS": "åŠå ´æ³¢è†½", "FTS": "ç¬¬ä¸€éšŠå…¥çƒ",
    "TTG": "ç¸½å…¥çƒ", "OOE": "å…¥çƒå–®é›™", "HFT": "åŠå…¨å ´",
    "FGS": "é¦–åå…¥çƒ", "LGS": "æœ€å¾Œå…¥çƒçƒå“¡", "AGS": "ä»»ä½•æ™‚é–“å…¥çƒçƒå“¡",
    "MSP": "ç‰¹åˆ¥é …ç›®",
}


def _clean_text(el):
    return el.get_text(strip=True) if el else None


def _clean_odds_text(span):
    if not span:
        return None
    text = span.get_text(strip=True)
    cleaned = re.sub(r"[^\d.]", "", text)
    return float(cleaned) if cleaned else None


def _next_match_row_container(coupon):
    if not coupon:
        return None
    sib = coupon.find_next_sibling()
    while sib is not None and not ("match-row-container" in sib.get("class", [])):
        sib = sib.find_next_sibling()
    return sib


def _looks_like_name(txt: str):
    if not txt:
        return False
    txt = txt.strip()
    if re.fullmatch(r"[\d\.\-/:]+", txt):
        return False
    return bool(re.search(r"[A-Za-z\u4e00-\u9fff]", txt))


# -----------------------
# HKJC Market Parsers
# -----------------------
def parse_allodds_match_header(soup):
    mi = soup.select_one(".match-info")
    if not mi:
        return {}
    match_id = _clean_text(mi.select_one(".match .val"))
    home = _clean_text(mi.select_one(".team .home"))
    away = _clean_text(mi.select_one(".team .away"))
    time_raw = _clean_text(mi.select_one(".time .val"))
    timg = mi.select_one(".matchInfoTourn img")
    tournament = timg["title"] if timg and timg.has_attr("title") else None
    return {
        "match_id": match_id,
        "home_team": home,
        "away_team": away,
        "tournament": tournament,
        "time_raw": time_raw,
    }


def _parse_had_like_from_row(row, odds_class):
    odds_block = row.select_one(f".odds.{odds_class}")
    if not odds_block: return None
    grids = odds_block.select(".oddsCheckboxGrid")
    if len(grids) < 3: return None
    co = _clean_odds_text
    return {
        "home_odds": co(grids[0].select_one(".add-to-slip")),
        "draw_odds": co(grids[1].select_one(".add-to-slip")),
        "away_odds": co(grids[2].select_one(".add-to-slip")),
    }


def _parse_hha_multi_lines_from_row(row):
    res = []
    odds_line = row.select_one(".oddsLine.HHA")
    if not odds_line: return res
    line_blocks = odds_line.select(".odds.show")
    for line_index, line_block in enumerate(line_blocks):
        items = line_block.select(".hdcOddsItem")
        if len(items) != 3: continue

        def cond(item):
            c = item.select_one(".cond")
            return _clean_text(c).strip("[]") if c else ""

        home_hcap = cond(items[0]);
        draw_hcap = cond(items[1]);
        away_hcap = cond(items[2])
        co = _clean_odds_text
        home_odds = co(items[0].select_one(".add-to-slip"))
        draw_odds = co(items[1].select_one(".add-to-slip"))
        away_odds = co(items[2].select_one(".add-to-slip"))
        give_home = None
        if home_hcap.startswith("-"):
            give_home = True
        elif away_hcap.startswith("-"):
            give_home = False
        else:
            if home_odds is not None and away_odds is not None:
                give_home = home_odds < away_odds
        market_type = "HHA" if line_index == 0 else "HHA_Extra"
        res.append({
            "market_type": market_type,
            "line_index": line_index + 1,
            "home_odds": home_odds, "draw_odds": draw_odds, "away_odds": away_odds,
            "euro_handicap_value": home_hcap,
            "euro_handicap_give_home": give_home,
        })
    return res


def _parse_hdc_from_row(row):
    odds_line = row.select_one(".oddsLine.HDC")
    if not odds_line: return None
    lb = odds_line.select_one(".odds.show")
    if not lb: return None
    items = lb.select(".hdcOddsItem")
    if len(items) != 2: return None

    def cond(item):
        c = item.select_one(".cond")
        return _clean_text(c).strip("[]") if c else ""

    home_hcap = cond(items[0]);
    away_hcap = cond(items[1])
    co = _clean_odds_text
    home_odds = co(items[0].select_one(".add-to-slip"))
    away_odds = co(items[1].select_one(".add-to-slip"))
    give_home = None
    if home_hcap.startswith("-"):
        give_home = True
    elif away_hcap.startswith("-"):
        give_home = False
    else:
        if home_odds is not None and away_odds is not None:
            give_home = home_odds < away_odds
    return {
        "asia_handicap_value": home_hcap,
        "asia_handicap_give_home": give_home,
        "home_odds": home_odds, "away_odds": away_odds,
    }


def _parse_ou_market_from_row(row, class_name, goal_field_name="goal_line"):
    odds_line = row.select_one(f".oddsLine.{class_name}")
    if not odds_line: return []
    res = []
    line_nums = odds_line.select(".lineNum.show")
    odds_blocks = odds_line.select(".odds.show")
    for line_idx, (ln, ob) in enumerate(zip(line_nums, odds_blocks)):
        line_text = _clean_text(ln).strip("[]") if ln else None
        grids = ob.select(".oddsCheckboxGrid")
        if len(grids) < 2: continue
        co = _clean_odds_text
        over_odds = co(grids[0].select_one(".add-to-slip"))
        under_odds = co(grids[1].select_one(".add-to-slip"))
        res.append({
            "line_index": line_idx + 1,
            goal_field_name: line_text,
            "over_odds": over_odds,
            "under_odds": under_odds,
        })
    return res


def _parse_crs_matrix(row):
    res = []
    for odds_cell in row.select(".crsTable .odds"):
        score = _clean_text(odds_cell.select_one(".crsSel"))
        odds = _clean_odds_text(odds_cell.select_one(".add-to-slip"))
        if score and odds is not None:
            res.append({"score": score, "odds": odds})
    return res


def _parse_fts(row):
    odds_block = row.select_one(".oddsFTS")
    if not odds_block: return None
    grids = odds_block.select(".oddsCheckboxGrid")
    if len(grids) < 3: return None
    co = _clean_odds_text
    return {
        "home_first": co(grids[0].select_one(".add-to-slip")),
        "no_goal": co(grids[1].select_one(".add-to-slip")),
        "away_first": co(grids[2].select_one(".add-to-slip")),
    }


def _parse_ttg(row):
    odds_block = row.select_one(".oddsTTG")
    if not odds_block: return []
    res = []
    for block in odds_block.find_all("div", recursive=False):
        goals = _clean_text(block.select_one(".goals-number"))
        odds = _clean_odds_text(block.select_one(".add-to-slip"))
        if goals and odds is not None:
            res.append({"goals": goals, "odds": odds})
    return res


def _parse_ooe(row):
    odds_block = row.select_one(".oddsOOE")
    if not odds_block: return None
    grids = odds_block.select(".oddsCheckboxGrid")
    if len(grids) < 2: return None
    co = _clean_odds_text
    return {
        "odd": co(grids[0].select_one(".add-to-slip")),
        "even": co(grids[1].select_one(".add-to-slip")),
    }


def _parse_hft(row):
    odds_block = row.select_one(".oddsHFT")
    if not odds_block: return []
    res = []
    for block in odds_block.find_all("div", recursive=False):
        label = _clean_text(block.select_one(".goals-number"))
        odds = _clean_odds_text(block.select_one(".add-to-slip"))
        if label and odds is not None:
            res.append({"combo": label, "odds": odds})
    return res


def _parse_scorer_market(row):
    res = []
    grids = row.select(".oddsCheckboxGrid")

    def pull_candidates(grid):
        cands = []
        for sib in grid.previous_siblings:
            if getattr(sib, "get_text", None):
                cands.append(_clean_text(sib))
        parent = grid.find_parent(["td", "div"])
        if parent:
            for sib in parent.find_previous_siblings():
                if getattr(sib, "get_text", None):
                    cands.append(_clean_text(sib))
        for prev in grid.find_all_previous(["div", "td", "th", "span"], limit=8):
            txt = _clean_text(prev)
            if txt:
                cands.append(txt)
        return cands

    for g in grids:
        odds = _clean_odds_text(g.find_next("span", class_="add-to-slip"))
        gid = g.get("id", "") or ""
        m = re.search(r"_(\d{3,})", gid)
        code = m.group(1) if m else None
        name = None
        for txt in pull_candidates(g):
            if not txt: continue
            m2 = re.search(r"\b(\d{3})\b\s*([A-Za-z\u4e00-\u9fff].+)", txt)
            if m2:
                if not code: code = m2.group(1)
                name = m2.group(2).strip()
                break
        res.append({"player_code": code, "player_name": name, "odds": odds})
    return res


def _parse_msp(row):
    raw = row.get_text(" ", strip=True)
    if not raw: return []
    items = []
    parts = re.split(r"é …ç›®ç·¨è™Ÿ[:ï¼š]\s*", raw)
    for part in parts:
        part = part.strip()
        if not part: continue
        m_id = re.match(r"(\d+)", part)
        item_id = m_id.group(1) if m_id else None
        m_q = re.split(r"\(\d+\)", part, maxsplit=1)
        if len(m_q) == 2:
            question = m_q[0].strip()
            rest = "(" + m_q[1]
        else:
            question = None
            rest = part
        options = []
        for opt_num, label, odds in re.findall(r"\((\d+)\)\s*([^(]+?)\s+(\d+(?:\.\d+)?)", rest):
            options.append({"option": opt_num, "label": label.strip(), "odds": float(odds)})
        items.append({"item_id": item_id, "question": question, "options": options, "raw": part})
    if not items:
        odds_list = [_clean_odds_text(span) for span in row.select(".add-to-slip")]
        items.append({"raw": raw, "odds": [o for o in odds_list if o is not None]})
    return items


def parse_allodds_from_html(html: str):
    soup = BeautifulSoup(html, "html.parser")
    match_meta = parse_allodds_match_header(soup)
    markets = {}

    def add_display(code, obj):
        if obj is None: return None
        if isinstance(obj, dict): obj["display_name_zh"] = DISPLAY_ZH.get(code, "")
        return obj

    def add_display_list(code, lst):
        if lst is None: return None
        return [item | {"display_name_zh": DISPLAY_ZH.get(code, "")} for item in lst]

    for code, cls, parser in [
        ("HAD", "couponHAD", lambda r: _parse_had_like_from_row(r, "oddsHAD")),
        ("FHA", "couponFHA", lambda r: _parse_had_like_from_row(r, "oddsFHA")),
        ("HDC", "couponHDC", _parse_hdc_from_row),
        ("CHD", "couponCHD", _parse_hdc_from_row),
        ("FHC", "couponFHC", _parse_hdc_from_row),
    ]:
        coupon = soup.select_one(f".coupon.{cls}")
        if coupon:
            row = _next_match_row_container(coupon)
            if row: markets[code] = add_display(code, parser(row))

    coupon = soup.select_one(".coupon.couponHHA")
    if coupon:
        row = _next_match_row_container(coupon)
        if row:
            hha_lines = _parse_hha_multi_lines_from_row(row)
            for line in hha_lines:
                line["display_name_zh"] = DISPLAY_ZH.get(line["market_type"], "")
                markets.setdefault(line["market_type"], []).append(line)

    for code, cls in [("HIL", "couponHIL"), ("FHL", "couponFHL"), ("CHL", "couponCHL"), ("FCH", "couponFCH")]:
        coupon = soup.select_one(f".coupon.{cls}")
        if coupon:
            row = _next_match_row_container(coupon)
            if row: markets[code] = add_display_list(code, _parse_ou_market_from_row(row, code if code in ("HIL",
                                                                                                           "FHL") else code))

    for code, cls in [("CRS", "couponCRS"), ("FCS", "couponFCS")]:
        coupon = soup.select_one(f".coupon.{cls}")
        if coupon:
            row = _next_match_row_container(coupon)
            if row: markets[code] = {"display_name_zh": DISPLAY_ZH[code], "scores": _parse_crs_matrix(row)}

    coupon = soup.select_one(".coupon.couponFTS")
    if coupon:
        row = _next_match_row_container(coupon)
        if row: markets["FTS"] = add_display("FTS", _parse_fts(row))
    coupon = soup.select_one(".coupon.couponTTG")
    if coupon:
        row = _next_match_row_container(coupon)
        if row: markets["TTG"] = {"display_name_zh": DISPLAY_ZH["TTG"], "buckets": _parse_ttg(row)}
    coupon = soup.select_one(".coupon.couponOOE")
    if coupon:
        row = _next_match_row_container(coupon)
        if row: markets["OOE"] = add_display("OOE", _parse_ooe(row))
    coupon = soup.select_one(".coupon.couponHFT")
    if coupon:
        row = _next_match_row_container(coupon)
        if row: markets["HFT"] = {"display_name_zh": DISPLAY_ZH["HFT"], "combos": _parse_hft(row)}

    for code, cls in [("FGS", "couponFGS"), ("LGS", "couponLGS"), ("AGS", "couponAGS")]:
        coupon = soup.select_one(f".coupon.{cls}")
        if coupon:
            row = _next_match_row_container(coupon)
            if row: markets[code] = {"display_name_zh": DISPLAY_ZH[code], "players": _parse_scorer_market(row)}

    coupon = soup.select_one(".coupon.couponMSP")
    if coupon:
        row = _next_match_row_container(coupon)
        if row: markets["MSP"] = {"display_name_zh": DISPLAY_ZH["MSP"], "items": _parse_msp(row)}

    return match_meta, markets


# -----------------------
# Robust numeric helpers
# -----------------------
def find_best_float_in_text(text: str, min_val: float = -1e9, max_val: float = 1e9) -> Optional[float]:
    if not text:
        return None
    tokens = re.findall(r'\d+\.\d+|\d+', text)
    for t in tokens:
        try:
            v = float(t)
        except ValueError:
            continue
        if min_val <= v <= max_val:
            return v
    return None


def parse_decimal_tokens_from_concatenated(text: str) -> List[float]:
    if not text:
        return []
    tokens = re.findall(r'\d{1,2}\.\d{1,2}', text)
    floats = []
    for t in tokens:
        try:
            v = float(t)
            if 0.0 <= v <= 10.0:
                floats.append(v)
        except ValueError:
            continue
    return floats


def extract_ratings_or_average_from_text(page_text: str) -> Tuple[Optional[float], List[float]]:
    if not page_text:
        return None, []
    m = re.search(r'å¹³å‡è©•åˆ†[:ï¼š]?\s*([0-9]{1,2}\.[0-9]{1,2})', page_text)
    if m:
        try:
            val = float(m.group(1))
            if 0.0 <= val <= 10.0:
                return val, [val]
        except Exception:
            pass
    m2 = re.search(r'(?:ä¸»éšŠ|å®¢éšŠ)?è¿‘10å ´å¹³å‡è©•åˆ†[:ï¼š]?\s*([0-9\.\s]{5,200})', page_text)
    if m2:
        snippet = m2.group(1)
        parsed = parse_decimal_tokens_from_concatenated(snippet)
        if parsed:
            avg = sum(parsed) / len(parsed)
            return avg, parsed
    all_decimals = parse_decimal_tokens_from_concatenated(page_text)
    if all_decimals:
        chosen = all_decimals[:10]
        avg = sum(chosen) / len(chosen)
        return avg, chosen
    return None, []


# -----------------------
# DeepSeek async client
# -----------------------
async def call_deepseek_api_async(prompt: str, timeout: int = None, max_retries: int = None) -> str:
    if timeout is None:
        timeout = int(DEEPSEEK_TIMEOUT)
    if max_retries is None:
        max_retries = int(DEEPSEEK_RETRIES)

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}" if DEEPSEEK_API_KEY else ""
    }
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7,
        "max_tokens": 1000
    }

    backoff_base = 0.6
    last_err = None
    async with httpx.AsyncClient(timeout=timeout) as client:
        for attempt in range(1, max_retries + 1):
            try:
                resp = await client.post(DEEPSEEK_API_URL, headers=headers, json=payload)
                resp.raise_for_status()
                data = resp.json()
                if isinstance(data, dict):
                    choices = data.get("choices")
                    if choices and isinstance(choices, list) and len(choices) > 0:
                        first = choices[0]
                        if isinstance(first, dict):
                            msg = first.get("message") or first.get("text") or {}
                            if isinstance(msg, dict):
                                content = msg.get("content") or msg.get("text") or ""
                            elif isinstance(msg, str):
                                content = msg
                            else:
                                content = ""
                        else:
                            content = str(first)
                        return content.strip()
                return resp.text
            except httpx.HTTPStatusError as e:
                last_err = str(e)
                status = getattr(e.response, "status_code", None)
                logger.error("DeepSeek HTTP error (attempt %d): %s", attempt, e)
                if status and status < 500 and status != 429:
                    break
            except Exception as e:
                last_err = str(e)
                logger.error("DeepSeek request failed (attempt %d): %s", attempt, e)
            await asyncio.sleep(backoff_base * attempt)
    raise RuntimeError(f"DeepSeek API calls failed: {last_err}")


# -----------------------
# AI integration helpers (with merge of sections)
# -----------------------
EXPECTED_TOP_LEVEL_SECTIONS = [
    "match", "league_standings", "data_comparison_recent10", "lineup_and_injuries",
    "last_match_player_ratings", "recent10_ratings_parsed", "future_matches",
    "head_to_head_sample", "league_trend_and_other_stats"
]


def normalize_parsed_data(parsed: Dict[str, Any]) -> Dict[str, Any]:
    merged = dict(parsed)
    if isinstance(parsed.get("sections"), dict):
        for k, v in parsed["sections"].items():
            if k not in merged or merged.get(k) is None:
                merged[k] = v

    normalized: Dict[str, Any] = {}
    missing: List[str] = []
    available: List[str] = []

    for key in EXPECTED_TOP_LEVEL_SECTIONS:
        val = merged.get(key)
        if val is None:
            normalized[key] = None
            missing.append(key)
        else:
            normalized[key] = val
            available.append(key)

    match_block = merged.get("match") or {}
    normalized["match"] = {
        "home_team": match_block.get("home_team") or merged.get("home_team"),
        "away_team": match_block.get("away_team") or merged.get("away_team"),
        "competition": match_block.get("competition") or merged.get("competition"),
        "datetime": match_block.get("datetime") or merged.get("datetime"),
        "venue": match_block.get("venue") or merged.get("venue"),
    }

    rr = merged.get("recent10_ratings_parsed") or {}
    normalized["recent10_ratings_parsed"] = {
        "home_recent_ratings_raw": rr.get("home_recent_ratings_raw"),
        "home_recent_ratings": list(rr.get("home_recent_ratings") or []),
        "home_recent_average": rr.get("home_recent_average") or merged.get("home_rating"),
        "away_recent_ratings_raw": rr.get("away_recent_ratings_raw"),
        "away_recent_ratings": list(rr.get("away_recent_ratings") or []),
        "away_recent_average": rr.get("away_recent_average") or merged.get("away_rating"),
    }

    section_counts = {}
    for key in EXPECTED_TOP_LEVEL_SECTIONS:
        v = normalized.get(key)
        if isinstance(v, list):
            section_counts[key] = len(v)
        elif isinstance(v, dict):
            section_counts[key] = len(v.keys())
        elif v is None:
            section_counts[key] = 0
        else:
            section_counts[key] = 1

    normalized["_meta"] = {
        "missing_fields": missing,
        "available_sections": available,
        "section_counts": section_counts
    }
    return normalized


def has_meaningful_data_for_ai(normalized_stats: Dict[str, Any]) -> bool:
    meta = normalized_stats.get("_meta", {})
    available = meta.get("available_sections", [])
    if available:
        return True
    rr = normalized_stats.get("recent10_ratings_parsed") or {}
    if rr.get("home_recent_ratings") or rr.get("away_recent_ratings") or rr.get("home_recent_average") or rr.get(
            "away_recent_average"):
        return True
    return False


def build_ai_prompt_with_availability(normalized_data: Dict[str, Any], use_chinese: bool = True) -> str:
    meta = normalized_data.get("_meta", {})
    available = meta.get("available_sections", [])
    missing = meta.get("missing_fields", [])

    def render_section(name: str, max_chars: int = 800) -> str:
        sec = normalized_data.get(name)
        if not sec:
            return ""
        try:
            s = json.dumps(sec, ensure_ascii=False, indent=0)
        except Exception:
            s = str(sec)
        return s[:max_chars] + ("..." if len(s) > max_chars else "")

    available_str = ", ".join(available) if available else "none"
    missing_str = ", ".join(missing) if missing else "none"

    excerpts = ""
    for sec_name in ("recent10_ratings_parsed", "league_standings", "data_comparison_recent10",
                     "last_match_player_ratings", "lineup_and_injuries"):
        excerpt = render_section(sec_name)
        if excerpt:
            excerpts += f"\n\n=== {sec_name} ===\n{excerpt}"

    if use_chinese:
        header = (
            f"ä½ æ˜¯ä¸€ä½è³‡æ·±è¶³çƒåšå½©åˆ†æå¸«ã€‚ä»¥ä¸‹è³‡æ–™å·²ç”±çˆ¬èŸ²è§£æå’Œæ¨™æº–åŒ–ï¼Œç³»çµ±èªªæ˜å“ªäº›æ¬„ä½å­˜åœ¨æˆ–éºæ¼ã€‚\n"
            f"Available sections: {available_str}\n"
            f"Missing sections: {missing_str}\n\n"
            "è«‹åŸºæ–¼å¯ç”¨æ•¸æ“šï¼ˆè‹¥æŸäº›æ¬„ä½éºå¤±ï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºï¼‰æ¨è–¦ä¸€å€‹æœ€æœ‰åƒ¹å€¼çš„æŠ•æ³¨é¸é …ï¼Œä¸¦ä»¥JSONæ ¼å¼å›è¦†ï¼Œ"
            "åƒ…åŒ…å«å¦‚ä¸‹å­—æ®µï¼š\n"
            '{ "best_bet_market": "æŠ•æ³¨å¸‚å ´", "best_bet_selection": "å…·é«”é¸æ“‡", "confidence_level": "1-10", "brief_reasoning": "ç°¡çŸ­åŸå› " }\n\n'
            "åªè¼¸å‡ºJSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"
        )
    else:
        header = (
            f"You are an experienced football betting analyst. The parser produced the following available/missing sections.\n"
            f"Available: {available_str}\nMissing: {missing_str}\n\n"
            "Based on available data (explicitly note missing fields if they matter), recommend a single best bet in JSON:\n"
            '{ "best_bet_market": "...", "best_bet_selection": "...", "confidence_level": "1-10", "brief_reasoning": "..." }\n'
            "Output only JSON."
        )

    prompt = header + excerpts
    return prompt


def parse_ai_json_response(text: str) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
    if not text:
        return None, None
    try:
        s = text.strip()
        if s.startswith("{") and s.endswith("}"):
            return json.loads(s), s
        m = re.search(r'(\{(?:.|\s)*\})', text)
        if m:
            candidate = m.group(1)
            return json.loads(candidate), candidate
    except Exception as e:
        logger.debug("AI JSON parse failed: %s", e)
    try:
        fallback = {}
        market = re.search(r'"?best_bet_market"?\s*[:=]\s*"([^"]+)"', text)
        selection = re.search(r'"?best_bet_selection"?\s*[:=]\s*"([^"]+)"', text)
        confidence = re.search(r'"?confidence_level"?\s*[:=]\s*([0-9]+)', text)
        reasoning = re.search(r'"?brief_reasoning"?\s*[:=]\s*"([^"]+)"', text)
        if market:
            fallback["best_bet_market"] = market.group(1)
        if selection:
            fallback["best_bet_selection"] = selection.group(1)
        if confidence:
            fallback["confidence_level"] = int(confidence.group(1))
        if reasoning:
            fallback["brief_reasoning"] = reasoning.group(1)
        if fallback:
            return fallback, json.dumps(fallback, ensure_ascii=False)
    except Exception:
        pass
    return None, None


async def perform_ai_analysis_for_match_async(
        normalized_stats: Dict[str, Any],
        call_deepseek_api_async_fn,
        max_retries: int = 2,
        short_circuit_when_no_data: bool = True
) -> Dict[str, Any]:
    result = {
        "best_bet_market": "No Data",
        "best_bet_selection": "No Analysis Available",
        "confidence_level": 0,
        "brief_reasoning": "Insufficient statistical data available for analysis.",
        "ai_raw_response": None,
        "ai_parsed_json": None,
        "data_availability": normalized_stats.get("_meta", {})
    }

    if short_circuit_when_no_data and not has_meaningful_data_for_ai(normalized_stats):
        logger.info("Short-circuiting AI call: no meaningful sections or ratings present")
        return result

    prompt = build_ai_prompt_with_availability(normalized_stats, use_chinese=True)
    if DEBUG_INSTRUMENTATION_AVAILABLE:
        try:
            save_parsed_json("ai_prompt", normalized_stats.get("match", {}).get("home_team", "unknown"),
                             {"prompt": prompt[:4000]})
            log_info("AI prompt built", {"available_sections": normalized_stats.get("_meta", {})})
        except Exception:
            pass

    last_err = None
    for attempt in range(1, max_retries + 1):
        try:
            ai_text = await call_deepseek_api_async_fn(prompt)
            result["ai_raw_response"] = ai_text
            if DEBUG_INSTRUMENTATION_AVAILABLE:
                try:
                    save_parsed_json("ai_response_raw", normalized_stats.get("match", {}).get("home_team", "unknown"),
                                     {"raw": ai_text[:4000]})
                except Exception:
                    pass
            parsed, raw = parse_ai_json_response(ai_text)
            if parsed:
                required = ["best_bet_market", "best_bet_selection", "confidence_level", "brief_reasoning"]
                if all(k in parsed for k in required):
                    result.update({
                        "best_bet_market": parsed.get("best_bet_market"),
                        "best_bet_selection": parsed.get("best_bet_selection"),
                        "confidence_level": parsed.get("confidence_level"),
                        "brief_reasoning": parsed.get("brief_reasoning"),
                        "ai_parsed_json": parsed
                    })
                    return result
                else:
                    result["ai_parsed_json"] = parsed
                    result["brief_reasoning"] = "AI returned partial result; missing keys"
                    result["confidence_level"] = parsed.get("confidence_level", 1)
                    return result
            logger.warning("AI response contained no parsable JSON (attempt %d).", attempt)
            last_err = "No JSON in response"
            await asyncio.sleep(0.6 * attempt)
        except Exception as e:
            last_err = str(e)
            logger.error("AI call attempt %d failed: %s", attempt, e)
            await asyncio.sleep(0.6 * attempt)
    result["brief_reasoning"] = f"AI call failed: {last_err}"
    result["confidence_level"] = 0
    return result


# -----------------------
# Titan007 stats scraper (improved + more patient + debug)
# -----------------------
async def scrape_match_stats_from_analysis_page(titan_match_id: str) -> Dict[str, Any]:
    url = f"https://zq.titan007.com/analysis/{titan_match_id}.htm"
    logger.info("ğŸ” Scraping analysis stats for Titan match ID: %s", titan_match_id)
    async with async_playwright() as playwright:
        browser = await playwright.chromium.launch(headless=True)
        page = await browser.new_page()
        try:
            await page.goto(url, wait_until='networkidle', timeout=60000)
            await asyncio.sleep(3.0)
            try:
                await page.wait_for_selector("table", timeout=3000)
            except Exception:
                pass
            content = await page.content()
            if DEBUG_INSTRUMENTATION_AVAILABLE:
                try:
                    save_rendered_html("titan_analysis", titan_match_id, content)
                except Exception:
                    pass
            soup = BeautifulSoup(content, "html.parser")
            data = {
                "match_id": titan_match_id,
                "scraped_at": datetime.now().isoformat(),
                "url": url,
                "sections": {},
                "stats_available": False
            }
            page_text = soup.get_text(separator=' ', strip=True)
            no_data_patterns = ["æš«ç„¡æ•¸æ“š", "æ•¸æ“šçµ±è¨ˆä¸­", "æœªé–‹å§‹", "è³‡æ–™æº–å‚™ä¸­", "å°šç„¡ç›¸é—œè³‡æ–™", "No data"]
            for pat in no_data_patterns:
                if pat in page_text:
                    logger.warning("âš ï¸ Titan %s: '%s' found â€” no stats available", titan_match_id, pat)
                    data["error"] = f"No stats: {pat}"
                    if DEBUG_INSTRUMENTATION_AVAILABLE:
                        try:
                            save_parsed_json("titan_analysis_parsed", titan_match_id, data)
                        except Exception:
                            pass
                    return data

            def extract_section_by_regex(regex: str) -> Optional[Any]:
                header = soup.find(string=re.compile(regex))
                if not header:
                    return None
                parent = None
                try:
                    if isinstance(header, Tag):
                        parent = header.find_parent()
                    else:
                        parent = header.parent if hasattr(header, "parent") else None
                except Exception:
                    parent = header.parent if hasattr(header, "parent") else None
                if not parent:
                    parent = header.parent if hasattr(header, "parent") else None
                if not parent:
                    return None
                tables = parent.find_all('table')
                for table in tables:
                    parsed = extract_table_data(table)
                    if parsed:
                        return parsed
                txt = parent.get_text(separator=' | ', strip=True)
                if txt and len(txt) > 30:
                    return [{"text_content": txt}]
                return None

            sections_to_try = [
                ("league_standings", r'è¯è³½ç©åˆ†æ’å'),
                ("head_to_head", r'å°è³½å¾€ç¸¾'),
                ("data_comparison", r'æ•¸æ“šå°æ¯”'),
                ("referee_stats", r'è£åˆ¤çµ±è¨ˆ'),
                ("league_trend", r'è¯è³½ç›¤è·¯èµ°å‹¢'),
                ("same_trend", r'ç›¸åŒç›¤è·¯'),
                ("goal_distribution", r'å…¥çƒæ•¸/ä¸Šä¸‹åŠå ´å…¥çƒåˆ†å¸ƒ'),
                ("halftime_fulltime", r'åŠå…¨å ´'),
                ("goal_count", r'é€²çƒæ•¸/å–®é›™'),
                ("goal_time", r'é€²çƒæ™‚é–“'),
                ("future_matches", r'æœªä¾†äº”å ´'),
                ("pre_match_brief", r'è³½å‰ç°¡å ±'),
                ("season_stats_comparison", r'æœ¬è³½å­£æ•¸æ“šçµ±è¨ˆæ¯”è¼ƒ'),
            ]
            sections_found = 0
            for key, regex in sections_to_try:
                try:
                    sec = extract_section_by_regex(regex)
                    if sec:
                        data["sections"][key] = sec
                        sections_found += 1
                        logger.debug("âœ… Extracted section %s for match %s", key, titan_match_id)
                except Exception:
                    logger.debug("Failed extracting section %s", key)
            formation_header = soup.find(string=re.compile(r'é™£å®¹æƒ…æ³'))
            if formation_header:
                parent = formation_header.find_parent() if hasattr(formation_header,
                                                                   "find_parent") else formation_header.parent
                if parent:
                    data["sections"]["team_formation"] = parent.get_text(separator=' | ', strip=True)
                    sections_found += 1
            try:
                home_avg, home_list = extract_ratings_or_average_from_text(page_text)
                away_avg, away_list = None, []
                m_home = re.search(r'ä¸»éšŠè¿‘10å ´å¹³å‡è©•åˆ†[:ï¼š]?\s*([0-9\.\s]{5,200})', page_text)
                m_away = re.search(r'å®¢éšŠè¿‘10å ´å¹³å‡è©•åˆ†[:ï¼š]?\s*([0-9\.\s]{5,200})', page_text)
                if m_home:
                    home_avg, home_list = extract_ratings_or_average_from_text("ä¸»éšŠè¿‘10å ´å¹³å‡è©•åˆ†:" + m_home.group(1))
                if m_away:
                    away_avg, away_list = extract_ratings_or_average_from_text("å®¢éšŠè¿‘10å ´å¹³å‡è©•åˆ†:" + m_away.group(1))
                if away_avg is None:
                    away_avg, away_list = extract_ratings_or_average_from_text(page_text)
                if home_avg is not None:
                    data["home_rating"] = home_avg
                    data["home_recent_ratings"] = home_list
                if away_avg is not None:
                    data["away_rating"] = away_avg
                    data["away_recent_ratings"] = away_list
            except Exception as e:
                logger.debug("Rating extraction exception: %s", e)
            if sections_found >= 1 or data.get("home_rating") or data.get("away_rating"):
                data["stats_available"] = True
                logger.info("âœ… Titan %s: scraped %d sections", titan_match_id, sections_found)
            else:
                data["error"] = f"Insufficient stats ({sections_found} sections)"
                logger.warning("âš ï¸ Titan %s: insufficient stats (%d sections)", titan_match_id, sections_found)
            if DEBUG_INSTRUMENTATION_AVAILABLE:
                try:
                    save_parsed_json("titan_analysis_parsed", titan_match_id, data)
                except Exception:
                    pass
            return data
        except Exception as e:
            logger.exception("Error scraping analysis stats for Titan match %s: %s", titan_match_id, e)
            return {
                "match_id": titan_match_id,
                "scraped_at": datetime.now().isoformat(),
                "url": url,
                "stats_available": False,
                "error": str(e)
            }
        finally:
            await browser.close()


# -----------------------
# Titan table helpers
# -----------------------
def extract_table_data_from_real_table(table_elem: Tag) -> List[Dict[str, str]]:
    rows = table_elem.find_all('tr')
    if not rows:
        return []
    header_row = None
    for row in rows:
        cells = row.find_all(['th', 'td'])
        if cells and 2 <= len(cells) <= 30:
            cell_texts = [c.get_text(strip=True) for c in cells]
            header_words = ['è³½', 'å‹', 'å¹³', 'è² ', 'å¾—', 'å¤±', 'ç©åˆ†', 'å‹ç‡', 'æ’å', 'ä¸»å ´', 'å®¢å ´']
            if any(any(w in txt for w in header_words) for txt in cell_texts):
                header_row = row
                break
    if not header_row:
        header_row = rows[0]
    headers = []
    for cell in header_row.find_all(['th', 'td']):
        text = cell.get_text(strip=True)
        clean_text = re.sub(r'\s+', ' ', text).strip() if text else f"col_{len(headers)}"
        headers.append(clean_text)
    if len(headers) < 2 or len(headers) > 30:
        return []
    data = []
    start_index = rows.index(header_row) + 1 if header_row in rows else 1
    for row in rows[start_index:]:
        cells = row.find_all(['td', 'th'])
        if not cells or len(cells) < 2:
            continue
        row_data = {}
        for i, cell in enumerate(cells):
            if i >= len(headers):
                break
            cell_text = cell.get_text(strip=True)
            clean_text = re.sub(r'\s+', ' ', cell_text).strip()
            if clean_text:
                row_data[headers[i]] = clean_text
                link = cell.find('a')
                if link and link.get('href'):
                    row_data[f"{headers[i]}_link"] = link.get('href')
        if row_data and len(row_data) >= 2:
            data.append(row_data)
    return data


def extract_table_data(table_elem: Tag) -> List[Dict[str, str]]:
    rows = table_elem.find_all('tr')
    if not rows:
        return []
    first_row_cells = rows[0].find_all(['th', 'td'])
    if len(first_row_cells) > 50:
        nested_tables = table_elem.find_all('table', class_=re.compile(r'oddsTable|dataTable|statsTable', re.I))
        if nested_tables:
            return extract_table_data_from_real_table(nested_tables[0])
        for nested in table_elem.find_all('table'):
            nested_rows = nested.find_all('tr')
            if nested_rows:
                first_nested_cells = nested_rows[0].find_all(['th', 'td'])
                if 3 <= len(first_nested_cells) <= 20:
                    return extract_table_data_from_real_table(nested)
        return []
    return extract_table_data_from_real_table(table_elem)


# -----------------------
# HKJC Detailed Odds Scraper
# -----------------------
class HKJCDetailedOddsScraper:
    def __init__(self):
        self.output_dir = Path("hkjc/odds")
        self.output_dir.mkdir(parents=True, exist_ok=True)

    async def scrape(self, event_id: str) -> Optional[Dict]:
        url = f"https://bet.hkjc.com/ch/football/allodds/{event_id}"
        logger.info("ğŸŒ Scraping HKJC All Odds for event: %s", event_id)
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/118.0.5993.117 Safari/537.36"
                )
            )
            page = await context.new_page()
            try:
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                try:
                    await page.wait_for_selector(".match-info", timeout=30000)
                except Exception:
                    logger.warning("HKJC detailed odds: .match-info not found for %s", event_id)
                html = await page.content()
            except Exception as e:
                logger.error("Error scraping HKJC detailed odds %s: %s", event_id, e)
                await browser.close()
                return None
            await browser.close()

        try:
            match_meta, markets = parse_allodds_from_html(html)
            out_data = {
                "created_at": datetime.now().isoformat(timespec="seconds"),
                "event_id": event_id,
                "url": url,
                "match": match_meta,
                "markets": markets,
            }
            # Save to file
            out_path = self.output_dir / f"hkjc_odds_{event_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            out_path.write_text(json.dumps(out_data, ensure_ascii=False, indent=2), encoding="utf-8")
            return out_data
        except Exception as e:
            logger.error("Error parsing HKJC detailed odds %s: %s", event_id, e)
            return None


# -----------------------
# MacauSlot odds scraper
# -----------------------
class MacauSlotOddsScraper:
    def __init__(self):
        self.base_url = "https://www.macau-slot.com/content/soccer/coming_bet.html"
        self.output_dir = Path("macauslot/odds")
        self.output_dir.mkdir(parents=True, exist_ok=True)

    async def _scrape_page_data_js(self, page) -> List[Dict]:
        try:
            return await page.evaluate("""() => {
                const matches = [];
                const containers = document.querySelectorAll('li.msl-ls-item, li.msl-odds-tr');
                containers.forEach(container => {
                    const eventId = container.getAttribute('data-ev-id');
                    if (!eventId) return;
                    const timeElem = container.querySelector('.minute');
                    const homeTeamElem = container.querySelector('.msl-odd-td-host');
                    const awayTeamElem = container.querySelector('.msl-odd-td-guest');
                    const flagWrap = container.querySelector('.msl-flag-wrap');
                    const home = homeTeamElem ? homeTeamElem.textContent.trim() : '';
                    const away = awayTeamElem ? awayTeamElem.textContent.trim() : '';
                    if (!home || !away) return;
                    const match = {
                        event_id: eventId,
                        time: timeElem ? timeElem.textContent.trim() : '',
                        competition: flagWrap ? (flagWrap.getAttribute('data-original-title') || '').trim() : '',
                        competition_short: flagWrap ? ((flagWrap.querySelector('.short') || {}).textContent || '').trim() : '',
                        home_team: home,
                        away_team: away,
                        odds: {
                            asian_handicap: [],
                            over_under: [],
                            home_draw_away: { home_odds: null, draw_odds: null, away_odds: null }
                        }
                    };
                    const oddsWrapper = container.querySelector('.msl-cm-odds-wrapper');
                    if (!oddsWrapper) {
                        matches.push(match);
                        return;
                    }
                    const stdCol = oddsWrapper.querySelector('.msl-odds-td.col-3.msl-odd-btn-bets') ||
                                   oddsWrapper.querySelector('.msl-odds-td.col-3');
                    if (stdCol) {
                        const buttons = stdCol.querySelectorAll('button.msl-bet');
                        buttons.forEach(btn => {
                            const sideBadge = btn.querySelector('.badge_front');
                            const oddsBadge = btn.querySelector('.badge');
                            if (!sideBadge || !oddsBadge) return;
                            const side = sideBadge.textContent.trim();
                            const oddsText = oddsBadge.textContent.trim();
                            const m = oddsText.match(/[\\d.]+/);
                            const odds = m ? parseFloat(m[0]) : null;
                            if (!odds) return;
                            if (side === 'ä¸»') match.odds.home_draw_away.home_odds = odds;
                            else if (side === 'å’Œ') match.odds.home_draw_away.draw_odds = odds;
                            else if (side === 'å®¢') match.odds.home_draw_away.away_odds = odds;
                        });
                    }
                    const ahSections = oddsWrapper.querySelectorAll(
                        '.msl-odds-td.col-1, .msl-odds-td.msl-odd-td-oddstype.col-1'
                    );
                    ahSections.forEach(section => {
                        const buttons = section.querySelectorAll('button.msl-bet');
                        buttons.forEach(btn => {
                            const sideBadge = btn.querySelector('.badge_left');
                            const lineBadge = btn.querySelector('.badge_front');
                            const oddsBadge = btn.querySelector('.badge');
                            if (!sideBadge || !lineBadge || !oddsBadge) return;
                            const side = sideBadge.textContent.trim();
                            const line = lineBadge.textContent.trim();
                            const oddsText = oddsBadge.textContent.trim();
                            const m = oddsText.match(/[\\d.]+/);
                            const odds = m ? parseFloat(m[0]) : null;
                            if (!odds) return;
                            let entry = match.odds.asian_handicap.find(x => x.handicap_value === line);
                            if (!entry) {
                                entry = { handicap_value: line, home_odds: null, away_odds: null };
                                match.odds.asian_handicap.push(entry);
                            }
                            if (side === 'ä¸»') entry.home_odds = odds;
                            else if (side === 'å®¢') entry.away_odds = odds;
                        });
                    });
                    const ouSections = oddsWrapper.querySelectorAll(
                        '.msl-odds-td.col-2, .msl-odds-td.msl-odd-td-oddstype.col-2'
                    );
                    ouSections.forEach(section => {
                        const buttons = section.querySelectorAll('button.msl-bet');
                        buttons.forEach(btn => {
                            const sideBadge = btn.querySelector('.badge_left');
                            const lineBadge = btn.querySelector('.badge_front');
                            const oddsBadge = btn.querySelector('.badge');
                            if (!sideBadge || !lineBadge || !oddsBadge) return;
                            const side = sideBadge.textContent.trim();
                            const line = lineBadge.textContent.trim();
                            const oddsText = oddsBadge.textContent.trim();
                            const m = oddsText.match(/[\\d.]+/);
                            const odds = m ? parseFloat(m[0]) : null;
                            if (!odds) return;
                            let entry = match.odds.over_under.find(x => x.goal_line === line);
                            if (!entry) {
                                entry = { goal_line: line, over_odds: null, under_odds: null };
                                match.odds.over_under.push(entry);
                            }
                            if (side === 'ä¸Š') entry.over_odds = odds;
                            else if (side === 'ä¸‹') entry.under_odds = odds;
                        });
                    });
                    matches.push(match);
                });
                return matches;
            }""")
        except Exception as e:
            logger.error("âš ï¸ JS scrape failed: %s", e)
            return []

    async def scrape(self, max_pages: int = 20) -> List[Dict]:
        logger.info("ğŸŒ Scraping Macau Slot live odds...")
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64)')
            page = await context.new_page()
            try:
                goto_retries = 3
                for attempt in range(1, goto_retries + 1):
                    try:
                        await page.goto(self.base_url, wait_until="domcontentloaded", timeout=30000)
                        break
                    except Exception as e:
                        logger.error("Macau page.goto failed (attempt %d/%d): %s", attempt, goto_retries, e)
                        if attempt == goto_retries:
                            raise
                        await asyncio.sleep(1.5 * attempt)
                await asyncio.sleep(2)
                if DEBUG_INSTRUMENTATION_AVAILABLE:
                    try:
                        content_initial = await page.content()
                        save_rendered_html("macau_page", "initial", content_initial)
                    except Exception:
                        pass
                try:
                    select = await page.query_selector('select.msl-cm-pager[name="msl_record_per_page"]')
                    if select:
                        await select.select_option('50')
                        await asyncio.sleep(1)
                except Exception:
                    pass
                all_matches = []
                for page_num in range(1, max_pages + 1):
                    if page_num > 1:
                        btn = await page.query_selector(f'input.msl-menu-page[value="{page_num}"]')
                        if btn and await btn.is_visible():
                            await btn.click()
                            await asyncio.sleep(1.5)
                        else:
                            break
                    if DEBUG_INSTRUMENTATION_AVAILABLE:
                        try:
                            content_page = await page.content()
                            save_rendered_html("macau_page", f"page_{page_num}", content_page)
                        except Exception:
                            pass
                    page_data = await self._scrape_page_data_js(page)
                    if page_data:
                        all_matches.extend(page_data)
                        logger.info("Page %d: Found %d matches", page_num, len(page_data))
                        if DEBUG_INSTRUMENTATION_AVAILABLE:
                            try:
                                save_parsed_json("macau_matches", f"page{page_num}", {"matches": page_data})
                            except Exception:
                                pass
                    else:
                        break
                await browser.close()
                return all_matches
            except Exception as e:
                logger.exception("âŒ Macau scrape error: %s", e)
                await browser.close()
                return []

    def save_to_json(self, data: List[Dict], filename: Optional[str] = None) -> str:
        if not filename:
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = self.output_dir / f"macauslot_odds_{ts}.json"
        else:
            filename = Path(filename)
        filename.parent.mkdir(parents=True, exist_ok=True)
        out = {
            "metadata": {
                "scraped_at": datetime.now().isoformat(),
                "source": "MacauSlot",
                "url": self.base_url,
                "total_matches": len(data)
            },
            "matches": data
        }
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(out, f, ensure_ascii=False, indent=2)
        logger.info("ğŸ’¾ Saved Macau odds to: %s", filename)
        return str(filename)


# -----------------------
# LiveMatchMatcher (orchestration with debug logs)
# -----------------------
class LiveMatchMatcher:
    def __init__(self, min_similarity_threshold: float = 0.75, time_tolerance_minutes: int = 30,
                 prioritize_similarity: bool = True):
        self.matched_games: List[Dict[str, Any]] = []
        self.unmatched_games: List[Dict[str, Any]] = []
        self.min_similarity_threshold = min_similarity_threshold
        self.time_tolerance_minutes = time_tolerance_minutes
        self.prioritize_similarity = prioritize_similarity
        self.data_quality_metrics = {
            "total_hkjc_matches": 0,
            "total_titan_matches": 0,
            "potential_matches_checked": 0,
            "high_confidence_matches": 0,
            "low_confidence_matches": 0
        }
        self.raw_hkjc_matches = []
        self.raw_titan_matches = []
        self.macau_mapping = {}

    def calculate_name_similarity(self, name1: str, name2: str) -> float:
        def clean_name(name: str) -> str:
            if not name:
                return ""
            name = re.sub(r'\[\d+\]', '', name)
            name = re.sub(r'\(ä¸­\)', '', name)
            name = re.sub(r'(å¥³è¶³|å¥³å­)$', '', name)
            return name.strip().lower()

        clean1 = clean_name(name1)
        clean2 = clean_name(name2)
        if not clean1 or not clean2:
            return 0.0
        return SequenceMatcher(None, clean1, clean2).ratio()

    def normalize_time(self, time_str: str) -> Optional[datetime]:
        try:
            if not time_str:
                return None
            time_str = time_str.strip()
            if "æœˆ" in time_str and "æ—¥" in time_str:
                m = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥\s*(\d{1,2}):(\d{2})', time_str)
                if m:
                    month, day, hour, minute = map(int, m.groups())
                    year = datetime.now().year
                    return datetime(year, month, day, hour, minute)
            if "/" in time_str and ":" in time_str:
                try:
                    if re.match(r'^\d{1,2}/\d{1,2}/\d{4}\s+\d{1,2}:\d{2}$', time_str):
                        return datetime.strptime(time_str, "%m/%d/%Y %H:%M")
                    elif re.match(r'^\d{1,2}/\d{1,2}\s+\d{1,2}:\d{2}$', time_str):
                        return datetime.strptime(time_str + f"/{datetime.now().year}", "%m/%d/%Y %H:%M")
                except Exception:
                    pass
            if ":" in time_str and len(time_str) <= 5:
                hour, minute = map(int, time_str.split(":"))
                today = datetime.now()
                return datetime(today.year, today.month, today.day, hour, minute)
            try:
                return datetime.fromisoformat(time_str)
            except Exception:
                return None
        except Exception:
            return None

    def is_exact_time_match(self, time1: Optional[datetime], time2: Optional[datetime]) -> bool:
        if not time1 or not time2:
            return False
        diff = abs((time1 - time2).total_seconds() / 60)
        return diff <= self.time_tolerance_minutes

    def are_teams_similar_enough(self, hkjc_home: str, hkjc_away: str,
                                 titan_home: str, titan_away: str) -> Tuple[bool, float, bool]:
        home_sim = self.calculate_name_similarity(hkjc_home, titan_home)
        away_sim = self.calculate_name_similarity(hkjc_away, titan_away)
        home_swapped = self.calculate_name_similarity(hkjc_home, titan_away)
        away_swapped = self.calculate_name_similarity(hkjc_away, titan_home)
        if home_sim >= self.min_similarity_threshold or away_sim >= self.min_similarity_threshold:
            return True, (home_sim + away_sim) / 2, False
        if home_swapped >= self.min_similarity_threshold or away_swapped >= self.min_similarity_threshold:
            return True, (home_swapped + away_swapped) / 2, True
        return False, 0.0, False

    def validate_match_data(self, match_data: Dict) -> bool:
        required_fields = ['home_team', 'away_team']
        for f in required_fields:
            if not match_data.get(f) or len(str(match_data[f]).strip()) < 2:
                return False
        for team_field in ['home_team', 'away_team']:
            name = match_data.get(team_field, "")
            if name and len(re.sub(r'[^a-zA-Z\u4e00-\u9fff]', '', name)) == 0:
                return False
        return True

    # --- HKJC scraper (with debug)
    async def scrape_hkjc_matches(self) -> List[Dict]:
        matches: List[Dict] = []
        raw_matches: List[Dict] = []
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            try:
                cprint("ğŸŒ Loading HKJC matches live...", Fore.BLUE)
                await page.goto("https://bet.hkjc.com/ch/football/had", wait_until='domcontentloaded', timeout=60000)
                await asyncio.sleep(2)
                try:
                    content = await page.content()
                    if DEBUG_INSTRUMENTATION_AVAILABLE:
                        save_rendered_html("hkjc_page", "index", content)
                except Exception:
                    pass
                await self.click_show_more_hkjc(page)
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                match_rows = soup.find_all('div', class_='match-row') or soup.find_all('div', class_='event-row')
                cprint(f"ğŸ” Found {len(match_rows)} match rows on HKJC", Fore.CYAN)
                for i, row in enumerate(match_rows):
                    try:
                        match_data = await self.extract_hkjc_match_data(row)
                        if match_data and self.validate_match_data(match_data):
                            norm_time = self.normalize_time(match_data.get('date', ''))
                            raw = {
                                "source": "HKJC",
                                "match_id": match_data.get('match_id', ''),
                                "event_id": match_data.get('event_id', ''),
                                "home_team": match_data['home_team'],
                                "away_team": match_data['away_team'],
                                "match_time_original": match_data.get('date', ''),
                                "normalized_time": norm_time,
                                "normalized_time_str": norm_time.isoformat() if norm_time else None,
                                "tournament": match_data.get('tournament', ''),
                                "scraped_at": datetime.now().isoformat()
                            }
                            raw_matches.append(raw)
                            matches.append(raw)
                            if i < 3:
                                cprint(
                                    f"  Sample: {raw['home_team']} vs {raw['away_team']} (ID: {raw.get('event_id')})",
                                    Fore.MAGENTA)
                    except Exception as e:
                        if i < 3:
                            cprint(f"  âš ï¸ Error in HKJC row {i + 1}: {e}", Fore.YELLOW)
                        continue
                self.data_quality_metrics['total_hkjc_matches'] = len(matches)
                self.raw_hkjc_matches = raw_matches
                if DEBUG_INSTRUMENTATION_AVAILABLE:
                    try:
                        save_parsed_json("hkjc_index_parsed", "index",
                                         {"raw_matches": raw_matches, "count": len(raw_matches)})
                    except Exception:
                        pass
                cprint(f"âœ… Successfully extracted {len(matches)} HKJC matches", Fore.GREEN)
                return matches
            except Exception as e:
                cprint(f"âŒ Error scraping HKJC: {e}", Fore.RED)
                return []
            finally:
                await browser.close()

    async def click_show_more_hkjc(self, page):
        try:
            xpaths = [
                "//div[contains(text(), 'é¡¯ç¤ºæ›´å¤š')]",
                "//button[contains(text(), 'é¡¯ç¤ºæ›´å¤š')]",
                "//span[contains(text(), 'é¡¯ç¤ºæ›´å¤š')]",
                "//a[contains(text(), 'é¡¯ç¤ºæ›´å¤š')]"
            ]
            for xp in xpaths:
                elements = await page.query_selector_all(f"xpath={xp}")
                for el in elements:
                    try:
                        if await el.is_visible():
                            await el.scroll_into_view_if_needed()
                            await asyncio.sleep(0.5)
                            await el.click()
                            cprint("  âœ… Clicked 'Show More' for HKJC", Fore.BLUE)
                            await asyncio.sleep(1.5)
                            return True
                    except Exception:
                        continue
            return False
        except Exception as e:
            cprint(f"  âš ï¸ Could not find 'Show More' ({e})", Fore.YELLOW)
            return False

    async def extract_hkjc_match_data(self, match_row) -> Optional[Dict]:
        try:
            match_id_elem = match_row.find('div', class_='fb-id')
            match_id = match_id_elem.get_text(strip=True) if match_id_elem else None
            date_elem = match_row.find('div', class_='date')
            date = date_elem.get_text(strip=True) if date_elem else ""
            tourn_elem = match_row.find('div', class_='tourn')
            tournament = ""
            if tourn_elem and tourn_elem.find('img'):
                tournament = tourn_elem.find('img').get('title', '') or ""
            home_team, away_team = self.extract_hkjc_teams(match_row)
            if not home_team or not away_team:
                return None

            # Extract event_id for detailed odds URL
            event_id = None
            all_odds_link = match_row.find('a', href=re.compile(r'allodds'))
            if all_odds_link:
                href = all_odds_link.get('href')
                m = re.search(r'/allodds/(\d+)', href)
                if m:
                    event_id = m.group(1)
            # fallback: look for coupon link with IDs
            if not event_id:
                link = match_row.find('a', href=re.compile(r'football/odds'))
                if link:
                    href = link.get('href')
                    m = re.search(r'id=(\d+)', href)
                    if m:
                        event_id = m.group(1)

            return {
                'match_id': match_id,
                'event_id': event_id,
                'date': date,
                'tournament': tournament,
                'home_team': home_team,
                'away_team': away_team
            }
        except Exception as e:
            logger.debug("extract_hkjc_match_data error: %s", e)
            return None

    def extract_hkjc_teams(self, match_row) -> Tuple[str, str]:
        home_team = ""
        away_team = ""
        try:
            team_icon = match_row.find('div', class_='teamIconSmall')
            if team_icon:
                team_container = team_icon.find('div', title=True)
                if team_container:
                    divs = team_container.find_all('div')
                    if len(divs) >= 2:
                        home_team = divs[0].get_text(strip=True)
                        away_team = divs[1].get_text(strip=True)
        except Exception as e:
            logger.debug("extract_hkjc_teams error: %s", e)
        return home_team, away_team

    # --- Titan007 list scraper (with debug)
    async def scrape_titan007_matches(self) -> List[Dict]:
        matches = []
        raw_matches = []
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            try:
                cprint("ğŸŒ Loading Titan007 matches live...", Fore.BLUE)
                await page.goto("https://live.titan007.com/indexall_big.aspx", wait_until='networkidle', timeout=30000)
                await asyncio.sleep(1.5)
                try:
                    content = await page.content()
                    if DEBUG_INSTRUMENTATION_AVAILABLE:
                        save_rendered_html("titan_index", "index", content)
                except Exception:
                    pass
                soup = BeautifulSoup(content, 'html.parser')
                main_table = None
                for table in soup.find_all('table'):
                    txt = table.get_text()
                    if 'æ™‚é–“' in txt and 'æ¯”è³½çƒéšŠ' in txt:
                        main_table = table
                        break
                if not main_table:
                    cprint("âŒ Could not find main Titan007 table", Fore.RED)
                    return []
                rows = main_table.find_all('tr')
                cprint(f"ğŸ” Found {len(rows)} rows in Titan007 table", Fore.CYAN)
                time_col_idx = 1
                status_col_idx = 2
                for i, row in enumerate(rows):
                    try:
                        if not row.get_text(strip=True):
                            continue
                        if 'æ™‚é–“' in row.get_text() and 'æ¯”è³½çƒéšŠ' in row.get_text():
                            cells = row.find_all(['td', 'th'])
                            for idx, cell in enumerate(cells):
                                txt = cell.get_text(strip=True)
                                if txt == 'æ™‚é–“':
                                    time_col_idx = idx
                                elif txt == 'ç‹€æ…‹':
                                    status_col_idx = idx
                            continue
                        team1 = row.find('a', id=lambda x: x and x.startswith('team1_'))
                        team2 = row.find('a', id=lambda x: x and x.startswith('team2_'))
                        if not team1 or not team2:
                            continue
                        match_id = team1.get('id', '').replace('team1_', '')
                        league = "Unknown"
                        cells = row.find_all(['td', 'th'])
                        if cells:
                            league_text = cells[0].get_text(strip=True)
                            if league_text and league_text != 'æ™‚é–“' and 'æ¯”è³½' not in league_text:
                                league = league_text
                        scheduled_time = ""
                        if len(cells) > time_col_idx:
                            scheduled_time = cells[time_col_idx].get_text(strip=True)
                        status = ""
                        if len(cells) > status_col_idx:
                            status = cells[status_col_idx].get_text(strip=True)
                        home_team = re.sub(r'\[\d+\]|\(ä¸­\)', '', team1.get_text(strip=True)).strip()
                        away_team = re.sub(r'\[\d+\]|\(ä¸­\)', '', team2.get_text(strip=True)).strip()
                        if not home_team or not away_team or len(home_team) < 2 or len(away_team) < 2:
                            continue
                        score = ""
                        for cell in cells:
                            cell_text = cell.get_text(strip=True)
                            if '-' in cell_text and len(cell_text) <= 7:
                                score = cell_text
                                break
                        normalized_time = None
                        if scheduled_time and re.match(r'^\d{1,2}:\d{2}$', scheduled_time):
                            try:
                                hour, minute = map(int, scheduled_time.split(":"))
                                today = datetime.now()
                                normalized_time = datetime(today.year, today.month, today.day, hour, minute)
                            except Exception:
                                normalized_time = None
                        raw_match = {
                            "source": "Titan007",
                            "match_id": match_id,
                            "league": league,
                            "home_team": home_team,
                            "away_team": away_team,
                            "scheduled_time_original": scheduled_time,
                            "match_status": status,
                            "score": score,
                            "normalized_time": normalized_time,
                            "normalized_time_str": normalized_time.isoformat() if normalized_time else None,
                            "scraped_at": datetime.now().isoformat()
                        }
                        raw_matches.append(raw_match)
                        if self.validate_match_data(raw_match):
                            matches.append(raw_match)
                        if len(matches) <= 3:
                            cprint(f"  Sample: {home_team} vs {away_team}", Fore.MAGENTA)
                    except Exception as e:
                        if i < 5:
                            cprint(f"  âš ï¸ Error parsing Titan row {i + 1}: {e}", Fore.YELLOW)
                        continue
                self.data_quality_metrics['total_titan_matches'] = len(matches)
                self.raw_titan_matches = raw_matches
                if DEBUG_INSTRUMENTATION_AVAILABLE:
                    try:
                        save_parsed_json("titan_index_parsed", "index",
                                         {"raw_matches": raw_matches, "matches": matches})
                    except Exception:
                        pass
                cprint(f"âœ… Successfully extracted {len(matches)} Titan007 matches", Fore.GREEN)
                return matches
            except Exception as e:
                cprint(f"âŒ Error scraping Titan007: {e}", Fore.RED)
                return []
            finally:
                await browser.close()

    # -----------------------
    # Matching orchestration
    # -----------------------
    async def find_matching_games(self) -> Tuple[List[Dict], List[Dict]]:
        cprint("\n" + "=" * 80, Fore.WHITE)
        cprint("ğŸ” FINDING MATCHES (HKJC + Titan007 + Macau Slot)", Fore.WHITE)
        cprint("=" * 80, Fore.WHITE)

        if DEBUG_INSTRUMENTATION_AVAILABLE:
            try:
                init_debug_session()
                log_info("Session started", {
                    "min_similarity_threshold": self.min_similarity_threshold,
                    "time_tolerance_minutes": self.time_tolerance_minutes
                })
            except Exception:
                pass

        # Step 1: Macau odds
        cprint("\nğŸ“¥ Step 1: Scraping Macau Slot odds...", Fore.BLUE)
        macau_scraper = MacauSlotOddsScraper()
        macau_odds = await macau_scraper.scrape(max_pages=20)
        if macau_odds:
            macau_file = macau_scraper.save_to_json(macau_odds)
            cprint(f"âœ… Macau Slot: {len(macau_odds)} matches saved to {macau_file}", Fore.GREEN)
        else:
            cprint("âš ï¸ No Macau Slot odds scraped", Fore.YELLOW)
            macau_odds = []

        # Step 2: HKJC and Titan
        cprint("\nğŸ“¥ Step 2: Scraping HKJC and Titan007 matches...", Fore.BLUE)
        hkjc_matches = await self.scrape_hkjc_matches()
        titan_matches = await self.scrape_titan007_matches()
        cprint(f"\nğŸ“Š Match Counts:", Fore.CYAN)
        cprint(f"  HKJC: {len(hkjc_matches)} matches", Fore.CYAN)
        cprint(f"  Titan007: {len(titan_matches)} matches", Fore.CYAN)
        cprint(f"  Macau Slot: {len(macau_odds)} matches", Fore.CYAN)

        if not hkjc_matches or not titan_matches:
            cprint("âŒ Cannot proceed: One or both sites returned no matches", Fore.RED)
            return [], []

        # Initialize HKJC Detailed Scraper
        hkjc_detailed_scraper = HKJCDetailedOddsScraper()

        # Step 3: Build Macau mapping to Titan
        cprint("\nğŸ”„ Step 3: Building Macau odds mapping to Titan...", Fore.BLUE)
        self.macau_mapping = {}
        for titan in titan_matches:
            titan_time = titan.get("normalized_time")
            for macau in macau_odds:
                macau_time = self.normalize_time(macau.get('time', ''))
                teams_similar, avg_sim, _ = self.are_teams_similar_enough(
                    titan['home_team'], titan['away_team'],
                    macau.get('home_team', ''), macau.get('away_team', '')
                )
                time_match = self.is_exact_time_match(titan_time, macau_time)
                if teams_similar and avg_sim >= 0.70 and time_match:
                    self.macau_mapping[titan['match_id']] = macau
        cprint(f"  Mapped {len(self.macau_mapping)} Titan matches to Macau odds", Fore.GREEN)

        # Step 4: Find HKJC <-> Titan matches, scrape details
        cprint("\nğŸ” Step 4: Finding matches and running analysis...", Fore.BLUE)
        matched = []
        unmatched_hkjc = []
        ai_rows_for_excel = []

        def classify(hkjc_match, titan_match, macau_match):
            has_h = hkjc_match is not None
            has_t = titan_match is not None
            has_m = macau_match is not None
            if has_h and has_t and has_m: return "hkjc_titan_macau"
            if has_h and has_t and not has_m: return "hkjc_titan"
            if has_h and not has_t and has_m: return "hkjc_macau"
            if has_h and not has_t and not has_m: return "hkjc_only"
            if has_t and has_m: return "titan_macau"
            if has_t: return "titan_only"
            if has_m: return "macau_only"
            return "unknown"

        for hkjc in hkjc_matches:
            best_match = None
            best_score = 0.0
            best_is_swapped = False

            for titan in titan_matches:
                self.data_quality_metrics['potential_matches_checked'] += 1
                teams_similar, avg_sim, is_swapped = self.are_teams_similar_enough(
                    hkjc['home_team'], hkjc['away_team'],
                    titan['home_team'], titan['away_team']
                )
                if avg_sim >= self.min_similarity_threshold:
                    score = avg_sim * 100
                    if score > best_score:
                        best_score = score
                        best_match = titan
                        best_is_swapped = is_swapped

            if best_match and best_score >= self.min_similarity_threshold * 100:
                titan_id = best_match['match_id']
                macau = self.macau_mapping.get(titan_id)
                source_type = classify(hkjc, best_match, macau)
                matched_item = {
                    "source_coverage": source_type,
                    "hkjc_match": {
                        "match_id": hkjc.get('match_id', ''),
                        "event_id": hkjc.get('event_id', ''),
                        "home_team": hkjc['home_team'],
                        "away_team": hkjc['away_team'],
                        "match_time": hkjc.get('match_time_original', ''),
                        "tournament": hkjc.get('tournament', '')
                    },
                    "titan_match": {
                        "match_id": titan_id,
                        "home_team": best_match['home_team'],
                        "away_team": best_match['away_team'],
                        "scheduled_time": best_match.get('scheduled_time_original', ''),
                        "league": hkjc.get('tournament', ''),
                        "status": best_match.get('match_status', '')
                    },
                    "macau_match": macau,
                    "similarity_score": best_score,
                    "teams_swapped": best_is_swapped,
                    "matched_at": datetime.now().isoformat()
                }

                sim_pct = best_score
                cprint(
                    f"\nâœ… {source_type.upper()}: {hkjc['home_team']} vs {hkjc['away_team']} "
                    f"(similarity={sim_pct:.1f}%)",
                    Fore.GREEN
                )
                cprint(f"   Titan: {best_match.get('scheduled_time_original', 'N/A')}", Fore.GREEN)

                # --- NEW: Scrape detailed HKJC odds if event_id is present ---
                hkjc_event_id = hkjc.get('event_id')
                if hkjc_event_id:
                    cprint(f"   ğŸ“Š Scraping HKJC All Odds (Event {hkjc_event_id})...", Fore.CYAN)
                    hkjc_details = await hkjc_detailed_scraper.scrape(hkjc_event_id)
                    if hkjc_details:
                        matched_item['hkjc_detailed_odds'] = hkjc_details
                        cprint("   âœ… HKJC detailed odds captured", Fore.GREEN)
                    else:
                        cprint("   âš ï¸ HKJC detailed odds failed", Fore.YELLOW)
                else:
                    cprint("   âš ï¸ No HKJC Event ID, skipping detailed odds", Fore.YELLOW)

                # Scrape Titan analysis stats and call AI
                try:
                    cprint("   ğŸ“Š Scraping Titan stats...", Fore.CYAN)
                    detailed_stats = await scrape_match_stats_from_analysis_page(titan_id)
                    matched_item['detailed_stats'] = detailed_stats
                    if detailed_stats.get('stats_available'):
                        matched_item['titan_stats_available'] = True
                        normalized = normalize_parsed_data(detailed_stats)
                        cprint("   ğŸ¤– Running AI analysis...", Fore.CYAN)
                        ai_result = await perform_ai_analysis_for_match_async(normalized, call_deepseek_api_async)
                        matched_item['ai_recommendation'] = ai_result
                        if ai_result and ai_result.get('best_bet_market') and ai_result.get(
                                'best_bet_selection') and ai_result.get('confidence_level', 0) > 0:
                            ai_rows_for_excel.append({
                                "match_id_titan": titan_id,
                                "match_id_hkjc": hkjc.get('match_id', ''),
                                "source_coverage": source_type,
                                "home_team": hkjc['home_team'],
                                "away_team": hkjc['away_team'],
                                "match_time": hkjc.get('match_time_original', ''),
                                "league": hkjc.get('tournament', ''),
                                "best_bet_market": ai_result.get('best_bet_market'),
                                "best_bet_selection": ai_result.get('best_bet_selection'),
                                "confidence_level": ai_result.get('confidence_level'),
                                "brief_reasoning": ai_result.get('brief_reasoning'),
                                "similarity_score": best_score,
                                "ai_analysis_timestamp": datetime.now().isoformat()
                            })
                            cprint(
                                f"   ğŸ’¡ AI Recommendation: {ai_result.get('best_bet_selection')} ({ai_result.get('confidence_level')}/10)",
                                Fore.CYAN)
                        else:
                            cprint("   âš ï¸ AI returned no strong recommendation", Fore.YELLOW)
                    else:
                        matched_item['titan_stats_available'] = False
                        matched_item['skipped_reason'] = detailed_stats.get('error', 'no_stats')
                        cprint("   âš ï¸ No stats available for analysis", Fore.YELLOW)
                except Exception as e:
                    logger.exception("Failed to analyze Titan match %s: %s", titan_id, e)

                matched.append(matched_item)
                self.data_quality_metrics['high_confidence_matches'] += 1
            else:
                unmatched_hkjc.append(hkjc)
                if len(unmatched_hkjc) <= 3:
                    reason = "No similar teams found"
                    cprint(f"\nâŒ NO MATCH: {hkjc['home_team']} vs {hkjc['away_team']}", Fore.RED)
                    cprint(f"   Reason: {reason}", Fore.RED)

        self.matched_games = matched
        self.unmatched_games = unmatched_hkjc

        # Reports
        cprint(f"\nğŸ“Š FINAL RESULTS:", Fore.CYAN)
        cprint(f"   âœ… Matched games: {len(matched)}", Fore.GREEN)
        cprint(f"   âŒ Unmatched HKJC games: {len(unmatched_hkjc)}", Fore.RED)

        if ai_rows_for_excel:
            excel_filename = f"ai_recommendations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            try:
                save_recommendations_to_excel(ai_rows_for_excel, excel_filename)
                cprint(f"\nğŸ“Š AI recommendations saved to: {excel_filename}", Fore.CYAN)
            except Exception as e:
                cprint(f"âŒ Failed saving AI Excel: {e}", Fore.RED)

        return matched, unmatched_hkjc

    # -----------------------
    # Reporting helpers
    # -----------------------
    def generate_detailed_report(self) -> Dict:
        report = {
            "summary": {
                "total_matched": len(self.matched_games),
                "total_unmatched": len(self.unmatched_games),
                "data_quality_metrics": self.data_quality_metrics
            },
            "top_matches": sorted(self.matched_games, key=lambda x: x.get('similarity_score', 0), reverse=True)[:5],
            "common_issues": [],
            "recommendations": []
        }
        if self.data_quality_metrics['total_hkjc_matches'] == 0:
            report['common_issues'].append("No matches found on HKJC")
        if self.data_quality_metrics['total_titan_matches'] == 0:
            report['common_issues'].append("No matches found on Titan007")
        return report

    def save_report(self, report: Dict, filename: Optional[str] = None):
        if not filename:
            filename = f"detailed_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        cprint(f"ğŸ“Š Detailed report saved to: {filename}", Fore.CYAN)

    def save_comparison_csv(self, filename: Optional[str] = None):
        if not filename:
            filename = f"all_scraped_matches_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        if not self.raw_hkjc_matches or not self.raw_titan_matches:
            cprint("âš ï¸ No data available for CSV export", Fore.YELLOW)
            return
        df_h = pd.DataFrame(self.raw_hkjc_matches)
        df_t = pd.DataFrame(self.raw_titan_matches)
        if 'normalized_time_str' in df_h.columns:
            df_h['normalized_time_str'] = df_h['normalized_time_str'].astype(str)
        if 'normalized_time_str' in df_t.columns:
            df_t['normalized_time_str'] = df_t['normalized_time_str'].astype(str)
        df_h_ren = df_h.add_suffix('_hkjc')
        df_t_ren = df_t.add_suffix('_titan')
        try:
            merged = pd.merge(df_h_ren, df_t_ren, left_on='normalized_time_str_hkjc',
                              right_on='normalized_time_str_titan', how='outer')
            merged.sort_values(by=['normalized_time_str_hkjc'], inplace=True, na_position='last')
            merged.to_csv(filename, index=False, encoding='utf-8-sig')
            cprint(f"ğŸ“‹ Side-by-side comparison saved to: {filename}", Fore.CYAN)
        except Exception as e:
            cprint(f"âŒ Error during CSV merge: {e}", Fore.RED)


# -----------------------
# Excel saver
# -----------------------
def save_recommendations_to_excel(recommendations: List[Dict], filename: str):
    with pd.ExcelWriter(filename, engine='openpyxl') as writer:
        df_all = pd.DataFrame(recommendations)
        df_all.to_excel(writer, sheet_name='Summary', index=False)
        if not df_all.empty:
            for market in df_all['best_bet_market'].unique():
                df_m = df_all[df_all['best_bet_market'] == market]
                safe_sheet = re.sub(r'[\\/*?:\[\]]', '_', str(market))[:31]
                try:
                    df_m.to_excel(writer, sheet_name=safe_sheet, index=False)
                except Exception:
                    pass
    logger.info("ğŸ’¾ AI recommendations saved to Excel file: %s", filename)


# -----------------------
# Main entrypoint
# -----------------------
async def main():
    cprint("ğŸš€ LIVE MATCH CROSS-REFERENCER â€” WITH MACAU SLOT & HKJC DETAILED ODDS", Fore.WHITE)
    cprint("=" * 80, Fore.WHITE)

    matcher = LiveMatchMatcher(min_similarity_threshold=0.75, time_tolerance_minutes=30, prioritize_similarity=True)
    matched_games, unmatched = await matcher.find_matching_games()

    report = matcher.generate_detailed_report()
    matcher.save_report(report)
    matcher.save_comparison_csv()

    if matched_games:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"matched_games_with_ai_analysis_{ts}.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(matched_games, f, ensure_ascii=False, indent=2)
        cprint(f"\nğŸ’¾ Saved {len(matched_games)} matched games (with AI analysis) to: {filename}", Fore.CYAN)

    if unmatched:
        analysis_file = f"unmatched_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(analysis_file, "w", encoding="utf-8") as f:
            json.dump({
                "unmatched_count": len(unmatched),
                "sample_unmatched": unmatched[:10],
                "analysis_time": datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        cprint(f"ğŸ’¾ Saved unmatched analysis to: {analysis_file}", Fore.CYAN)

    cprint("\nâœ… Process complete!", Fore.GREEN)
    cprint(f"ğŸ“Š Summary: {len(matched_games)} matches analyzed", Fore.CYAN)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        cprint("Interrupted by user.", Fore.YELLOW)
    except Exception as e:
        logger.exception("Fatal error in main: %s", e)
