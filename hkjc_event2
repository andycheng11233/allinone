#!/usr/bin/env python3
import asyncio, json, re, sys
from pathlib import Path
from datetime import datetime
from contextlib import suppress
from playwright.async_api import async_playwright

BET_HOME     = "https://bet.hkjc.com/ch/football/home"
ROWS_SEL     = ".match-row,.event-row"
OUTPUT       = "debug_bet_home_event_ids.json"
DT_FORMAT    = "%d/%m/%Y %H:%M"
PAUSE        = 0.6
CLICK_WAIT   = 1.8
TIMEOUT_MS   = 8000
HEADLESS     = True
MAX_GROWTH_LOOPS = 12
MAX_ROWS_CAP = 200  # safety if declared missing

def parse_row_start(txt: str):
    try:
        return datetime.strptime(txt.strip(), DT_FORMAT)
    except Exception:
        return None

def extract_id_from_url(url: str):
    m = re.search(r"/allodds/(\d+)", url)
    return m.group(1) if m else None

async def get_declared(page):
    # Try regex on full HTML, fallback to text locator
    html = await page.content()
    m = re.search(r"共有\s*(\d+)\s*場賽事", html)
    if m:
        return int(m.group(1))
    try:
        loc = page.locator("text=/共有\\s*\\d+\\s*場賽事/")
        txt = await loc.first.text_content(timeout=2000)
        m2 = re.search(r"共有\s*(\d+)\s*場賽事", txt or "")
        if m2:
            return int(m2.group(1))
    except Exception:
        pass
    return None

async def click_show_more(page):
    xps = [
        "//div[contains(text(), '顯示更多')]",
        "//button[contains(text(), '顯示更多')]",
        "//span[contains(text(), '顯示更多')]",
        "//a[contains(text(), '顯示更多')]",
    ]
    for xp in xps:
        for el in await page.query_selector_all(f"xpath={xp}"):
            if await el.is_visible():
                await el.scroll_into_view_if_needed()
                await asyncio.sleep(0.2)
                await el.click()
                await asyncio.sleep(0.8)
                return True
    return False

async def scroll_bottom(page):
    await page.mouse.wheel(0, 2000)
    await asyncio.sleep(PAUSE)
    await page.evaluate("() => window.scrollTo(0, document.body.scrollHeight)")
    await asyncio.sleep(PAUSE)

async def load_rows_full(page, declared):
    await page.goto(BET_HOME, wait_until="domcontentloaded", timeout=90000)
    await asyncio.sleep(0.8)

    last = 0
    for _ in range(MAX_GROWTH_LOOPS):
        await click_show_more(page)
        await scroll_bottom(page)
        cnt = await page.evaluate(f"() => document.querySelectorAll('{ROWS_SEL}').length")
        if declared and cnt >= declared:
            break
        if cnt <= last:
            break
        last = cnt
    rows = page.locator(ROWS_SEL)
    total = await rows.count()
    return rows, total

async def main():
    now = datetime.now()
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=HEADLESS)
        ctx = await browser.new_context()
        page = await ctx.new_page()
        page.set_default_timeout(TIMEOUT_MS)

        collected = set()
        mod_key = ["Meta"] if sys.platform == "darwin" else ["Control"]

        def log_url(url: str):
            eid = extract_id_from_url(url)
            if eid:
                collected.add(eid)

        ctx.on("request",  lambda req: log_url(req.url))
        ctx.on("response", lambda resp: log_url(resp.url))

        await page.goto(BET_HOME, wait_until="domcontentloaded", timeout=90000)
        await asyncio.sleep(0.8)
        declared = await get_declared(page)
        print(f"Declared: {declared} matches" if declared else "Declared not found")

        rows, total = await load_rows_full(page, declared)
        target_total = declared or min(total, MAX_ROWS_CAP)
        print(f"Rows detected: {total}, processing up to: {target_total}")

        i = 0
        while i < target_total:
            # Refresh rows each loop to avoid stale locators
            rows, total = await load_rows_full(page, declared)
            if i >= total:
                break
            row = rows.nth(i)

            try:
                rid = None
                with suppress(Exception):
                    rid = await row.get_attribute("id")
                if not rid:
                    with suppress(Exception):
                        rid = await row.locator(".fb-id").first.text_content()
                rid = (rid or "").strip()

                dt_txt = None
                with suppress(Exception):
                    dt_txt = await row.locator(".date").first.text_content()
                start_dt = parse_row_start(dt_txt or "")
                if start_dt and start_dt <= now:
                    print(f"[{i+1}/{total}] {rid} skip (started)")
                    i += 1
                    continue

                trigger = None
                for sel in [
                    '[title*="賠率"]',
                    '[title*="所有賠率"]',
                    ".teamIconSmall [title]",
                    ".teamIconSmall",
                    ".team",
                ]:
                    cand = row.locator(sel)
                    if await cand.count():
                        trigger = cand.first
                        break
                if not trigger:
                    print(f"[{i+1}/{total}] {rid} no trigger")
                    i += 1
                    continue

                pages_before = set(ctx.pages)
                with suppress(Exception):
                    await trigger.click(modifiers=mod_key, force=True)
                await asyncio.sleep(CLICK_WAIT)

                pages_after = set(ctx.pages)
                new_pages = list(pages_after - pages_before)
                url_captured = None

                if new_pages:
                    np = new_pages[0]
                    with suppress(Exception):
                        await np.wait_for_load_state("domcontentloaded", timeout=TIMEOUT_MS)
                    url_captured = np.url
                    with suppress(Exception):
                        await np.close()
                else:
                    if extract_id_from_url(page.url):
                        url_captured = page.url
                        with suppress(Exception):
                            await page.go_back()

                if url_captured:
                    eid = extract_id_from_url(url_captured)
                    if eid:
                        collected.add(eid)
                        print(f"[{i+1}/{total}] {rid} captured {eid}")
                    else:
                        print(f"[{i+1}/{total}] {rid} nav w/o id")
                else:
                    print(f"[{i+1}/{total}] {rid} no popup/nav")

            except Exception as e:
                print(f"[{i+1}/{total}] error: {e}")

            i += 1

        ids_sorted = sorted(collected)
        Path(OUTPUT).write_text(json.dumps(ids_sorted, ensure_ascii=False, indent=2), encoding="utf-8")
        print(f"Scraped IDs: {len(ids_sorted)} (saved to {OUTPUT})")

        if declared is not None and len(ids_sorted) != declared:
            print(f"FAILED: declared {declared}, scraped {len(ids_sorted)}")
            await browser.close()
            sys.exit(1)

        print("SUCCESS")
        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
