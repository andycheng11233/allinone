#!/usr/bin/env python3
"""
Unified MacauSlot + Titan007 + HKJC matcher + AI analysis.

v21b updates:
- Added fast-skip for HKJC Step 0: if declared match count is <= cached odds count (existing cache + processed set),
  we skip the per-row clicking entirely to save time.
- Existing behavior retained (stringified cache IDs, cached AI reuse, etc.).
"""

import asyncio
import os
import re
import sys
import json
import logging
import contextlib
import unicodedata
from datetime import datetime
from difflib import SequenceMatcher
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any

import httpx
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import pandas as pd

# Debug instrumentation (optional)
try:
    from debug_instrumentation import (
        init_debug_session,
        save_rendered_html,
        save_parsed_json,
        log_mapping_decision,
        log_info
    )
    DEBUG_INSTRUMENTATION_AVAILABLE = True
except Exception:
    DEBUG_INSTRUMENTATION_AVAILABLE = False

# Color output (optional)
try:
    from colorama import init as _init_colorama, Fore, Style
    _init_colorama(autoreset=True)
    COLORS_AVAILABLE = True
except Exception:
    COLORS_AVAILABLE = False

    class _Dummy:
        GREEN = RED = YELLOW = BLUE = MAGENTA = CYAN = WHITE = ''
        BRIGHT = NORMAL = ''
    Fore = _Dummy()
    Style = _Dummy()

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logging.getLogger("playwright").setLevel(logging.WARNING)
logger = logging.getLogger("matcher")

DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_API_URL = os.getenv("DEEPSEEK_API_URL", "https://api.deepseek.com/chat/completions")
DEEPSEEK_TIMEOUT = float(os.getenv("DEEPSEEK_TIMEOUT", "30"))
DEEPSEEK_RETRIES = int(os.getenv("DEEPSEEK_RETRIES", "3"))

if not DEEPSEEK_API_KEY:
    logger.warning("DEEPSEEK_API_KEY not set. AI functionality will fail unless you set the env var.")

AI_CACHE_PATH = Path(".cache/ai_processed.json")
HKJC_ODDS_PROCESSED_PATH = Path(".cache/hkjc_odds_processed.json")
TITAN_STATS_PROCESSED_PATH = Path(".cache/titan_stats_processed.json")

TITAN_STATS_BASE = Path("titan/stats")

ALIAS_TABLE: Dict[str, set] = {
    # "barcelona": {"barca", "fc barcelona", "å·´å¡", "å·´å¡éš†æ‹¿"},
}

def resolve_alias(name: str) -> str:
    key = name.strip().lower()
    for canon, variants in ALIAS_TABLE.items():
        if key == canon:
            return canon
        if key in variants:
            return canon
    return key

def strip_accents(text: str) -> str:
    return ''.join(ch for ch in unicodedata.normalize('NFKD', text) if not unicodedata.combining(ch))

def normalize_team_name(name: str) -> str:
    if not name:
        return ""
    name = strip_accents(name)
    name = name.lower()
    name = re.sub(r'\[\d+\]', '', name)
    name = re.sub(r'\(ä¸­\)', '', name)
    name = re.sub(r'(å¥³è¶³|å¥³å­)$', '', name)
    name = re.sub(r'\b(fc|cf|sc|afc|cfc)\b', '', name)
    name = re.sub(r'[^a-z0-9\u4e00-\u9fff]+', ' ', name)
    name = re.sub(r'\s+', ' ', name).strip()
    name = resolve_alias(name)
    return name

def normalize_league(text: str) -> str:
    if not text:
        return ""
    t = strip_accents(text).lower()
    t = re.sub(r'[^a-z0-9\u4e00-\u9fff]+', ' ', t)
    return re.sub(r'\s+', ' ', t).strip()

def league_bonus(l1: str, l2: str, bonus: float = 0.05) -> float:
    if not l1 or not l2:
        return 0.0
    return bonus if normalize_league(l1) == normalize_league(l2) else 0.0

def name_similarity(a: str, b: str) -> float:
    na = normalize_team_name(a)
    nb = normalize_team_name(b)
    if not na or not nb:
        return 0.0
    return SequenceMatcher(None, na, nb).ratio()

def load_ai_cache() -> Dict[str, Any]:
    try:
        if AI_CACHE_PATH.exists():
            return json.loads(AI_CACHE_PATH.read_text(encoding="utf-8"))
    except Exception as e:
        logger.warning("Failed to load AI cache: %s", e)
    return {}

def save_ai_cache(cache: Dict[str, Any]):
    try:
        AI_CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)
        AI_CACHE_PATH.write_text(json.dumps(cache, ensure_ascii=False, indent=2), encoding="utf-8")
    except Exception as e:
        logger.warning("Failed to save AI cache: %s", e)

def load_cache_set(path: Path, as_str: bool = False) -> set:
    try:
        if path.exists():
            loaded = json.loads(path.read_text(encoding="utf-8"))
            if as_str:
                return set(str(x) for x in loaded)
            return set(loaded)
    except Exception:
        pass
    return set()

def save_cache_set(path: Path, data: set):
    try:
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(json.dumps(sorted(list(data))), encoding="utf-8")
    except Exception as e:
        logger.warning("Failed to save cache set to %s: %s", path, e)

def load_hkjc_odds_from_disk(odds_dir: Path = Path("hkjc/odds")) -> Dict[str, Dict[str, Any]]:
    cache = {}
    if not odds_dir.exists():
        return cache
    for f in odds_dir.glob("hkjc_odds_*.json"):
        try:
            data = json.loads(f.read_text(encoding="utf-8"))
            eid = str(data.get("event_id") or "")
            if eid:
                cache[eid] = data
        except Exception:
            continue
    return cache

def cprint(text: str, color: str = '', style: str = ''):
    if COLORS_AVAILABLE:
        print(f"{style}{color}{text}{Style.RESET_ALL}")
    else:
        print(text)

def find_best_float_in_text(text: str, min_val: float = -1e9, max_val: float = 1e9) -> Optional[float]:
    if not text:
        return None
    tokens = re.findall(r'\d+\.\d+|\d+', text)
    for t in tokens:
        try:
            v = float(t)
        except ValueError:
            continue
        if min_val <= v <= max_val:
            return v
    return None



async def call_deepseek_api_async(prompt: str, timeout: int = None, max_retries: int = None) -> str:
    if timeout is None:
        timeout = int(DEEPSEEK_TIMEOUT)
    if max_retries is None:
        max_retries = int(DEEPSEEK_RETRIES)

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}" if DEEPSEEK_API_KEY else ""
    }
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7,
        "max_tokens": 1000
    }

    backoff_base = 0.6
    last_err = None
    async with httpx.AsyncClient(timeout=timeout) as client:
        for attempt in range(1, max_retries + 1):
            try:
                resp = await client.post(DEEPSEEK_API_URL, headers=headers, json=payload)
                resp.raise_for_status()
                data = resp.json()
                if isinstance(data, dict):
                    choices = data.get("choices")
                    if choices and isinstance(choices, list) and len(choices) > 0:
                        first = choices[0]
                        if isinstance(first, dict):
                            msg = first.get("message") or first.get("text") or {}
                            if isinstance(msg, dict):
                                content = msg.get("content") or msg.get("text") or ""
                            elif isinstance(msg, str):
                                content = msg
                            else:
                                content = ""
                        else:
                            content = str(first)
                        return content.strip()
                return resp.text
            except httpx.HTTPStatusError as e:
                last_err = str(e)
                status = getattr(e.response, "status_code", None)
                logger.error("DeepSeek HTTP error (attempt %d): %s", attempt, e)
                if status and status < 500 and status != 429:
                    break
            except Exception as e:
                last_err = str(e)
                logger.error("DeepSeek request failed (attempt %d): %s", attempt, e)
            await asyncio.sleep(backoff_base * attempt)
    raise RuntimeError(f"DeepSeek API calls failed: {last_err}")

EXPECTED_TOP_LEVEL_SECTIONS = [
    "match", "league_standings", "data_comparison_recent10", "lineup_and_injuries",
    "last_match_player_ratings", "recent10_ratings_parsed", "future_matches",
    "head_to_head_sample", "league_trend_and_other_stats"
]

def normalize_parsed_data(parsed: Dict[str, Any]) -> Dict[str, Any]:
    merged = dict(parsed)
    if isinstance(parsed.get("sections"), dict):
        for k, v in parsed["sections"].items():
            if k not in merged or merged.get(k) is None:
                merged[k] = v

    normalized: Dict[str, Any] = {}
    missing: List[str] = []
    available: List[str] = []

    for key in EXPECTED_TOP_LEVEL_SECTIONS:
        val = merged.get(key)
        if val is None:
            normalized[key] = None
            missing.append(key)
        else:
            normalized[key] = val
            available.append(key)

    match_block = merged.get("match") or {}
    normalized["match"] = {
        "home_team": match_block.get("home_team") or merged.get("home_team"),
        "away_team": match_block.get("away_team") or merged.get("away_team"),
        "competition": match_block.get("competition") or merged.get("competition"),
        "datetime": match_block.get("datetime") or merged.get("datetime"),
        "venue": match_block.get("venue") or merged.get("venue"),
    }

    rr = merged.get("recent10_ratings_parsed") or {}
    normalized["recent10_ratings_parsed"] = {
        "home_recent_ratings_raw": rr.get("home_recent_ratings_raw"),
        "home_recent_ratings": list(rr.get("home_recent_ratings") or []),
        "home_recent_average": rr.get("home_recent_average") or merged.get("home_rating"),
        "away_recent_ratings_raw": rr.get("away_recent_ratings_raw"),
        "away_recent_ratings": list(rr.get("away_recent_ratings") or []),
        "away_recent_average": rr.get("away_recent_average") or merged.get("away_rating"),
    }

    section_counts = {}
    for key in EXPECTED_TOP_LEVEL_SECTIONS:
        v = normalized.get(key)
        if isinstance(v, list):
            section_counts[key] = len(v)
        elif isinstance(v, dict):
            section_counts[key] = len(v.keys())
        elif v is None:
            section_counts[key] = 0
        else:
            section_counts[key] = 1

    normalized["_meta"] = {
        "missing_fields": missing,
        "available_sections": available,
        "section_counts": section_counts
    }
    return normalized

def has_meaningful_data_for_ai(normalized_stats: Dict[str, Any]) -> bool:
    meta = normalized_stats.get("_meta", {})
    available = meta.get("available_sections", [])
    if available:
        return True
    rr = normalized_stats.get("recent10_ratings_parsed") or {}
    if rr.get("home_recent_ratings") or rr.get("away_recent_ratings") or rr.get("home_recent_average") or rr.get("away_recent_average"):
        return True
    return False

def build_ai_prompt_with_availability(normalized_data: Dict[str, Any], use_chinese: bool = True) -> str:
    meta = normalized_data.get("_meta", {})
    available = meta.get("available_sections", [])
    missing = meta.get("missing_fields", [])

    def render_section(name: str, max_chars: int = 800) -> str:
        sec = normalized_data.get(name)
        if not sec:
            return ""
        try:
            s = json.dumps(sec, ensure_ascii=False, indent=0)
        except Exception:
            s = str(sec)
        return s[:max_chars] + ("..." if len(s) > max_chars else "")

    available_str = ", ".join(available) if available else "none"
    missing_str = ", ".join(missing) if missing else "none"

    excerpts = ""
    for sec_name in ("recent10_ratings_parsed", "league_standings", "data_comparison_recent10",
                     "last_match_player_ratings", "lineup_and_injuries"):
        excerpt = render_section(sec_name)
        if excerpt:
            excerpts += f"\n\n=== {sec_name} ===\n{excerpt}"

    if use_chinese:
        header = (
            f"ä½ æ˜¯ä¸€ä½è³‡æ·±è¶³çƒåšå½©åˆ†æå¸«ã€‚ä»¥ä¸‹è³‡æ–™å·²ç”±çˆ¬èŸ²è§£æå’Œæ¨™æº–åŒ–ï¼Œç³»çµ±èªªæ˜å“ªäº›æ¬„ä½å­˜åœ¨æˆ–éºæ¼ã€‚\n"
            f"Available sections: {available_str}\n"
            f"Missing sections: {missing_str}\n\n"
            "è«‹åŸºæ–¼å¯ç”¨æ•¸æ“šï¼ˆè‹¥æŸäº›æ¬„ä½éºå¤±ï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºï¼‰æ¨è–¦ä¸€å€‹æœ€æœ‰åƒ¹å€¼çš„æŠ•æ³¨é¸é …ï¼Œä¸¦ä»¥JSONæ ¼å¼å›è¦†ï¼Œ"
            "åƒ…åŒ…å«å¦‚ä¸‹å­—æ®µï¼š\n"
            '{ "best_bet_market": "æŠ•æ³¨å¸‚å ´", "best_bet_selection": "å…·é«”é¸æ“‡", "confidence_level": "1-10", "brief_reasoning": "ç°¡çŸ­åŸå› " }\n\n'
            "åªè¼¸å‡ºJSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"
        )
    else:
        header = (
            f"You are an experienced football betting analyst. The parser produced the following available/missing sections.\n"
            f"Available: {available_str}\nMissing: {missing_str}\n\n"
            "Based on available data (explicitly note missing fields if they matter), recommend a single best bet in JSON:\n"
            '{ "best_bet_market": "...", "best_bet_selection": "...", "confidence_level": "1-10", "brief_reasoning": "..." }\n'
            "Output only JSON."
        )

    prompt = header + excerpts
    return prompt

def parse_ai_json_response(text: str) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
    if not text:
        return None, None
    try:
        s = text.strip()
        if s.startswith("{") and s.endswith("}"):
            return json.loads(s), s
        m = re.search(r'(\{(?:.|\s)*\})', text)
        if m:
            candidate = m.group(1)
            return json.loads(candidate), candidate
    except Exception as e:
        logger.debug("AI JSON parse failed: %s", e)
    try:
        fallback = {}
        market = re.search(r'"?best_bet_market"?\s*[:=]\s*"([^"]+)"', text)
        selection = re.search(r'"?best_bet_selection"?\s*[:=]\s*"([^"]+)"', text)
        confidence = re.search(r'"?confidence_level"?\s*[:=]\s*([0-9]+)', text)
        reasoning = re.search(r'"?brief_reasoning"?\s*[:=]\s*"([^"]+)"', text)
        if market:
            fallback["best_bet_market"] = market.group(1)
        if selection:
            fallback["best_bet_selection"] = selection.group(1)
        if confidence:
            fallback["confidence_level"] = int(confidence.group(1))
        if reasoning:
            fallback["brief_reasoning"] = reasoning.group(1)
        if fallback:
            return fallback, json.dumps(fallback, ensure_ascii=False)
    except Exception:
        pass
    return None, None

async def perform_ai_analysis_for_match_async(
    normalized_stats: Dict[str, Any],
    call_deepseek_api_async_fn,
    max_retries: int = 2,
    short_circuit_when_no_data: bool = True
) -> Dict[str, Any]:
    result = {
        "best_bet_market": "No Data",
        "best_bet_selection": "No Analysis Available",
        "confidence_level": 0,
        "brief_reasoning": "Insufficient statistical data available for analysis.",
        "ai_raw_response": None,
        "ai_parsed_json": None,
        "data_availability": normalized_stats.get("_meta", {})
    }

    if short_circuit_when_no_data and not has_meaningful_data_for_ai(normalized_stats):
        logger.info("Short-circuiting AI call: no meaningful sections or ratings present")
        return result

    prompt = build_ai_prompt_with_availability(normalized_stats, use_chinese=True)
    if DEBUG_INSTRUMENTATION_AVAILABLE:
        try:
            save_parsed_json("ai_prompt", normalized_stats.get("match", {}).get("home_team", "unknown"), {"prompt": prompt[:4000]})
            log_info("AI prompt built", {"available_sections": normalized_stats.get("_meta", {})})
        except Exception:
            pass

    last_err = None
    for attempt in range(1, max_retries + 1):
        try:
            ai_text = await call_deepseek_api_async_fn(prompt)
            result["ai_raw_response"] = ai_text
            if DEBUG_INSTRUMENTATION_AVAILABLE:
                try:
                    save_parsed_json("ai_response_raw", normalized_stats.get("match", {}).get("home_team", "unknown"), {"raw": ai_text[:4000]})
                except Exception:
                    pass
            parsed, raw = parse_ai_json_response(ai_text)
            if parsed:
                required = ["best_bet_market", "best_bet_selection", "confidence_level", "brief_reasoning"]
                if all(k in parsed for k in required):
                    result.update({
                        "best_bet_market": parsed.get("best_bet_market"),
                        "best_bet_selection": parsed.get("best_bet_selection"),
                        "confidence_level": parsed.get("confidence_level"),
                        "brief_reasoning": parsed.get("brief_reasoning"),
                        "ai_parsed_json": parsed
                    })
                    return result
                else:
                    result["ai_parsed_json"] = parsed
                    result["brief_reasoning"] = "AI returned partial result; missing keys"
                    result["confidence_level"] = parsed.get("confidence_level", 1)
                    return result
            logger.warning("AI response contained no parsable JSON (attempt %d).", attempt)
            last_err = "No JSON in response"
            await asyncio.sleep(0.6 * attempt)
        except Exception as e:
            last_err = str(e)
            logger.error("AI call attempt %d failed: %s", attempt, e)
            await asyncio.sleep(0.6 * attempt)
    result["brief_reasoning"] = f"AI call failed: {last_err}"
    result["confidence_level"] = 0
    return result

AO_DISPLAY_ZH = {
    "HAD": "ä¸»å®¢å’Œ", "FHA": "åŠå ´ä¸»å®¢å’Œ", "HHA": "è®“çƒä¸»å®¢å’Œ", "HHA_Extra": "è®“çƒä¸»å®¢å’Œ",
    "HDC": "è®“çƒ", "HIL": "å…¥çƒå¤§ç´°", "FHL": "åŠå ´å…¥çƒå¤§ç´°",
    "CHL": "é–‹å‡ºè§’çƒå¤§ç´°", "FCH": "åŠå ´é–‹å‡ºè§’çƒå¤§ç´°",
    "CHD": "é–‹å‡ºè§’çƒè®“çƒ", "FHC": "åŠå ´é–‹å‡ºè§’çƒè®“çƒ",
    "CRS": "æ³¢è†½", "FCS": "åŠå ´æ³¢è†½", "FTS": "ç¬¬ä¸€éšŠå…¥çƒ",
    "TTG": "ç¸½å…¥çƒ", "OOE": "å…¥çƒå–®é›™", "HFT": "åŠå…¨å ´",
    "FGS": "é¦–åå…¥çƒ", "LGS": "æœ€å¾Œå…¥çƒçƒå“¡", "AGS": "ä»»ä½•æ™‚é–“å…¥çƒçƒå“¡",
    "MSP": "ç‰¹åˆ¥é …ç›®",
}

def ao_clean_text(el):
    return el.get_text(strip=True) if el else None

def ao_clean_odds_text(span):
    if not span:
        return None
    text = span.get_text(strip=True)
    cleaned = re.sub(r"[^\d.]", "", text)
    return float(cleaned) if cleaned else None

def ao_next_match_row_container(coupon):
    if not coupon:
        return None
    sib = coupon.find_next_sibling()
    while sib is not None and not ("match-row-container" in sib.get("class", [])):
        sib = sib.find_next_sibling()
    return sib

def ao_parse_allodds_match_header(soup):
    mi = soup.select_one(".match-info")
    if not mi:
        return {}
    match_id = ao_clean_text(mi.select_one(".match .val"))
    home = ao_clean_text(mi.select_one(".team .home"))
    away = ao_clean_text(mi.select_one(".team .away"))
    time_raw = ao_clean_text(mi.select_one(".time .val"))
    timg = mi.select_one(".matchInfoTourn img")
    tournament = timg["title"] if timg and timg.has_attr("title") else None
    return {
        "match_id": match_id,
        "home_team": home,
        "away_team": away,
        "tournament": tournament,
        "time_raw": time_raw,
    }

def ao_parse_had_like_from_row(row, odds_class):
    odds_block = row.select_one(f".odds.{odds_class}")
    if not odds_block:
        return None
    grids = odds_block.select(".oddsCheckboxGrid")
    if len(grids) < 3:
        return None
    co = ao_clean_odds_text
    return {
        "home_odds": co(grids[0].select_one(".add-to-slip")),
        "draw_odds": co(grids[1].select_one(".add-to-slip")),
        "away_odds": co(grids[2].select_one(".add-to-slip")),
    }

def ao_parse_hha_multi_lines_from_row(row):
    res = []
    odds_line = row.select_one(".oddsLine.HHA")
    if not odds_line:
        return res
    line_blocks = odds_line.select(".odds.show")
    for line_index, line_block in enumerate(line_blocks):
        items = line_block.select(".hdcOddsItem")
        if len(items) != 3:
            continue

        def cond(item):
            c = item.select_one(".cond")
            return ao_clean_text(c).strip("[]") if c else ""

        home_hcap = cond(items[0]); draw_hcap = cond(items[1]); away_hcap = cond(items[2])
        co = ao_clean_odds_text
        home_odds = co(items[0].select_one(".add-to-slip"))
        draw_odds = co(items[1].select_one(".add-to-slip"))
        away_odds = co(items[2].select_one(".add-to-slip"))
        give_home = None
        if home_hcap.startswith("-"):
            give_home = True
        elif away_hcap.startswith("-"):
            give_home = False
        else:
            if home_odds is not None and away_odds is not None:
                give_home = home_odds < away_odds
        market_type = "HHA" if line_index == 0 else "HHA_Extra"
        res.append({
            "market_type": market_type,
            "line_index": line_index + 1,
            "home_odds": home_odds, "draw_odds": draw_odds, "away_odds": away_odds,
            "euro_handicap_value": home_hcap,
            "euro_handicap_give_home": give_home,
        })
    return res

def ao_parse_hdc_from_row(row):
    odds_line = row.select_one(".oddsLine.HDC")
    if not odds_line:
        return None
    lb = odds_line.select_one(".odds.show")
    if not lb:
        return None
    items = lb.select(".hdcOddsItem")
    if len(items) != 2:
        return None

    def cond(item):
        c = item.select_one(".cond")
        return ao_clean_text(c).strip("[]") if c else ""

    home_hcap = cond(items[0]); away_hcap = cond(items[1])
    co = ao_clean_odds_text
    home_odds = co(items[0].select_one(".add-to-slip"))
    away_odds = co(items[1].select_one(".add-to-slip"))
    give_home = None
    if home_hcap.startswith("-"):
        give_home = True
    elif away_hcap.startswith("-"):
        give_home = False
    else:
        if home_odds is not None and away_odds is not None:
            give_home = home_odds < away_odds
    return {
        "asia_handicap_value": home_hcap,
        "asia_handicap_give_home": give_home,
        "home_odds": home_odds, "away_odds": away_odds,
    }

def ao_parse_ou_market_from_row(row, class_name, goal_field_name="goal_line"):
    odds_line = row.select_one(f".oddsLine.{class_name}")
    if not odds_line:
        return []
    res = []
    line_nums = odds_line.select(".lineNum.show")
    odds_blocks = odds_line.select(".odds.show")
    for line_idx, (ln, ob) in enumerate(zip(line_nums, odds_blocks)):
        line_text = ao_clean_text(ln).strip("[]") if ln else None
        grids = ob.select(".oddsCheckboxGrid")
        if len(grids) < 2:
            continue
        co = ao_clean_odds_text
        over_odds = co(grids[0].select_one(".add-to-slip"))
        under_odds = co(grids[1].select_one(".add-to-slip"))
        res.append({
            "line_index": line_idx + 1,
            goal_field_name: line_text,
            "over_odds": over_odds,
            "under_odds": under_odds,
        })
    return res

def ao_parse_crs_matrix(row):
    res = []
    for odds_cell in row.select(".crsTable .odds"):
        score = ao_clean_text(odds_cell.select_one(".crsSel"))
        odds = ao_clean_odds_text(odds_cell.select_one(".add-to-slip"))
        if score and odds is not None:
            res.append({"score": score, "odds": odds})
    return res

def ao_parse_fts(row):
    odds_block = row.select_one(".oddsFTS")
    if not odds_block:
        return None
    grids = odds_block.select(".oddsCheckboxGrid")
    if len(grids) < 3:
        return None
    co = ao_clean_odds_text
    return {
        "home_first": co(grids[0].select_one(".add-to-slip")),
        "no_goal": co(grids[1].select_one(".add-to-slip")),
        "away_first": co(grids[2].select_one(".add-to-slip")),
    }

def ao_parse_ttg(row):
    odds_block = row.select_one(".oddsTTG")
    if not odds_block:
        return []
    res = []
    for block in odds_block.find_all("div", recursive=False):
        goals = ao_clean_text(block.select_one(".goals-number"))
        odds = ao_clean_odds_text(block.select_one(".add-to-slip"))
        if goals and odds is not None:
            res.append({"goals": goals, "odds": odds})
    return res

def ao_parse_ooe(row):
    odds_block = row.select_one(".oddsOOE")
    if not odds_block:
        return None
    grids = odds_block.select(".oddsCheckboxGrid")
    if len(grids) < 2:
        return None
    co = ao_clean_odds_text
    return {
        "odd": co(grids[0].select_one(".add-to-slip")),
        "even": co(grids[1].select_one(".add-to-slip")),
    }

def ao_parse_hft(row):
    odds_block = row.select_one(".oddsHFT")
    if not odds_block:
        return []
    res = []
    for block in odds_block.find_all("div", recursive=False):
        label = ao_clean_text(block.select_one(".goals-number"))
        odds = ao_clean_odds_text(block.select_one(".add-to-slip"))
        if label and odds is not None:
            res.append({"combo": label, "odds": odds})
    return res

def ao_parse_scorer_market(row):
    res = []
    grids = row.select(".oddsCheckboxGrid")

    def pull_candidates(grid):
        cands = []
        for sib in grid.previous_siblings:
            if getattr(sib, "get_text", None):
                cands.append(ao_clean_text(sib))
        parent = grid.find_parent(["td", "div"])
        if parent:
            for sib in parent.find_previous_siblings():
                if getattr(sib, "get_text", None):
                    cands.append(ao_clean_text(sib))
        for prev in grid.find_all_previous(["div", "td", "th", "span"], limit=8):
            txt = ao_clean_text(prev)
            if txt:
                cands.append(txt)
        return cands

    for g in grids:
        odds = ao_clean_odds_text(g.find_next("span", class_="add-to-slip"))
        gid = g.get("id", "") or ""
        m = re.search(r"_(\d{3,})", gid)
        code = m.group(1) if m else None
        name = None
        for txt in pull_candidates(g):
            if not txt:
                continue
            m2 = re.search(r"\b(\d{3})\b\s*([A-Za-z\u4e00-\u9fff].+)", txt)
            if m2:
                if not code:
                    code = m2.group(1)
                name = m2.group(2).strip()
                break
        res.append({"player_code": code, "player_name": name, "odds": odds})
    return res

def ao_parse_msp(row):
    raw = row.get_text(" ", strip=True)
    if not raw:
        return []
    items = []
    parts = re.split(r"é …ç›®ç·¨è™Ÿ[:ï¼š]\s*", raw)
    for part in parts:
        part = part.strip()
        if not part:
            continue
        m_id = re.match(r"(\d+)", part)
        item_id = m_id.group(1) if m_id else None
        m_q = re.split(r"\(\d+\)", part, maxsplit=1)
        if len(m_q) == 2:
            question = m_q[0].strip()
            rest = "(" + m_q[1]
        else:
            question = None
            rest = part
        options = []
        for opt_num, label, odds in re.findall(r"\((\d+)\)\s*([^(]+?)\s+(\d+(?:\.\d+)?)", rest):
            options.append({"option": opt_num, "label": label.strip(), "odds": float(odds)})
        items.append({"item_id": item_id, "question": question, "options": options, "raw": part})
    if not items:
        odds_list = [ao_clean_odds_text(span) for span in row.select(".add-to-slip")]
        items.append({"raw": raw, "odds": [o for o in odds_list if o is not None]})
    return items

def ao_parse_allodds_from_html(html: str):
    soup = BeautifulSoup(html, "html.parser")
    match_meta = ao_parse_allodds_match_header(soup)
    markets = {}

    def add_display(code, obj):
        if obj is None:
            return None
        if isinstance(obj, dict):
            obj["display_name_zh"] = AO_DISPLAY_ZH.get(code, "")
        return obj

    def add_display_list(code, lst):
        if lst is None:
            return None
        return [item | {"display_name_zh": AO_DISPLAY_ZH.get(code, "")} for item in lst]

    for code, cls, parser in [
        ("HAD", "couponHAD", lambda r: ao_parse_had_like_from_row(r, "oddsHAD")),
        ("FHA", "couponFHA", lambda r: ao_parse_had_like_from_row(r, "oddsFHA")),
        ("HDC", "couponHDC", ao_parse_hdc_from_row),
        ("CHD", "couponCHD", ao_parse_hdc_from_row),
        ("FHC", "couponFHC", ao_parse_hdc_from_row),
    ]:
        coupon = soup.select_one(f".coupon.{cls}")
        if coupon:
            row = ao_next_match_row_container(coupon)
            if row:
                markets[code] = add_display(code, parser(row))

    coupon = soup.select_one(".coupon.couponHHA")
    if coupon:
        row = ao_next_match_row_container(coupon)
        if row:
            hha_lines = ao_parse_hha_multi_lines_from_row(row)
            for line in hha_lines:
                line["display_name_zh"] = AO_DISPLAY_ZH.get(line["market_type"], "")
                markets.setdefault(line["market_type"], []).append(line)

    for code, cls in [("HIL", "couponHIL"), ("FHL", "couponFHL"), ("CHL", "couponCHL"), ("FCH", "couponFCH")]:
        coupon = soup.select_one(f".coupon.{cls}")
        if coupon:
            row = ao_next_match_row_container(coupon)
            if row:
                markets[code] = add_display_list(code, ao_parse_ou_market_from_row(row, code))

    for code, cls in [("CRS", "couponCRS"), ("FCS", "couponFCS")]:
        coupon = soup.select_one(f".coupon.{cls}")
        if coupon:
            row = ao_next_match_row_container(coupon)
            if row:
                markets[code] = {"display_name_zh": AO_DISPLAY_ZH[code], "scores": ao_parse_crs_matrix(row)}

    coupon = soup.select_one(".coupon.couponFTS")
    if coupon:
        row = ao_next_match_row_container(coupon)
        if row:
            markets["FTS"] = add_display("FTS", ao_parse_fts(row))
    coupon = soup.select_one(".coupon.couponTTG")
    if coupon:
        row = ao_next_match_row_container(coupon)
        if row:
            markets["TTG"] = {"display_name_zh": AO_DISPLAY_ZH["TTG"], "buckets": ao_parse_ttg(row)}
    coupon = soup.select_one(".coupon.couponOOE")
    if coupon:
        row = ao_next_match_row_container(coupon)
        if row:
            markets["OOE"] = add_display("OOE", ao_parse_ooe(row))
    coupon = soup.select_one(".coupon.couponHFT")
    if coupon:
        row = ao_next_match_row_container(coupon)
        if row:
            markets["HFT"] = {"display_name_zh": AO_DISPLAY_ZH["HFT"], "combos": ao_parse_hft(row)}

    for code, cls in [("FGS","couponFGS"), ("LGS","couponLGS"), ("AGS","couponAGS")]:
        coupon = soup.select_one(f".coupon.{cls}")
        if coupon:
            row = ao_next_match_row_container(coupon)
            if row:
                markets[code] = {"display_name_zh": AO_DISPLAY_ZH[code], "players": ao_parse_scorer_market(row)}

    coupon = soup.select_one(".coupon.couponMSP")
    if coupon:
        row = ao_next_match_row_container(coupon)
        if row:
            markets["MSP"] = {"display_name_zh": AO_DISPLAY_ZH["MSP"], "items": ao_parse_msp(row)}

    return match_meta, markets

class HKJCDetailedOddsScraper:
    def __init__(self, output_dir: Path = Path("hkjc/odds")):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

    async def scrape(self, event_id: str) -> Optional[Dict[str, Any]]:
        url = f"https://bet.hkjc.com/ch/football/allodds/{event_id}"
        logger.info("ğŸŒ Scraping HKJC All Odds for event: %s", event_id)
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/118.0.5993.117 Safari/537.36"
                )
            )
            page = await context.new_page()
            try:
                await page.goto(url, wait_until="domcontentloaded", timeout=90000)
                try:
                    await page.wait_for_selector(".match-info", timeout=30000)
                except Exception:
                    logger.warning("HKJC detailed odds: .match-info not found for %s", event_id)
                html = await page.content()
            except Exception as e:
                logger.error("Error scraping HKJC detailed odds %s: %s", event_id, e)
                await browser.close()
                return None
            await browser.close()

        try:
            match_meta, markets = ao_parse_allodds_from_html(html)
            out_data = {
                "created_at": datetime.now().isoformat(timespec="seconds"),
                "event_id": event_id,
                "url": url,
                "match": match_meta,
                "markets": markets,
            }
            out_path = self.output_dir / f"hkjc_odds_{event_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            out_path.write_text(json.dumps(out_data, ensure_ascii=False, indent=2), encoding="utf-8")
            return out_data
        except Exception as e:
            logger.error("Error parsing HKJC detailed odds %s: %s", event_id, e)
            return None

class HKJCHomeScraper:
    BET_HOME = "https://bet.hkjc.com/ch/football/home"
    ROWS_SEL = ".match-row,.event-row"
    DT_FORMAT = "%d/%m/%Y %H:%M"
    CLICK_WAIT = 4.0
    TIMEOUT_MS = 9000
    HEADLESS = True

    async def safe_goto(self, page, url: str, max_attempts: int = 3) -> None:
        for attempt in range(1, max_attempts + 1):
            try:
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                with contextlib.suppress(Exception):
                    await page.wait_for_load_state("networkidle", timeout=15000)
                return
            except Exception as e:
                if attempt == max_attempts:
                    raise
                logger.warning("Goto attempt %d failed (%s); retrying...", attempt, e)
                await asyncio.sleep(2 * attempt)

    @staticmethod
    def parse_row_start(txt: str) -> Optional[datetime]:
        try:
            return datetime.strptime(txt.strip(), HKJCHomeScraper.DT_FORMAT)
        except Exception:
            return None

    @staticmethod
    def extract_id_from_url(url: str) -> Optional[str]:
        m = re.search(r"/allodds/(\d+)", url or "")
        return m.group(1) if m else None

    async def get_declared(self, page) -> Optional[int]:
        html = await page.content()
        m = re.search(r"å…±æœ‰\s*(\d+)\s*å ´è³½äº‹", html)
        return int(m.group(1)) if m else None

    async def scroll_bottom(self, page):
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await asyncio.sleep(1.0)

    async def click_show_more(self, page) -> bool:
        xps = ["//*[contains(text(), 'é¡¯ç¤ºæ›´å¤š')]"]
        for xp in xps:
            els = await page.query_selector_all(f"xpath={xp}")
            for el in els:
                if await el.is_visible():
                    await el.click(force=True)
                    logger.info("AUTO-CLICKED 'é¡¯ç¤ºæ›´å¤š'")
                    return True
        return False

    async def scrape(self, fast_skip_if_cache_sufficient: bool = False, cached_count: int = 0) -> List[Dict[str, Any]]:
        results: List[Dict[str, Any]] = []
        collected: set[str] = set()
        now = datetime.now()

        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=self.HEADLESS,
                args=["--disable-blink-features=AutomationControlled"]
            )
            ctx = await browser.new_context()
            page = await ctx.new_page()
            page.set_default_timeout(self.TIMEOUT_MS)

            mod_key = ["Meta"] if sys.platform == "darwin" else ["Control"]

            await self.safe_goto(page, self.BET_HOME, max_attempts=3)
            await asyncio.sleep(4.0)

            declared = await self.get_declared(page)
            logger.info("Declared: %s matches", declared or "unknown")

            # Fast-skip: if we already have enough cached odds, skip per-row clicking
            if fast_skip_if_cache_sufficient and declared and cached_count >= declared:
                logger.info(
                    "Fast-skip HKJC row scan: declared=%d, cached_odds=%d (>=), skipping row clicks.",
                    declared, cached_count
                )
                await browser.close()
                return results  # empty list -> collector will see zero new IDs

            await self.scroll_bottom(page)
            await self.click_show_more(page)
            await asyncio.sleep(4.0)

            total = await page.locator(self.ROWS_SEL).count()
            logger.info("Total rows: %d â€” starting scrape", total)

            for i in range(total):
                row = page.locator(self.ROWS_SEL).nth(i)

                rid = ""
                with contextlib.suppress(Exception):
                    rid = await row.locator(".fb-id").first.text_content(timeout=5000)
                    rid = rid.strip()
                if not rid:
                    with contextlib.suppress(Exception):
                        rid = await row.get_attribute("id") or ""
                        rid = rid.strip()
                if not rid:
                    code_loc = row.locator("td:has-text('FB'), div:has-text('FB')").first
                    with contextlib.suppress(Exception):
                        code_text = await code_loc.text_content(timeout=8000)
                        m = re.search(r'(HAD_)?FB\d{4,}', (code_text or "").strip(), re.IGNORECASE)
                        if m:
                            rid = m.group(0).upper()
                rid = rid.strip()
                if not rid:
                    cprint(f"[HKJC rows] {i+1}/{total} no code â€” re-clicking 'é¡¯ç¤ºæ›´å¤š'", Fore.YELLOW)
                    await self.scroll_bottom(page)
                    if await self.click_show_more(page):
                        await asyncio.sleep(4.0)
                    row = page.locator(self.ROWS_SEL).nth(i)
                    with contextlib.suppress(Exception):
                        rid = await row.locator(".fb-id").first.text_content(timeout=5000)
                        rid = (rid or "").strip()
                    if not rid:
                        with contextlib.suppress(Exception):
                            rid = await row.get_attribute("id") or ""
                            rid = rid.strip()
                    if not rid:
                        code_loc = row.locator("td:has-text('FB'), div:has-text('FB')").first
                        with contextlib.suppress(Exception):
                            code_text = await code_loc.text_content(timeout=8000)
                            m = re.search(r'(HAD_)?FB\d{4,}', (code_text or "").strip(), re.IGNORECASE)
                            if m:
                                rid = m.group(0).upper()
                    if not rid:
                        cprint(f"[HKJC rows] {i+1}/{total} still no code â€” skipping row", Fore.YELLOW)
                        await self.safe_goto(page, self.BET_HOME, max_attempts=3)
                        await asyncio.sleep(3.0)
                        continue

                cprint(f"[HKJC rows] {i+1}/{total} START (id={rid})", Fore.CYAN)

                dt_txt = ""
                with contextlib.suppress(Exception):
                    dt_txt = await row.locator(".date").first.text_content(timeout=8000)
                start_dt = self.parse_row_start(dt_txt or "")
                if start_dt and start_dt <= now:
                    cprint(f"[HKJC rows] {i+1}/{total} {rid} SKIPPED (already started)", Fore.YELLOW)
                    await self.safe_goto(page, self.BET_HOME, max_attempts=3)
                    await asyncio.sleep(3.0)
                    continue

                trigger = None
                for sel in [
                    '[title*="è³ ç‡"]',
                    '[title*="æ‰€æœ‰è³ ç‡"]',
                    ".teamIconSmall [title]",
                    ".teamIconSmall",
                    ".team",
                ]:
                    cand = row.locator(sel)
                    if await cand.count():
                        trigger = cand.first
                        cprint(f"[HKJC rows] {i+1}/{total} {rid} trigger: {sel}", Fore.BLUE)
                        break
                if not trigger:
                    cprint(f"[HKJC rows] {i+1}/{total} {rid} NO TRIGGER â€” skip", Fore.YELLOW)
                    await self.safe_goto(page, self.BET_HOME, max_attempts=3)
                    await asyncio.sleep(3.0)
                    continue

                await trigger.click(modifiers=mod_key, force=True)
                await asyncio.sleep(self.CLICK_WAIT)

                eid = self.extract_id_from_url(page.url)
                if eid:
                    collected.add(eid)
                    cprint(f"[HKJC rows] {i+1}/{total} {rid} CAPTURED {eid}", Fore.GREEN)
                else:
                    cprint(f"[HKJC rows] {i+1}/{total} {rid} NO ID captured", Fore.YELLOW)

                await self.safe_goto(page, self.BET_HOME, max_attempts=3)
                await asyncio.sleep(3.0)

            ids_sorted = sorted(collected, key=int)
            logger.info("HKJC home scraper collected %d event ids (unique)", len(ids_sorted))
            for eid in ids_sorted:
                results.append({"event_id": eid, "home_team": None, "away_team": None, "raw_text": ""})

            if declared and len(ids_sorted) != declared:
                cprint(f"[HKJC] Declared {declared}, collected {len(ids_sorted)}", Fore.YELLOW)
                logger.warning("Declared %d, collected %d", declared, len(ids_sorted))

            await browser.close()

        return results

class HKJCBulkOddsCollector:
    def __init__(self, home_scraper: HKJCHomeScraper, detailed_scraper: HKJCDetailedOddsScraper,
                 existing_cache: Optional[Dict[str, Any]] = None, skip_ids: Optional[set] = None):
        self.home_scraper = home_scraper
        self.detailed_scraper = detailed_scraper
        self.existing_cache = existing_cache or {}
        self.skip_ids = set(str(x) for x in (skip_ids or []))

    async def collect(self, max_events: Optional[int] = None, concurrency: int = 4, force_rescrape: bool = False,
                      fast_skip_if_cache_sufficient: bool = True) -> Tuple[Dict[str, Dict[str, Any]], set, List[Dict[str, Any]]]:
        # cached_count = odds already present (existing_cache or processed)
        cached_count = len(set(self.existing_cache.keys()) | set(self.skip_ids))

        home_rows = await self.home_scraper.scrape(
            fast_skip_if_cache_sufficient=fast_skip_if_cache_sufficient,
            cached_count=cached_count
        )
        all_ids = [str(r["event_id"]) for r in home_rows if r.get("event_id")]

        if force_rescrape:
            event_ids = all_ids
            skipped_reason = "force_rescrape=True (ignoring cache)"
        else:
            event_ids = [
                r for r in all_ids
                if r not in self.skip_ids and r not in self.existing_cache
            ]
            skipped_reason = "cached in skip_ids/existing_cache"

        if max_events is not None:
            event_ids = event_ids[:max_events]
        skipped_ids = [eid for eid in all_ids if eid not in event_ids]

        logger.info("HKJC skip cache: processed=%d, existing_cache=%d", len(self.skip_ids), len(self.existing_cache))
        if self.skip_ids:
            logger.info("HKJC skip cache sample: %s", list(sorted(self.skip_ids))[:10])
        if self.existing_cache:
            logger.info("HKJC existing cache sample: %s", list(sorted(self.existing_cache.keys()))[:10])
        cached_ids = [eid for eid in all_ids if (eid in self.skip_ids or eid in self.existing_cache)]
        if cached_ids:
            logger.info("HKJC will skip %d ids already cached; sample: %s", len(cached_ids), cached_ids[:10])

        logger.info(
            "Bulk HKJC odds: found %d ids; using %d%s; skipped %d (%s)",
            len(all_ids),
            len(event_ids),
            f", cap={max_events}" if max_events is not None else " (no cap)",
            len(skipped_ids),
            skipped_reason,
        )
        if skipped_ids:
            logger.info("  Sample skipped ids: %s", skipped_ids[:10])
        if event_ids:
            logger.info("  Sample to-scrape ids: %s", event_ids[:10])

        sem = asyncio.Semaphore(concurrency)
        results: Dict[str, Dict[str, Any]] = dict(self.existing_cache)
        failed: List[str] = []

        async def worker(eid: str):
            nonlocal results, failed
            async with sem:
                try:
                    data = await self.detailed_scraper.scrape(eid)
                    if data:
                        results[eid] = data
                        self.skip_ids.add(eid)
                except Exception as e:
                    logger.warning("Bulk HKJC odds failed for %s: %s", eid, e)
                    failed.append(eid)

        await asyncio.gather(*(worker(eid) for eid in event_ids))
        out_dir = self.detailed_scraper.output_dir
        out_path = out_dir / f"hkjc_allodds_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        payload = {
            "metadata": {
                "scraped_at": datetime.now().isoformat(),
                "source": "HKJC",
                "total_event_ids": len(event_ids),
                "succeeded": len(results),
                "failed": failed,
            },
            "events": list(results.values())
        }
        out_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
        logger.info("ğŸ’¾ Saved HKJC all odds to: %s (events=%d)", out_path, len(results))
        return results, self.skip_ids, home_rows





class MacauSlotOddsScraper:
    def __init__(self):
        self.base_url = "https://www.macau-slot.com/content/soccer/coming_bet.html"
        self.output_dir = Path("macauslot/odds")
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def load_latest_from_disk(self) -> Optional[Dict[str, Any]]:
        if not self.output_dir.exists():
            return None
        files = sorted(self.output_dir.glob("macauslot_odds_*.json"), reverse=True)
        for f in files:
            try:
                data = json.loads(f.read_text(encoding="utf-8"))
                if isinstance(data, dict) and isinstance(data.get("matches"), list) and data["matches"]:
                    logger.info("âœ… Reusing Macau odds from %s (matches=%d)", f, len(data["matches"]))
                    return data
            except Exception:
                continue
        return None

    async def _scrape_page_data_js(self, page) -> List[Dict]:
        try:
            return await page.evaluate("""() => {
                const matches = [];
                const containers = document.querySelectorAll('li.msl-ls-item, li.msl-odds-tr');

                containers.forEach(container => {
                    const eventId = container.getAttribute('data-ev-id');
                    if (!eventId) return;

                    const timeElem = container.querySelector('.minute');
                    const homeTeamElem = container.querySelector('.msl-odd-td-host');
                    const awayTeamElem = container.querySelector('.msl-odd-td-guest');
                    const flagWrap = container.querySelector('.msl-flag-wrap');

                    const home = homeTeamElem ? homeTeamElem.textContent.trim() : '';
                    const away = awayTeamElem ? awayTeamElem.textContent.trim() : '';
                    if (!home || !away) return;

                    const match = {
                        event_id: eventId,
                        time: timeElem ? timeElem.textContent.trim() : '',
                        competition: flagWrap ? (flagWrap.getAttribute('data-original-title') || '').trim() : '',
                        competition_short: flagWrap ? ((flagWrap.querySelector('.short') || {}).textContent || '').trim() : '',
                        home_team: home,
                        away_team: away,
                        odds: {
                            asian_handicap: [],
                            over_under: [],
                            home_draw_away: { home_odds: null, draw_odds: null, away_odds: null }
                        }
                    };

                    const oddsWrapper = container.querySelector('.msl-cm-odds-wrapper');
                    if (!oddsWrapper) {
                        matches.push(match);
                        return;
                    }

                    const stdCol = oddsWrapper.querySelector('.msl-odds-td.col-3.msl-odd-btn-bets') ||
                                   oddsWrapper.querySelector('.msl-odds-td.col-3');
                    if (stdCol) {
                        const buttons = stdCol.querySelectorAll('button.msl-bet');
                        buttons.forEach(btn => {
                            const sideBadge = btn.querySelector('.badge_front');
                            const oddsBadge = btn.querySelector('.badge');
                            if (!sideBadge || !oddsBadge) return;

                            const side = sideBadge.textContent.trim();
                            const oddsText = oddsBadge.textContent.trim();
                            const m = oddsText.match(/[\\d.]+/);
                            const odds = m ? parseFloat(m[0]) : null;
                            if (!odds) return;

                            if (side === 'ä¸»') match.odds.home_draw_away.home_odds = odds;
                            else if (side === 'å’Œ') match.odds.home_draw_away.draw_odds = odds;
                            else if (side === 'å®¢') match.odds.home_draw_away.away_odds = odds;
                        });
                    }

                    const ahSections = oddsWrapper.querySelectorAll(
                        '.msl-odds-td.col-1, .msl-odds-td.msl-odd-td-oddstype.col-1'
                    );
                    ahSections.forEach(section => {
                        const buttons = section.querySelectorAll('button.msl-bet');
                        buttons.forEach(btn => {
                            const sideBadge = btn.querySelector('.badge_left');
                            const lineBadge = btn.querySelector('.badge_front');
                            const oddsBadge = btn.querySelector('.badge');
                            if (!sideBadge || !lineBadge || !oddsBadge) return;

                            const side = sideBadge.textContent.trim();
                            const line = lineBadge.textContent.trim();
                            const oddsText = oddsBadge.textContent.trim();
                            const m = oddsText.match(/[\\d.]+/);
                            const odds = m ? parseFloat(m[0]) : null;
                            if (!odds) return;

                            let entry = match.odds.asian_handicap.find(x => x.handicap_value === line);
                            if (!entry) {
                                entry = { handicap_value: line, home_odds: null, away_odds: null };
                                match.odds.asian_handicap.push(entry);
                            }
                            if (side === 'ä¸»') entry.home_odds = odds;
                            else if (side === 'å®¢') entry.away_odds = odds;
                        });
                    });

                    const ouSections = oddsWrapper.querySelectorAll(
                        '.msl-odds-td.col-2, .msl-odds-td.msl-odd-td-oddstype.col-2'
                    );
                    ouSections.forEach(section => {
                        const buttons = section.querySelectorAll('button.msl-bet');
                        buttons.forEach(btn => {
                            const sideBadge = btn.querySelector('.badge_left');
                            const lineBadge = btn.querySelector('.badge_front');
                            const oddsBadge = btn.querySelector('.badge');
                            if (!sideBadge || !lineBadge || !oddsBadge) return;

                            const side = sideBadge.textContent.trim();
                            const line = lineBadge.textContent.trim();
                            const oddsText = oddsBadge.textContent.trim();
                            const m = oddsText.match(/[\\d.]+/);
                            const odds = m ? parseFloat(m[0]) : null;
                            if (!odds) return;

                            let entry = match.odds.over_under.find(x => x.goal_line === line);
                            if (!entry) {
                                entry = { goal_line: line, over_odds: null, under_odds: null };
                                match.odds.over_under.push(entry);
                            }
                            if (side === 'ä¸Š') entry.over_odds = odds;
                            else if (side === 'ä¸‹') entry.under_odds = odds;
                        });
                    });

                    matches.push(match);
                });

                return matches;
            }""")
        except Exception as e:
            logger.error("âš ï¸ JS scrape failed: %s", e)
            return []

    async def scrape_with_logging(self, max_pages: int = 20, skip_ids: Optional[set] = None) -> List[Dict]:
        logger.info("ğŸŒ Scraping Macau Slot live odds...")
        skip_ids = skip_ids or set()
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64)')
            page = await context.new_page()
            try:
                goto_retries = 3
                for attempt in range(1, goto_retries + 1):
                    try:
                        await page.goto(self.base_url, wait_until="domcontentloaded", timeout=30000)
                        break
                    except Exception as e:
                        logger.error("Macau page.goto failed (attempt %d/%d): %s", attempt, goto_retries, e)
                        if attempt == goto_retries:
                            raise
                        await asyncio.sleep(1.5 * attempt)

                await asyncio.sleep(3)
                all_matches = []
                for page_num in range(1, max_pages + 1):
                    if page_num > 1:
                        btn = await page.query_selector(f'input.msl-menu-page[value="{page_num}"]')
                        if btn and await btn.is_visible():
                            await btn.click()
                            await asyncio.sleep(1.5)
                        else:
                            break
                    page_data = await self._scrape_page_data_js(page)
                    logger.info("Macau page %d: raw items=%d", page_num, len(page_data or []))
                    if page_data:
                        filtered = [m for m in page_data if m.get("event_id") not in skip_ids]
                        all_matches.extend(filtered)
                        logger.info("Macau page %d: kept %d (skipped %d)", page_num, len(filtered), len(page_data) - len(filtered))
                    else:
                        break
                await browser.close()
                return all_matches
            except Exception as e:
                logger.exception("âŒ Macau scrape error: %s", e)
                await browser.close()
                return []

    def save_to_json(self, data: List[Dict], filename: Optional[str] = None) -> str:
        if not filename:
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = self.output_dir / f"macauslot_odds_{ts}.json"
        else:
            filename = Path(filename)
        filename.parent.mkdir(parents=True, exist_ok=True)
        out = {
            "metadata": {
                "scraped_at": datetime.now().isoformat(),
                "source": "MacauSlot",
                "url": self.base_url,
                "total_matches": len(data)
            },
            "matches": data
        }
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(out, f, ensure_ascii=False, indent=2)
        logger.info("ğŸ’¾ Saved Macau odds to: %s", filename)
        return str(filename)

class LiveMatchMatcher:
    def __init__(self, min_similarity_threshold: float = 0.70, time_tolerance_minutes: int = 30,
                 prioritize_similarity: bool = True,
                 hk_titan_time_tolerance: int = 45,
                 titan_macau_time_tolerance: int = 10):
        self.matched_games: List[Dict[str, Any]] = []
        self.unmatched_games: List[Dict[str, Any]] = []
        self.min_similarity_threshold = min_similarity_threshold
        self.time_tolerance_minutes = time_tolerance_minutes
        self.hk_titan_time_tolerance = hk_titan_time_tolerance
        self.titan_macau_time_tolerance = titan_macau_time_tolerance
        self.time_assisted_threshold = 0.50
        self.macau_name_only_threshold = 0.80
        self.prioritize_similarity = prioritize_similarity
        self.data_quality_metrics = {
            "total_hkjc_matches": 0,
            "total_titan_matches": 0,
            "potential_matches_checked": 0,
            "high_confidence_matches": 0,
            "low_confidence_matches": 0
        }
        self.raw_hkjc_matches = []
        self.raw_titan_matches = []
        self.macau_mapping = {}
        self.ai_cache = load_ai_cache()

        disk_odds = load_hkjc_odds_from_disk(Path("hkjc/odds"))
        self.hkjc_bulk_odds: Dict[str, Dict[str, Any]] = disk_odds
        self.hkjc_odds_processed: set = load_cache_set(HKJC_ODDS_PROCESSED_PATH, as_str=True)
        self.hkjc_odds_processed |= set(disk_odds.keys())

        self.titan_stats_processed: set = load_cache_set(TITAN_STATS_PROCESSED_PATH, as_str=True)

    def normalize_time(self, time_str: str) -> Optional[datetime]:
        if not time_str:
            return None
        s = time_str.strip()
        now = datetime.now()
        formats = [
            "%d/%m/%Y %H:%M",
            "%d/%m %H:%M",
            "%m/%d/%Y %H:%M",
            "%m/%d %H:%M",
            "%Y-%m-%d %H:%M",
        ]
        for fmt in formats:
            try:
                dt = datetime.strptime(s, fmt)
                if fmt in ("%d/%m %H:%M", "%m/%d %H:%M"):
                    dt = dt.replace(year=now.year)
                return dt
            except Exception:
                continue
        if re.match(r'^\d{1,2}:\d{2}$', s):
            try:
                h, m = map(int, s.split(":"))
                return datetime(now.year, now.month, now.day, h, m)
            except Exception:
                pass
        try:
            return datetime.fromisoformat(s)
        except Exception:
            logger.debug("normalize_time failed for %s", time_str)
            return None

    def is_within_tolerance(self, t1: Optional[datetime], t2: Optional[datetime], tol_min: int) -> bool:
        if not t1 or not t2:
            return False
        return abs((t1 - t2).total_seconds()) <= tol_min * 60

    def are_teams_similar_enough(self, hkjc_home: str, hkjc_away: str,
                                 titan_home: str, titan_away: str) -> Tuple[bool, float, bool]:
        home_sim = name_similarity(hkjc_home, titan_home)
        away_sim = name_similarity(hkjc_away, titan_away)
        home_swapped = name_similarity(hkjc_home, titan_away)
        away_swapped = name_similarity(hkjc_away, titan_home)
        best_direct = (home_sim + away_sim) / 2
        best_swapped = (home_swapped + away_swapped) / 2
        if best_direct >= self.min_similarity_threshold:
            return True, best_direct, False
        if best_swapped >= self.min_similarity_threshold:
            return True, best_swapped, True
        return False, max(best_direct, best_swapped), best_swapped > best_direct

    def validate_match_data(self, match_data: Dict) -> bool:
        required_fields = ['home_team', 'away_team']
        for f in required_fields:
            if not match_data.get(f) or len(str(match_data[f]).strip()) < 2:
                return False
        for team_field in ['home_team', 'away_team']:
            name = match_data.get(team_field, "")
            if name and len(re.sub(r'[^a-zA-Z\u4e00-\u9fff]', '', name)) == 0:
                return False
        return True

    def filter_future_hkjc_matches(self, hkjc_matches: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        now = datetime.now()
        future, started = [], []
        for m in hkjc_matches:
            nt = m.get("normalized_time")
            if isinstance(nt, str):
                try:
                    nt = datetime.fromisoformat(nt)
                except Exception:
                    nt = None
            if nt and now >= nt:
                started.append(m)
            else:
                future.append(m)
        if started:
            logger.info("Filtered out %d started HKJC matches", len(started))
        return future, started

    def enrich_hkjc_with_home_event_ids(self, hkjc_matches: List[Dict], home_rows: List[Dict]):
        if not home_rows:
            return
        for m in hkjc_matches:
            if m.get("event_id"):
                continue
            best = None
            best_sim = 0.0
            for sb in home_rows:
                home, away = sb.get("home_team"), sb.get("away_team")
                if not home or not away:
                    continue
                teams_similar, avg_sim, _ = self.are_teams_similar_enough(
                    m.get("home_team", ""), m.get("away_team", ""), home, away
                )
                if teams_similar and avg_sim > best_sim:
                    best_sim = avg_sim
                    best = sb
            if best and best_sim >= 0.70:
                m["event_id"] = best.get("event_id")
                logger.info("Attached event_id %s to HKJC %s vs %s (sim=%.2f)",
                            m["event_id"], m.get("home_team"), m.get("away_team"), best_sim)

    async def scrape_hkjc_matches(self) -> List[Dict]:
        matches: List[Dict] = []
        raw_matches: List[Dict] = []
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            try:
                cprint("ğŸŒ Loading HKJC matches live...", Fore.BLUE)
                await page.goto("https://bet.hkjc.com/ch/football/had", wait_until='domcontentloaded', timeout=60000)
                await asyncio.sleep(2)
                await self.scroll_page_fully(page, max_attempts=18, pause=0.8)
                await self.scroll_until_count_stable(page, selector=".match-row,.event-row", max_rounds=10, pause=0.9)
                try:
                    content = await page.content()
                    if DEBUG_INSTRUMENTATION_AVAILABLE:
                        save_rendered_html("hkjc_page", "index", content)
                except Exception:
                    pass
                await self.click_show_more_hkjc(page)
                await self.scroll_page_fully(page, max_attempts=18, pause=0.8)
                await self.scroll_until_count_stable(page, selector=".match-row,.event-row", max_rounds=12, pause=0.9)
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                match_rows = soup.find_all('div', class_='match-row') or soup.find_all('div', class_='event-row')
                cprint(f"ğŸ” Found {len(match_rows)} match rows on HKJC", Fore.CYAN)
                rows_with_evt = 0
                for r in match_rows:
                    a = r.find('a', href=True)
                    has_link = a and re.search(r'/allodds/\d+', a['href'])
                    if has_link:
                        rows_with_evt += 1
                cprint(f"â„¹ï¸  Rows with event_id (via link): {rows_with_evt}; without: {len(match_rows) - rows_with_evt}", Fore.YELLOW if rows_with_evt < len(match_rows) else Fore.GREEN)
                for i, row in enumerate(match_rows):
                    try:
                        match_data = await self.extract_hkjc_match_data(row)
                        if match_data and self.validate_match_data(match_data):
                            norm_time_dt = self.normalize_time(match_data.get('date', ''))
                            norm_time_str = norm_time_dt.isoformat() if norm_time_dt else None
                            raw = {
                                "source": "HKJC",
                                "match_id": match_data.get('match_id', ''),
                                "event_id": match_data.get('event_id', ''),
                                "home_team": match_data['home_team'],
                                "away_team": match_data['away_team'],
                                "match_time_original": match_data.get('date', ''),
                                "normalized_time": norm_time_str,
                                "normalized_time_str": norm_time_str,
                                "tournament": match_data.get('tournament', ''),
                                "scraped_at": datetime.now().isoformat()
                            }
                            raw_matches.append(raw)
                            matches.append(raw)
                            if i < 3:
                                cprint(f"  Sample: {raw['home_team']} vs {raw['away_team']} (event_id={raw.get('event_id','')})", Fore.MAGENTA)
                    except Exception as e:
                        if i < 3:
                            cprint(f"  âš ï¸ Error in HKJC row {i+1}: {e}", Fore.YELLOW)
                        continue
                self.data_quality_metrics['total_hkjc_matches'] = len(matches)
                self.raw_hkjc_matches = raw_matches
                if DEBUG_INSTRUMENTATION_AVAILABLE:
                    try:
                        save_parsed_json("hkjc_index_parsed", "index", {"raw_matches": raw_matches, "count": len(raw_matches)})
                    except Exception:
                        pass
                cprint(f"âœ… Successfully extracted {len(matches)} HKJC matches", Fore.GREEN)
                return matches
            except Exception as e:
                cprint(f"âŒ Error scraping HKJC: {e}", Fore.RED)
                return []
            finally:
                await browser.close()

    async def click_show_more_hkjc(self, page):
        try:
            xpaths = [
                "//div[contains(text(), 'é¡¯ç¤ºæ›´å¤š')]",
                "//button[contains(text(), 'é¡¯ç¤ºæ›´å¤š')]",
                "//span[contains(text(), 'é¡¯ç¤ºæ›´å¤š')]",
                "//a[contains(text(), 'é¡¯ç¤ºæ›´å¤š')]"
            ]
            for xp in xpaths:
                elements = await page.query_selector_all(f"xpath={xp}")
                for el in elements:
                    try:
                        if await el.is_visible():
                            await el.scroll_into_view_if_needed()
                            await asyncio.sleep(0.5)
                            await el.click()
                            cprint("  âœ… Clicked 'Show More' for HKJC", Fore.BLUE)
                            await asyncio.sleep(1.5)
                            return True
                    except Exception:
                        continue
            return False
        except Exception as e:
            cprint(f"  âš ï¸ Could not find 'Show More' ({e})", Fore.YELLOW)
            return False

    async def scroll_page_fully(self, page, max_attempts: int = 10, pause: float = 0.6):
        last_height = await page.evaluate("() => document.body.scrollHeight")
        for attempt in range(max_attempts):
            await page.mouse.wheel(0, 1200)
            await asyncio.sleep(pause)
            await page.evaluate("() => window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(pause)
            new_height = await page.evaluate("() => document.body.scrollHeight")
            if new_height <= last_height + 20:
                break
            last_height = new_height

    async def scroll_until_count_stable(self, page, selector: str, max_rounds: int = 10, pause: float = 0.8):
        last_count = 0
        for i in range(max_rounds):
            await page.mouse.wheel(0, 1400)
            await asyncio.sleep(pause)
            await page.evaluate("() => window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(pause)
            count = await page.evaluate(f"() => document.querySelectorAll('{selector}').length")
            if count <= last_count:
                break
            last_count = count
        logger.info("Scroll-until-stable: final count for '%s' = %d", selector, last_count)

    async def extract_hkjc_match_data(self, match_row) -> Optional[Dict]:
        try:
            match_id_elem = match_row.find('div', class_='fb-id')
            match_id = match_id_elem.get_text(strip=True) if match_id_elem else None
            date_elem = match_row.find('div', class_='date')
            date = date_elem.get_text(strip=True) if date_elem else ""
            tourn_elem = match_row.find('div', class_='tourn')
            tournament = ""
            if tourn_elem and tourn_elem.find('img'):
                tournament = tourn_elem.find('img').get('title', '') or ""
            home_team, away_team = self.extract_hkjc_teams(match_row)

            event_id = None
            link = match_row.find('a', href=True)
            if link and link['href']:
                m = re.search(r'/allodds/(\d+)', link['href'])
                if m:
                    event_id = m.group(1)

            if not home_team or not away_team:
                return None
            return {'match_id': match_id, 'event_id': event_id, 'date': date, 'tournament': tournament, 'home_team': home_team, 'away_team': away_team}
        except Exception as e:
            logger.debug("extract_hkjc_match_data error: %s", e)
            return None

    def extract_hkjc_teams(self, match_row) -> Tuple[str, str]:
        home_team = ""
        away_team = ""
        try:
            team_icon = match_row.find('div', class_='teamIconSmall')
            if team_icon:
                team_container = team_icon.find('div', title=True)
                if team_container:
                    divs = team_container.find_all('div')
                    if len(divs) >= 2:
                        home_team = divs[0].get_text(strip=True)
                        away_team = divs[1].get_text(strip=True)
        except Exception as e:
            logger.debug("extract_hkjc_teams error: %s", e)
        return home_team, away_team

    async def scrape_titan007_matches(self) -> List[Dict]:
        matches = []
        raw_matches = []
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            try:
                cprint("ğŸŒ Loading Titan007 matches live...", Fore.BLUE)
                await page.goto("https://live.titan007.com/indexall_big.aspx", wait_until='networkidle', timeout=30000)
                await asyncio.sleep(1.5)
                try:
                    content = await page.content()
                    if DEBUG_INSTRUMENTATION_AVAILABLE:
                        save_rendered_html("titan_index", "index", content)
                    else:
                        _ = content
                except Exception:
                    pass
                soup = BeautifulSoup(content, 'html.parser')
                main_table = None
                for table in soup.find_all('table'):
                    txt = table.get_text()
                    if 'æ™‚é–“' in txt and 'æ¯”è³½çƒéšŠ' in txt:
                        main_table = table
                        break
                if not main_table:
                    cprint("âŒ Could not find main Titan007 table", Fore.RED)
                    return []
                rows = main_table.find_all('tr')
                cprint(f"ğŸ” Found {len(rows)} rows in Titan007 table", Fore.CYAN)
                time_col_idx = 1
                status_col_idx = 2
                for i, row in enumerate(rows):
                    try:
                        if not row.get_text(strip=True):
                            continue
                        if 'æ™‚é–“' in row.get_text() and 'æ¯”è³½çƒéšŠ' in row.get_text():
                            cells = row.find_all(['td', 'th'])
                            for idx, cell in enumerate(cells):
                                txt = cell.get_text(strip=True)
                                if txt == 'æ™‚é–“':
                                    time_col_idx = idx
                                elif txt == 'ç‹€æ…‹':
                                    status_col_idx = idx
                            continue
                        team1 = row.find('a', id=lambda x: x and x.startswith('team1_'))
                        team2 = row.find('a', id=lambda x: x and x.startswith('team2_'))
                        if not team1 or not team2:
                            continue
                        match_id = team1.get('id', '').replace('team1_', '')
                        league = "Unknown"
                        cells = row.find_all(['td', 'th'])
                        if cells:
                            league_text = cells[0].get_text(strip=True)
                            if league_text and league_text != 'æ™‚é–“' and 'æ¯”è³½' not in league_text:
                                league = league_text
                        scheduled_time = ""
                        if len(cells) > time_col_idx:
                            scheduled_time = cells[time_col_idx].get_text(strip=True)
                        status = ""
                        if len(cells) > status_col_idx:
                            status = cells[status_col_idx].get_text(strip=True)
                        home_team = re.sub(r'\[\d+\]|\(ä¸­\)', '', team1.get_text(strip=True)).strip()
                        away_team = re.sub(r'\[\d+\]|\(ä¸­\)', '', team2.get_text(strip=True)).strip()
                        if not home_team or not away_team or len(home_team) < 2 or len(away_team) < 2:
                            continue
                        score = ""
                        for cell in cells:
                            cell_text = cell.get_text(strip=True)
                            if '-' in cell_text and len(cell_text) <= 7:
                                score = cell_text
                                break
                        normalized_time = None
                        if scheduled_time and re.match(r'^\d{1,2}:\d{2}$', scheduled_time):
                            try:
                                hour, minute = map(int, scheduled_time.split(":"))
                                today = datetime.now()
                                normalized_time = datetime(today.year, today.month, today.day, hour, minute)
                            except Exception:
                                normalized_time = None
                        raw_match = {
                            "source": "Titan007",
                            "match_id": match_id,
                            "league": league,
                            "home_team": home_team,
                            "away_team": away_team,
                            "scheduled_time_original": scheduled_time,
                            "match_status": status,
                            "score": score,
                            "normalized_time": normalized_time,
                            "normalized_time_str": normalized_time.isoformat() if normalized_time else None,
                            "scraped_at": datetime.now().isoformat()
                        }
                        raw_matches.append(raw_match)
                        if self.validate_match_data(raw_match):
                            matches.append(raw_match)
                        if len(matches) <= 3:
                            cprint(f"  Sample: {home_team} vs {away_team}", Fore.MAGENTA)
                    except Exception as e:
                        if i < 5:
                            cprint(f"  âš ï¸ Error parsing Titan row {i + 1}: {e}", Fore.YELLOW)
                        continue
                self.data_quality_metrics['total_titan_matches'] = len(matches)
                self.raw_titan_matches = raw_matches
                if DEBUG_INSTRUMENTATION_AVAILABLE:
                    try:
                        save_parsed_json("titan_index_parsed", "index", {"raw_matches": raw_matches, "matches": matches})
                    except Exception:
                        pass
                cprint(f"âœ… Successfully extracted {len(matches)} Titan007 matches", Fore.GREEN)
                return matches
            except Exception as e:
                cprint(f"âŒ Error scraping Titan007: {e}", Fore.RED)
                return []
            finally:
                await browser.close()

    def load_titan_stats_from_disk(self, match_id: str) -> Tuple[Optional[Dict[str, Any]], str]:
        base = TITAN_STATS_BASE
        full_path = base / "full" / f"{match_id}.json"
        inc_path = base / "incomplete" / f"{match_id}.json"
        miss_path = base / "completelymissing" / f"{match_id}.json"

        try_paths = [("full", full_path), ("incomplete", inc_path), ("missing", miss_path)]
        for status, p in try_paths:
            if p.exists():
                try:
                    data = json.loads(p.read_text(encoding="utf-8"))
                    return data, status
                except Exception as e:
                    logger.warning("Failed reading titan stats %s (%s): %s", match_id, status, e)
                    return None, "none"
        return None, "none"

    async def find_matching_games(self) -> Tuple[List[Dict], List[Dict]]:
        cprint("\n" + "=" * 80, Fore.WHITE)
        cprint("ğŸ” FINDING MATCHES (HKJC + Titan007 + Macau Slot)", Fore.WHITE)
        cprint("=" * 80, Fore.WHITE)

        if DEBUG_INSTRUMENTATION_AVAILABLE:
            try:
                init_debug_session()
                log_info("Session started", {
                    "min_similarity_threshold": self.min_similarity_threshold,
                    "time_tolerance_minutes": self.time_tolerance_minutes
                })
            except Exception:
                pass

        # Step 0: HKJC bulk odds with fast-skip based on cache size vs declared
        cprint("\nğŸ“¥ Step 0: Scraping HKJC All Odds (bulk, skip cached)...", Fore.BLUE)
        home_scraper = HKJCHomeScraper()
        hkjc_detailed_scraper = HKJCDetailedOddsScraper()
        bulk_collector = HKJCBulkOddsCollector(
            home_scraper,
            hkjc_detailed_scraper,
            existing_cache=self.hkjc_bulk_odds,
            skip_ids=self.hkjc_odds_processed,
        )
        self.hkjc_bulk_odds, self.hkjc_odds_processed, home_rows_from_step0 = await bulk_collector.collect(
            max_events=None, concurrency=5, force_rescrape=False, fast_skip_if_cache_sufficient=True
        )
        save_cache_set(HKJC_ODDS_PROCESSED_PATH, self.hkjc_odds_processed)
        cprint(f"âœ… HKJC bulk odds collected/reused: {len(self.hkjc_bulk_odds)} events", Fore.GREEN)

        # Step 1: Macau odds
        cprint("\nğŸ“¥ Step 1: Scraping Macau Slot odds...", Fore.BLUE)
        macau_scraper = MacauSlotOddsScraper()
        macau_disk = macau_scraper.load_latest_from_disk()
        if macau_disk:
            macau_odds = macau_disk["matches"]
            cprint(f"âœ… Macau Slot (reused from disk): {len(macau_odds)} matches", Fore.GREEN)
        else:
            macau_odds = await macau_scraper.scrape_with_logging(max_pages=20, skip_ids=None)
            if macau_odds:
                macau_file = macau_scraper.save_to_json(macau_odds)
                cprint(f"âœ… Macau Slot scraped: {len(macau_odds)} matches saved to {macau_file}", Fore.GREEN)
            else:
                cprint("âš ï¸ No Macau Slot odds scraped", Fore.YELLOW)
                macau_odds = []

        # Step 2: HKJC list
        cprint("\nğŸ“¥ Step 2: Scraping HKJC HAD list...", Fore.BLUE)
        hkjc_matches = await self.scrape_hkjc_matches()

        # Step 2a: Enrich HKJC event IDs using Step0 home rows
        cprint("\nğŸ“¥ Step 2a: Enrich HKJC event IDs using Step0 home rows...", Fore.BLUE)
        home_rows = home_rows_from_step0
        self.enrich_hkjc_with_home_event_ids(hkjc_matches, home_rows)

        # Step 2b: Titan list
        titan_matches = await self.scrape_titan007_matches()
        cprint(f"\nğŸ“Š Match Counts:", Fore.CYAN)
        cprint(f"  HKJC: {len(hkjc_matches)} matches", Fore.CYAN)
        cprint(f"  Titan007: {len(titan_matches)} matches", Fore.CYAN)
        cprint(f"  Macau Slot: {len(macau_odds)} matches", Fore.CYAN)

        future_hkjc_matches, started_hkjc_matches = self.filter_future_hkjc_matches(hkjc_matches)
        if started_hkjc_matches:
            cprint(f"â­ï¸ Skipping {len(started_hkjc_matches)} HKJC matches that already started.", Fore.YELLOW)
        hkjc_matches = future_hkjc_matches

        if not hkjc_matches or not titan_matches:
            cprint("âŒ Cannot proceed: One or both sites returned no matches", Fore.RED)
            return [], []

        # Step 3: Build Macau mapping to Titan
        cprint("\nğŸ”„ Step 3: Building Macau odds mapping to Titan...", Fore.BLUE)
        self.macau_mapping = {}
        for titan in titan_matches:
            titan_time = titan.get("normalized_time")
            titan_league = titan.get("league")
            for macau in macau_odds:
                macau_time = self.normalize_time(macau.get('time', ''))
                sim_home = name_similarity(titan['home_team'], macau.get('home_team', ''))
                sim_away = name_similarity(titan['away_team'], macau.get('away_team', ''))
                avg_sim = (sim_home + sim_away) / 2
                time_ok = self.is_within_tolerance(titan_time, macau_time, self.titan_macau_time_tolerance)
                league_bump = league_bonus(titan_league, macau.get("competition"), 0.05)
                effective_sim = min(1.0, avg_sim + league_bump)

                allow_by_time = time_ok and effective_sim >= 0.50
                allow_by_name_only = (not titan_time or not macau_time) and effective_sim >= self.macau_name_only_threshold

                if allow_by_time or allow_by_name_only:
                    prev = self.macau_mapping.get(titan['match_id'])
                    if (not prev) or (effective_sim > prev.get("_sim", 0)):
                        self.macau_mapping[titan['match_id']] = {**macau, "_sim": effective_sim}

                if DEBUG_INSTRUMENTATION_AVAILABLE:
                    try:
                        log_mapping_decision("macau_to_titan_attempt", {
                            "titan_id": titan.get("match_id"),
                            "macau_event_id": macau.get("event_id"),
                            "avg_sim": avg_sim,
                            "effective_sim": effective_sim,
                            "time_match": time_ok,
                            "league_bump": league_bump,
                        })
                    except Exception:
                        pass
        cprint(f"  Mapped {len(self.macau_mapping)} Titan matches to Macau odds", Fore.GREEN)

        # Step 4: Matching + AI
        cprint("\nğŸ” Step 4: Finding matches and running AI per-match...", Fore.BLUE)
        matched = []
        unmatched_hkjc = []

        def classify(hkjc_match, titan_match, macau_match):
            has_h = hkjc_match is not None
            has_t = titan_match is not None
            has_m = macau_match is not None
            if has_h and has_t and has_m:
                return "hkjc_titan_macau"
            if has_h and has_t and not has_m:
                return "hkjc_titan"
            if has_h and not has_t and has_m:
                return "hkjc_macau"
            if has_h and not has_t and not has_m:
                return "hkjc_only"
            if has_t and has_m:
                return "titan_macau"
            if has_t:
                return "titan_only"
            if has_m:
                return "macau_only"
            return "unknown"

        for hkjc in hkjc_matches:
            best_match = None
            best_score = 0.0
            best_is_swapped = False

            hkjc_time = self.normalize_time(hkjc.get("match_time_original") or hkjc.get("normalized_time"))
            hkjc_league = hkjc.get("tournament", "")

            for titan in titan_matches:
                self.data_quality_metrics['potential_matches_checked'] += 1
                titan_time = titan.get("normalized_time")
                time_ok = self.is_within_tolerance(hkjc_time, titan_time, self.hk_titan_time_tolerance)

                teams_similar, avg_sim, is_swapped = self.are_teams_similar_enough(
                    hkjc['home_team'], hkjc['away_team'],
                    titan['home_team'], titan['away_team']
                )

                league_bump = league_bonus(hkjc_league, titan.get("league", ""), 0.05)
                effective_sim = min(1.0, avg_sim + league_bump)

                threshold = self.time_assisted_threshold if time_ok else self.min_similarity_threshold
                if effective_sim >= threshold and effective_sim > best_score:
                    best_score = effective_sim
                    best_match = titan
                    best_is_swapped = is_swapped

                if DEBUG_INSTRUMENTATION_AVAILABLE:
                    try:
                        log_mapping_decision("hkjc_to_titan_attempt", {
                            "hkjc_home": hkjc.get("home_team"),
                            "hkjc_away": hkjc.get("away_team"),
                            "titan_id": titan.get("match_id"),
                            "titan_home": titan.get("home_team"),
                            "titan_away": titan.get("away_team"),
                            "avg_sim": avg_sim,
                            "effective_sim": effective_sim,
                            "is_swapped": is_swapped,
                            "time_match": time_ok,
                            "league_bump": league_bump,
                            "threshold_used": threshold,
                        })
                    except Exception:
                        pass

            if best_match and best_score >= self.time_assisted_threshold:
                titan_id = best_match['match_id']
                macau = self.macau_mapping.get(titan_id)
                source_type = classify(hkjc, best_match, macau)
                matched_item = {
                    "source_coverage": source_type,
                    "hkjc_match": {
                        "match_id": hkjc.get('match_id', ''),
                        "event_id": hkjc.get('event_id', ''),
                        "home_team": hkjc['home_team'],
                        "away_team": hkjc['away_team'],
                        "match_time": hkjc.get('match_time_original', ''),
                        "tournament": hkjc.get('tournament', '')
                    },
                    "titan_match": {
                        "match_id": titan_id,
                        "home_team": best_match['home_team'],
                        "away_team": best_match['away_team'],
                        "scheduled_time": best_match.get('scheduled_time_original', ''),
                        "league": hkjc.get('tournament', ''),
                        "status": best_match.get('match_status', '')
                    },
                    "macau_match": macau,
                    "similarity_score": best_score,
                    "teams_swapped": best_is_swapped,
                    "matched_at": datetime.now().isoformat()
                }

                hkjc_event_id = hkjc.get("event_id")
                if hkjc_event_id:
                    cached_odds = self.hkjc_bulk_odds.get(hkjc_event_id)
                    if cached_odds:
                        matched_item["hkjc_detailed_odds"] = cached_odds
                        cprint(f"   ğŸ“Š HKJC odds from bulk cache (Event {hkjc_event_id})", Fore.GREEN)
                    else:
                        cprint(f"   ğŸ“Š Scraping HKJC All Odds live (Event {hkjc_event_id})...", Fore.CYAN)
                        detailed_odds = await hkjc_detailed_scraper.scrape(hkjc_event_id)
                        if detailed_odds:
                            matched_item["hkjc_detailed_odds"] = detailed_odds
                            cprint("   âœ… HKJC detailed odds captured", Fore.GREEN)
                        else:
                            cprint("   âš ï¸ HKJC detailed odds failed", Fore.YELLOW)

                disk_stats, disk_status = self.load_titan_stats_from_disk(titan_id)
                if disk_status == "full":
                    detailed_stats = disk_stats
                    detailed_stats["stats_available"] = True
                    self.titan_stats_processed.add(titan_id)
                    save_cache_set(TITAN_STATS_PROCESSED_PATH, self.titan_stats_processed)
                    matched_item['detailed_stats'] = detailed_stats
                    matched_item['titan_stats_available'] = True
                else:
                    matched_item['titan_stats_available'] = False
                    matched_item['skipped_reason'] = f"titan_stats_{disk_status}_on_disk"
                    matched_item["ai_status"] = "skipped"
                    matched_item["ai_reason"] = matched_item['skipped_reason']
                    matched.append(matched_item)
                    self.data_quality_metrics['high_confidence_matches'] += 1
                    continue

                cached_ai = self.ai_cache.get(titan_id)
                if cached_ai and cached_ai.get("ai_result"):
                    matched_item["ai_recommendation"] = cached_ai["ai_result"]
                    matched_item["ai_status"] = "cached"
                    matched_item["ai_reason"] = "ai_cached"
                    cprint("   â­ï¸ Using cached AI recommendation (no API call)", Fore.YELLOW)
                else:
                    normalized = normalize_parsed_data(detailed_stats)
                    if DEBUG_INSTRUMENTATION_AVAILABLE:
                        try:
                            save_parsed_json("ai_prompt", titan_id, {"prompt_src": "full"})
                            log_info("Calling AI for titan", {"titan_id": titan_id, "available_sections": normalized.get("_meta")})
                        except Exception:
                            pass
                    cprint("   ğŸ¤– Running AI analysis...", Fore.CYAN)
                    ai_result = await perform_ai_analysis_for_match_async(normalized, call_deepseek_api_async)
                    matched_item['ai_recommendation'] = ai_result
                    matched_item["ai_status"] = "ok" if ai_result.get("ai_parsed_json") else "no_json"
                    matched_item["ai_reason"] = "" if ai_result.get("ai_parsed_json") else ai_result.get("brief_reasoning", "No parsable JSON")
                    self.ai_cache[titan_id] = {
                        "processed_at": datetime.now().isoformat(),
                        "hkjc_match_id": hkjc.get("match_id", ""),
                        "home_team": hkjc.get("home_team"),
                        "away_team": hkjc.get("away_team"),
                        "ai_result": ai_result,
                    }
                    save_ai_cache(self.ai_cache)

                matched.append(matched_item)
                self.data_quality_metrics['high_confidence_matches'] += 1
            else:
                unmatched_hkjc.append(hkjc)
                if len(unmatched_hkjc) <= 3:
                    reason = "No similar teams found"
                    if best_match:
                        reason = f"Similarity {best_score:.2f} below threshold"
                    cprint(f"\nâŒ NO MATCH: {hkjc['home_team']} vs {hkjc['away_team']}", Fore.RED)
                    cprint(f"   Reason: {reason}", Fore.RED)

        # Titan + Macau-only AI (no HKJC match)
        cprint("\nğŸ”„ Titan + Macau-only AI (no HKJC match)...", Fore.BLUE)
        matched_titan_ids = {m["titan_match"]["match_id"] for m in matched if m.get("titan_match")}
        macau_only_pairs = []
        for titan in titan_matches:
            tid = titan.get("match_id")
            if tid in matched_titan_ids:
                continue
            macau = self.macau_mapping.get(tid)
            if not macau:
                continue
            macau_only_pairs.append((titan, macau))

        for titan, macau in macau_only_pairs:
            titan_id = titan["match_id"]
            matched_item = {
                "source_coverage": "titan_macau",
                "hkjc_match": None,
                "titan_match": {
                    "match_id": titan_id,
                    "home_team": titan["home_team"],
                    "away_team": titan["away_team"],
                    "scheduled_time": titan.get("scheduled_time_original", ""),
                    "league": titan.get("league", ""),
                    "status": titan.get("match_status", "")
                },
                "macau_match": macau,
                "similarity_score": macau.get("_sim", 1.0),
                "teams_swapped": False,
                "matched_at": datetime.now().isoformat()
            }

            cached_ai = self.ai_cache.get(titan_id)
            if cached_ai and cached_ai.get("ai_result"):
                matched_item["ai_recommendation"] = cached_ai["ai_result"]
                matched_item["ai_status"] = "cached"
                matched_item["ai_reason"] = "ai_cached"
                matched_item["titan_stats_available"] = False
                matched.append(matched_item)
                self.data_quality_metrics["high_confidence_matches"] += 1
                continue

            disk_stats, disk_status = self.load_titan_stats_from_disk(titan_id)
            if disk_status == "full":
                detailed_stats = disk_stats
                detailed_stats["stats_available"] = True
                self.titan_stats_processed.add(titan_id)
                save_cache_set(TITAN_STATS_PROCESSED_PATH, self.titan_stats_processed)
            else:
                matched_item["titan_stats_available"] = False
                matched_item["skipped_reason"] = f"titan_stats_{disk_status}_on_disk"
                matched_item["ai_status"] = "skipped"
                matched_item["ai_reason"] = matched_item["skipped_reason"]
                matched.append(matched_item)
                continue

            matched_item["detailed_stats"] = detailed_stats
            matched_item["titan_stats_available"] = True

            normalized = normalize_parsed_data(detailed_stats)
            cprint("   ğŸ¤– Running AI analysis (Titan+Macau-only)...", Fore.CYAN)
            ai_result = await perform_ai_analysis_for_match_async(normalized, call_deepseek_api_async)
            matched_item["ai_recommendation"] = ai_result
            matched_item["ai_status"] = "ok" if ai_result.get("ai_parsed_json") else "no_json"
            matched_item["ai_reason"] = "" if ai_result.get("ai_parsed_json") else ai_result.get("brief_reasoning", "No parsable JSON")

            self.ai_cache[titan_id] = {
                "processed_at": datetime.now().isoformat(),
                "hkjc_match_id": "",
                "home_team": titan.get("home_team"),
                "away_team": titan.get("away_team"),
                "ai_result": ai_result,
            }
            save_ai_cache(self.ai_cache)

            matched.append(matched_item)
            self.data_quality_metrics["high_confidence_matches"] += 1

        self.matched_games = matched
        self.unmatched_games = unmatched_hkjc
        save_ai_cache(self.ai_cache)
        save_cache_set(HKJC_ODDS_PROCESSED_PATH, self.hkjc_odds_processed)
        save_cache_set(TITAN_STATS_PROCESSED_PATH, self.titan_stats_processed)

        cprint(f"\nğŸ“Š FINAL RESULTS:", Fore.CYAN)
        cprint(f"   âœ… Matched games: {len(matched)}", Fore.GREEN)
        cprint(f"   âŒ Unmatched HKJC games: {len(unmatched_hkjc)}", Fore.RED)
        hkjc_matched_count = sum(1 for m in matched if m.get("hkjc_match"))
        if hkjc_matches:
            success_rate = hkjc_matched_count / len(hkjc_matches) * 100
        else:
            success_rate = 0
        cprint(f"   ğŸ“ˆ Success rate: {success_rate:.1f}%", Fore.CYAN)

        self.save_all_sources_ordered_excel(hkjc_matches, titan_matches, macau_odds, matched, unmatched_hkjc)
        return matched, unmatched_hkjc

    def generate_detailed_report(self) -> Dict:
        report = {
            "summary": {
                "total_matched": len(self.matched_games),
                "total_unmatched": len(self.unmatched_games),
                "data_quality_metrics": self.data_quality_metrics
            },
            "top_matches": sorted(self.matched_games, key=lambda x: x.get('similarity_score', 0), reverse=True)[:5],
            "common_issues": [],
            "recommendations": []
        }
        if self.data_quality_metrics['total_hkjc_matches'] == 0:
            report['common_issues'].append("No matches found on HKJC")
        if self.data_quality_metrics['total_titan_matches'] == 0:
            report['common_issues'].append("No matches found on Titan007")
        return report

    def save_report(self, report: Dict, filename: Optional[str] = None):
        if not filename:
            filename = f"detailed_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        cprint(f"ğŸ“Š Detailed report saved to: {filename}", Fore.CYAN)

    def save_comparison_excel(self, filename: Optional[str] = None):
        if not filename:
            filename = f"all_scraped_matches_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        if not self.matched_games and not self.unmatched_games:
            cprint("âš ï¸ No data available for comparison export", Fore.YELLOW)
            return

        rows = []
        for m in self.matched_games:
            h = m.get("hkjc_match") or {}
            t = m.get("titan_match") or {}
            mc = m.get("macau_match") or {}
            rows.append({
                "hkjc_match_id": h.get("match_id"),
                "hkjc_event_id": h.get("event_id"),
                "hkjc_home": h.get("home_team"),
                "hkjc_away": h.get("away_team"),
                "hkjc_time": h.get("match_time"),
                "titan_match_id": t.get("match_id"),
                "titan_home": t.get("home_team"),
                "titan_away": t.get("away_team"),
                "titan_time": t.get("scheduled_time"),
                "macau_event_id": mc.get("event_id"),
                "macau_time": mc.get("time"),
                "macau_competition": mc.get("competition"),
                "macau_home": mc.get("home_team"),
                "macau_away": mc.get("away_team"),
                "similarity_score": m.get("similarity_score"),
                "source_coverage": m.get("source_coverage"),
            })
        for h in self.unmatched_games:
            rows.append({
                "hkjc_match_id": h.get("match_id"),
                "hkjc_event_id": h.get("event_id"),
                "hkjc_home": h.get("home_team"),
                "hkjc_away": h.get("away_team"),
                "hkjc_time": h.get("match_time_original"),
                "titan_match_id": None,
                "titan_home": None,
                "titan_away": None,
                "titan_time": None,
                "macau_event_id": None,
                "macau_time": None,
                "macau_competition": None,
                "macau_home": None,
                "macau_away": None,
                "similarity_score": None,
                "source_coverage": "hkjc_only",
            })
        df = pd.DataFrame(rows)
        with pd.ExcelWriter(filename, engine="openpyxl") as writer:
            df.to_excel(writer, sheet_name="Comparison", index=False)
        cprint(f"ğŸ“‹ Comparison saved to: {filename}", Fore.CYAN)

    def save_ai_results_excel(self, matched_games: List[Dict], filename: Optional[str] = None):
        if not filename:
            filename = f"matched_games_with_ai_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        rows = []
        for m in matched_games:
            h = m.get("hkjc_match") or {}
            t = m.get("titan_match") or {}
            mc = m.get("macau_match") or {}  # â†â†â†â† åŠ ä¸Šè¿™ä¸€è¡Œï¼
            ai = m.get("ai_recommendation", {}) or {}
            rows.append({
                "hkjc_match_id": h.get("match_id"),
                "hkjc_event_id": h.get("event_id"),
                "hkjc_home": h.get("home_team"),
                "hkjc_away": h.get("away_team"),
                "macauslot_event_id": mc.get("event_id"),
                "macauslot_home": mc.get("home_team"),
                "macauslot_away": mc.get("away_team"),
                "macauslot_league": mc.get("competition") or mc.get("league"),
                "macauslot_time": mc.get("time"),
                "titan_match_id": t.get("match_id") if t else None,
                "titan_home": t.get("home_team") if t else None,
                "titan_away": t.get("away_team") if t else None,
                "best_bet_market": ai.get("best_bet_market"),
                "best_bet_selection": ai.get("best_bet_selection"),
                "confidence_level": ai.get("confidence_level"),
                "brief_reasoning": ai.get("brief_reasoning"),
                "similarity_score": m.get("similarity_score"),
                "source_coverage": m.get("source_coverage"),
                "ai_status": m.get("ai_status"),
                "ai_reason": m.get("ai_reason"),
            })
        df = pd.DataFrame(rows)
        with pd.ExcelWriter(filename, engine="openpyxl") as writer:
            df.to_excel(writer, sheet_name="AI_Results", index=False)
        cprint(f"AI results saved to: {filename}", Fore.CYAN)

    def save_comparison_csv(self, filename: Optional[str] = None):
        self.save_comparison_excel(filename.replace(".csv", ".xlsx") if filename else None)

    def save_all_sources_ordered_excel(self,
                                       hkjc_matches: List[Dict],
                                       titan_matches: List[Dict],
                                       macau_matches: List[Dict],
                                       matched_games: List[Dict],
                                       unmatched_hkjc: List[Dict],
                                       filename: Optional[str] = None):
        if not filename:
            filename = f"all_sources_ordered_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        rows = []
        matched_titan_ids = set()
        matched_macau_ids = set()
        matched_hkjc_ids = set()

        for idx, m in enumerate(matched_games, 1):
            group_id = f"M{idx}"
            h = m.get("hkjc_match", {}) or {}
            t = m.get("titan_match", {}) or {}
            mc = m.get("macau_match", {}) or {}

            matched_hkjc_ids.add(h.get("match_id"))
            matched_titan_ids.add(t.get("match_id"))
            if mc.get("event_id"):
                matched_macau_ids.add(mc.get("event_id"))

            rows.append({
                "group": group_id, "source": "HKJC", "matched_flag": True,
                "match_id": h.get("match_id"), "event_id": h.get("event_id"),
                "home": h.get("home_team"), "away": h.get("away_team"),
                "time": h.get("match_time"), "competition": h.get("tournament"),
                "similarity_score": m.get("similarity_score"), "coverage": m.get("source_coverage"),
            })
            rows.append({
                "group": group_id, "source": "Titan007", "matched_flag": True,
                "match_id": t.get("match_id"), "event_id": None,
                "home": t.get("home_team"), "away": t.get("away_team"),
                "time": t.get("scheduled_time"), "competition": t.get("league"),
                "similarity_score": m.get("similarity_score"), "coverage": m.get("source_coverage"),
            })
            if mc:
                rows.append({
                    "group": group_id, "source": "MacauSlot", "matched_flag": True,
                    "match_id": mc.get("event_id"), "event_id": mc.get("event_id"),
                    "home": mc.get("home_team"), "away": mc.get("away_team"),
                    "time": mc.get("time"), "competition": mc.get("competition"),
                    "similarity_score": m.get("similarity_score"), "coverage": m.get("source_coverage"),
                })

        for h in unmatched_hkjc:
            rows.append({
                "group": "U_HKJC", "source": "HKJC", "matched_flag": False,
                "match_id": h.get("match_id"), "event_id": h.get("event_id"),
                "home": h.get("home_team"), "away": h.get("away_team"),
                "time": h.get("match_time_original"), "competition": h.get("tournament", ""),
                "similarity_score": None, "coverage": "hkjc_only",
            })

        for t in titan_matches:
            if t.get("match_id") in matched_titan_ids:
                continue
            rows.append({
                "group": "U_Titan", "source": "Titan007", "matched_flag": False,
                "match_id": t.get("match_id"), "event_id": None,
                "home": t.get("home_team"), "away": t.get("away_team"),
                "time": t.get("scheduled_time_original"), "competition": t.get("league"),
                "similarity_score": None, "coverage": "titan_only",
            })

        for mc in macau_matches:
            if mc.get("event_id") in matched_macau_ids:
                continue
            rows.append({
                "group": "U_Macau", "source": "MacauSlot", "matched_flag": False,
                "match_id": mc.get("event_id"), "event_id": mc.get("event_id"),
                "home": mc.get("home_team"), "away": mc.get("away_team"),
                "time": mc.get("time"), "competition": mc.get("competition"),
                "similarity_score": None, "coverage": "macau_only",
            })

        df = pd.DataFrame(rows)
        with pd.ExcelWriter(filename, engine="openpyxl") as writer:
            df.to_excel(writer, sheet_name="AllSources", index=False)
        cprint(f"ğŸ“‹ All sources (matched first) saved to: {filename}", Fore.CYAN)

def save_recommendations_to_excel(recommendations: List[Dict], filename: str):
    logger.info("ai_recommendations export disabled; no file written.")

async def main():
    cprint("ğŸš€ LIVE MATCH CROSS-REFERENCER â€” WITH HKJC ALL ODDS + MACAU SLOT", Fore.WHITE)
    cprint("=" * 80, Fore.WHITE)

    matcher = LiveMatchMatcher(min_similarity_threshold=0.70, time_tolerance_minutes=30,
                               prioritize_similarity=True,
                               hk_titan_time_tolerance=45,
                               titan_macau_time_tolerance=10)
    matched_games, unmatched = await matcher.find_matching_games()

    report = matcher.generate_detailed_report()
    matcher.save_report(report)
    matcher.save_comparison_excel()

    if matched_games:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename_json = f"matched_games_with_ai_analysis_{ts}.json"
        with open(filename_json, "w", encoding="utf-8") as f:
            json.dump(matched_games, f, ensure_ascii=False, indent=2)
        cprint(f"\nğŸ’¾ Saved {len(matched_games)} matched games (with AI analysis) to: {filename_json}", Fore.CYAN)
        matcher.save_ai_results_excel(matched_games)

    if unmatched:
        analysis_file = f"unmatched_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(analysis_file, "w", encoding="utf-8") as f:
            json.dump({
                "unmatched_count": len(unmatched),
                "sample_unmatched": unmatched[:10],
                "analysis_time": datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        cprint(f"ğŸ’¾ Saved unmatched analysis to: {analysis_file}", Fore.CYAN)

    cprint("\nâœ… Process complete!", Fore.GREEN)
    cprint(f"ğŸ“Š Summary: {len(matched_games)} matches analyzed with AI recommendations", Fore.CYAN)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        cprint("Interrupted by user.", Fore.YELLOW)
    except Exception as e:
        logger.exception("Fatal error in main: %s", e)
