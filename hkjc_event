#!/usr/bin/env python3
import asyncio, json, re, sys
from pathlib import Path
from datetime import datetime
from contextlib import suppress
from playwright.async_api import async_playwright

BET_HOME    = "https://bet.hkjc.com/ch/football/home"
ROWS_SEL    = ".match-row,.event-row"
OUTPUT      = "debug_bet_home_event_ids.json"
DT_FORMAT   = "%d/%m/%Y %H:%M"   # e.g., 20/12/2025 23:00
PAUSE       = 0.6
CLICK_WAIT  = 2.0
TIMEOUT_MS  = 12000

def parse_row_start(txt: str):
    try:
        return datetime.strptime(txt.strip(), DT_FORMAT)
    except Exception:
        return None

def extract_id_from_url(url: str):
    m = re.search(r"/allodds/(\d+)", url)
    return m.group(1) if m else None

async def get_declared(page):
    html = await page.content()
    m = re.search(r"共有\s*(\d+)\s*場賽事", html)
    return int(m.group(1)) if m else None

async def scroll_bottom(page):
    await page.mouse.wheel(0, 1800)
    await asyncio.sleep(PAUSE)
    await page.evaluate("() => window.scrollTo(0, document.body.scrollHeight)")
    await asyncio.sleep(PAUSE)

async def click_show_more_once(page):
    xps = [
        "//div[contains(text(), '顯示更多')]",
        "//button[contains(text(), '顯示更多')]",
        "//span[contains(text(), '顯示更多')]",
        "//a[contains(text(), '顯示更多')]",
    ]
    for xp in xps:
        for el in await page.query_selector_all(f"xpath={xp}"):
            if await el.is_visible():
                await el.scroll_into_view_if_needed()
                await asyncio.sleep(0.2)
                await el.click()
                await asyncio.sleep(0.8)
                return True
    return False

async def main():
    now = datetime.now()
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)  # headful recommended
        ctx = await browser.new_context()
        page = await ctx.new_page()

        collected = set()
        mod_key = ["Meta"] if sys.platform == "darwin" else ["Control"]

        def log_url(url: str):
            eid = extract_id_from_url(url)
            if eid:
                collected.add(eid)
                print(f"  captured via network: {eid}")

        ctx.on("request",  lambda req: log_url(req.url))
        ctx.on("response", lambda resp: log_url(resp.url))

        await page.goto(BET_HOME, wait_until="domcontentloaded", timeout=90000)
        await asyncio.sleep(1.0)
        declared = await get_declared(page)
        print(f"Declared: {declared} matches" if declared else "Declared not found")

        await click_show_more_once(page)
        await scroll_bottom(page)
        row_count = await page.evaluate(f"() => document.querySelectorAll('{ROWS_SEL}').length")
        if declared and row_count < declared:
            await scroll_bottom(page)
        rows = page.locator(ROWS_SEL)
        total = await rows.count()
        print(f"Row count: {total}")

        for i in range(total):
            row = rows.nth(i)
            try:
                rid = (await row.get_attribute("id")) or (await row.locator(".fb-id").first.text_content())
                rid = (rid or "").strip()
                dt_txt = await row.locator(".date").first.text_content()
                start_dt = parse_row_start(dt_txt or "")
                if start_dt and start_dt <= now:
                    print(f"[{i+1}/{total}] {rid} skip (started)")
                    continue

                # Find a trigger
                trigger = None
                for sel in [
                    '[title*="賠率"]',
                    '[title*="所有賠率"]',
                    ".teamIconSmall [title]",
                    ".teamIconSmall",
                    ".team",
                ]:
                    cand = row.locator(sel)
                    if await cand.count():
                        trigger = cand.first
                        print(f"[{i+1}/{total}] {rid} using selector: {sel}")
                        break
                if not trigger:
                    print(f"[{i+1}/{total}] {rid} no trigger found")
                    continue

                pages_before = set(ctx.pages)
                with suppress(Exception):
                    await trigger.click(modifiers=mod_key, force=True)
                await asyncio.sleep(CLICK_WAIT)

                pages_after = set(ctx.pages)
                new_pages = list(pages_after - pages_before)
                url_captured = None

                if new_pages:
                    np = new_pages[0]
                    with suppress(Exception):
                        await np.wait_for_load_state("domcontentloaded", timeout=TIMEOUT_MS)
                    url_captured = np.url
                    print(f"[{i+1}/{total}] {rid} popup URL: {url_captured}")
                    with suppress(Exception):
                        await np.close()
                else:
                    if extract_id_from_url(page.url):
                        url_captured = page.url
                        print(f"[{i+1}/{total}] {rid} same-page URL: {url_captured}")
                        with suppress(Exception):
                            await page.go_back()

                if url_captured:
                    eid = extract_id_from_url(url_captured)
                    if eid:
                        collected.add(eid)
                        print(f"[{i+1}/{total}] {rid} captured ID: {eid}")
                    else:
                        print(f"[{i+1}/{total}] {rid} no ID in URL")
                else:
                    print(f"[{i+1}/{total}] {rid} no popup/nav")

            except Exception as e:
                print(f"[{i+1}/{total}] error: {e}")

        ids_sorted = sorted(collected)
        Path(OUTPUT).write_text(json.dumps(ids_sorted, ensure_ascii=False, indent=2), encoding="utf-8")
        print(f"Scraped IDs: {len(ids_sorted)} (saved to {OUTPUT})")

        if declared is not None and len(ids_sorted) != declared:
            print(f"FAILED: declared {declared}, scraped {len(ids_sorted)}")
            await browser.close()
            sys.exit(1)

        print("SUCCESS")
        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
