import argparse
import asyncio
import json
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from playwright.async_api import async_playwright
import openpyxl
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("titan_stat_main")

INDEX_URL = "https://live.titan007.com/indexall_big.aspx"
GAME_FILE = "game.json"

BAD_STRING = "暫無數據"
MIN_SECTIONS_FOR_FULL = 3  # Amount required to call match 'full'--adjust as needed.

SECTIONS_TO_TRY = [
    ("league_standings",           r"聯賽積分排名"),
    ("head_to_head",               r"對賽往績"),
    ("data_comparison",            r"數據對比"),
    ("referee_stats",              r"裁判統計"),
    ("league_trend",               r"聯賽盤路走勢"),
    ("same_trend",                 r"相同盤路"),
    ("goal_distribution",          r"入球數/上下半場入球分布"),
    ("halftime_fulltime",          r"半全場"),
    ("goal_count",                 r"進球數/單雙"),
    ("goal_time",                  r"進球時間"),
    ("future_matches",             r"未來五場"),
    ("pre_match_brief",            r"賽前簡報"),
    ("season_stats_comparison",    r"本賽季數據統計比較"),
    ("recent_form",                r"近期戰績"),
    ("last_match_player_ratings",  r"球員上一場出場評分"),
    ("lineup_and_injuries",        r"陣容情況"),
    ("pre_match_table",            r"賽前積分榜"),
]

def load_gamejson():
    with open(GAME_FILE, encoding="utf-8") as f:
        ls = json.load(f)
    gids = set(x["SclassID"] for x in ls)
    subids = set(x["SubID"] for x in ls if x.get("SubID"))
    simp_trad = set()
    for x in ls:
        simp_trad.add(x["simp"])
        simp_trad.add(x["trad"])
    return gids, subids, simp_trad

async def filter_match_ids_only_my_leagues(all_match_ids):
    gids, subids, names = load_gamejson()
    filtered = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(INDEX_URL)
        await page.wait_for_timeout(2_200)

        mapping = await page.evaluate("""
            () => {
              let out = {};
              let sources = [window.A, window.arr, window.matchList, window.B, window.C, window.Match];
              for (const src of sources) {
                if (Array.isArray(src)) {
                  for (const row of src) {
                    if (!row) continue;
                    let mid = String(row[0] || row.matchid || row.MatchID || row.matchId || row.id || "");
                    let sclassid = String(row[2] || row[4] || row.sclassid || row.SclassID || row.sclassID || "");
                    let subid = String(row[9] || row.subid || row.SubID || "");
                    let simp = row[1] || row.simp || "";
                    let trad = row[1] || row.trad || "";
                    out[mid] = { sclassid, subid, simp, trad };
                  }
                }
              }
              return out;
            }
        """)
        for mid in all_match_ids:
            if not mid.isdigit():
                continue
            info = mapping.get(mid) or {}
            scid = info.get("sclassid", "")
            subid = info.get("subid", "")
            simp = info.get("simp", "")
            trad = info.get("trad", "")
            if scid in gids or (subid and subid in subids) or simp in names or trad in names:
                filtered.append(mid)
                continue
            tr = await page.query_selector(f'tr[id="tr1_{mid}"]')
            if tr:
                tds = await tr.query_selector_all("td")
                league_name = (await tds[1].inner_text()).strip() if len(tds) > 1 else ""
                if league_name in names:
                    filtered.append(mid)
        await browser.close()
    return filtered

def is_real_data_table(tbl):
    rows = tbl.find_all("tr")
    if len(rows) < 2:
        return False
    for row in rows:
        cells = [c.get_text(strip=True) for c in row.find_all(['td', 'th'])]
        for cell in cells:
            if cell and BAD_STRING not in cell:
                return True
    return False

def analyze_sections(html):
    soup = BeautifulSoup(html, "html.parser")
    found, missing = [], []
    debug_info = {}
    for key, regex in SECTIONS_TO_TRY:
        header = soup.find(string=re.compile(regex))
        section_status = "not_found"
        if header:
            tbl = header.find_next("table")
            if tbl and is_real_data_table(tbl):
                section_status = "real_data"
                found.append(key)
            elif tbl:
                section_status = "only_bad_string"
                missing.append(key)
            else:
                section_status = "table_not_found"
                missing.append(key)
        else:
            missing.append(key)
        debug_info[key] = section_status
    return found, missing, debug_info

async def titan_scrape_stats(match_id: str):
    url = f"https://zq.titan007.com/analysis/{match_id}.htm"
    out = {
        "match_id": match_id,
        "url": url,
        "scraped_at": datetime.now().isoformat(),
        "sections_found": [],
        "sections_missing": [],
        "sections_debug": {},
        "has_stats": False,
        "home_team": "",
        "away_team": "",
        "game_time": "",
        "league": "",
    }
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        try:
            await page.goto(url, wait_until="networkidle", timeout=60000)
            await asyncio.sleep(2.5)
            html = await page.content()
        except Exception as e:
            out["error"] = str(e)
            await browser.close()
            return out
        await browser.close()

    found, missing, debug = analyze_sections(html)
    out["sections_found"] = found
    out["sections_missing"] = missing
    out["sections_debug"] = debug
    out["has_stats"] = len(found) >= MIN_SECTIONS_FOR_FULL

    # Title parsing (as before)
    soup = BeautifulSoup(html, "html.parser")
    m = re.search(r"(.+?) vs (.+?)[\s|【]", soup.title.string if soup.title else "")
    if m:
        out["home_team"] = m.group(1).strip()
        out["away_team"] = m.group(2).strip()
    info_div = soup.find("div", {"class": "jifen_header"})
    if info_div:
        info_text = info_div.get_text(separator=" ", strip=True)
        mtime = re.search(r"\d{2}:\d{2}", info_text)
        if mtime:
            out["game_time"] = mtime.group()
        mleague = re.search(r".*?联.*? ", info_text)
        if mleague:
            out["league"] = mleague.group().strip()

    return out

def excel_add_rows(excel_path, rows, headers):
    if not Path(excel_path).exists():
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.append(headers)
        wb.save(excel_path)
    wb = openpyxl.load_workbook(excel_path)
    ws = wb.active
    for row in rows:
        ws.append(row)
    wb.save(excel_path)

async def discover_match_ids(limit: Optional[int]) -> List[str]:
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(INDEX_URL, wait_until="domcontentloaded", timeout=60000)
        await asyncio.sleep(4.0)
        ids = await page.evaluate(
            """
            () => {
              const out = new Set();
              const globals = ['A','B','C','arr','arrData','Match','matchList'];
              for (const k of globals) {
                const v = window[k];
                if (Array.isArray(v)) {
                  for (const row of v) {
                    const cand = Array.isArray(row)
                      ? row[0]
                      : (row && (row.matchid || row.MatchID || row.matchId || row.id));
                    if (cand) out.add(String(cand));
                  }
                }
              }
              document.querySelectorAll('a[href*="analysis/"][href$=".htm"]').forEach(a => {
                const m = a.href.match(/analysis\\/(\\d+)\\.htm/);
                if (m) out.add(m[1]);
              });
              return Array.from(out);
            }
            """
        )
        await browser.close()
    uniq = []
    seen = set()
    for mid in ids:
        if mid not in seen:
            seen.add(mid)
            uniq.append(mid)
    if limit:
        uniq = uniq[:limit]
    logger.info("Discovered %d match ids from live page (limit=%s)", len(uniq), limit)
    return uniq

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--limit", type=int, help="Limit how many IDs from discovery")
    parser.add_argument("--base", default="titan/stats", help="Base stats directory (flat layout)")
    args = parser.parse_args()

    base = Path(args.base)
    full_dir = base / "full"
    missing_dir = base / "missing"
    full_dir.mkdir(parents=True, exist_ok=True)
    missing_dir.mkdir(parents=True, exist_ok=True)

    ids = await discover_match_ids(args.limit)
    ids = await filter_match_ids_only_my_leagues(ids)
    logger.info("Filtered: %d match ids will be scraped.", len(ids))

    missing_rows = []
    cur_time_tag = datetime.now().strftime("%Y%m%d%H%M")
    excel_path = Path(f"missing{cur_time_tag}.xlsx")

    headers = ["match_id", "home_team", "away_team", "game_time", "league", "stat_url", "found_sections", "missing_sections"]

    print(f"Ready to process {len(ids)} matches:\n{ids}")

    for mid in ids:
        print(f"Scraping {mid} ...")
        out = await titan_scrape_stats(mid)
        if out.get("has_stats"):
            (full_dir / f"{mid}.json").write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
            print(f"OK - FULL: {mid}")
        else:
            (missing_dir / f"{mid}.json").write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
            print(f"WARNING - MISSING: {mid} | Sections found: {out['sections_found']}")
            missing_rows.append([
                out["match_id"],
                out.get("home_team", ""),
                out.get("away_team", ""),
                out.get("game_time", ""),
                out.get("league", ""),
                out.get("url"),
                ";".join(out.get("sections_found") or []),
                ";".join(out.get("sections_missing") or [])
            ])

    if missing_rows:
        excel_add_rows(excel_path, missing_rows, headers)
        logger.info(f"Excel written: {excel_path} ({len(missing_rows)} missing games)")
    print(f"Completed scrape. {len(missing_rows)} matches missing stats. See Excel/log as configured.")

if __name__ == "__main__":
    asyncio.run(main())
