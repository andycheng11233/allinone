#!/usr/bin/env python3
"""
Titan pre-scrape: discover match IDs from live.titan007.com, then scrape stats pages.

Outputs (flat layout, no date subfolder):
  titan/stats/full/<match_id>.json               # usable stats
  titan/stats/incomplete/<match_id>.json         # has data but not enough sections/keys (or still retrying)
  titan/stats/completelymissing/<match_id>.json  # no stats after 3 attempts; skip future runs
  titan/html/<match_id>.html                     # if --save-html

Behavior:
- Skip only when attempts >= 3 (whether previously in incomplete/ or completelymissing/).
- Retry all others (new or attempts < 3).
- After scrape: if full -> full/, if empty and attempts >= 3 -> completelymissing/, else -> incomplete/.
"""

import argparse
import asyncio
import json
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from bs4 import BeautifulSoup, Tag
from playwright.async_api import async_playwright

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("titan_pre_scrape")

INDEX_URL = "https://live.titan007.com/indexall_big.aspx"

# ---------------- Titan stats scraper ----------------
async def titan_scrape_stats(match_id: str, save_html: bool = False, page=None) -> Dict[str, Any]:
    url = f"https://zq.titan007.com/analysis/{match_id}.htm"
    out = {
        "match_id": match_id,
        "url": url,
        "scraped_at": datetime.now().isoformat(),
        "sections": {},
        "stats_available": False,
    }

    own_browser = None
    own_page = False
    try:
        if page is None:
            own_page = True
            async with async_playwright() as p:
                own_browser = await p.chromium.launch(headless=True)
                page = await own_browser.new_page()
        await page.goto(url, wait_until="networkidle", timeout=60000)
        await asyncio.sleep(3.0)
        try:
            await page.wait_for_selector("table", timeout=3000)
        except Exception:
            pass
        html = await page.content()
    except Exception as e:
        logger.exception("Fetch error for %s: %s", match_id, e)
        out["error"] = str(e)
        return out
    finally:
        if own_page and own_browser:
            try:
                await own_browser.close()
            except Exception:
                pass

    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text(separator=" ", strip=True)
    if any(bad in text for bad in ["暫無數據", "數據統計中", "資料準備中", "未開始", "尚無相關資料", "No data"]):
        out["error"] = "No stats on page"
        if save_html:
            out["html"] = html
        return out

    sections_to_try = [
        ("league_standings", r"聯賽積分排名"),
        ("head_to_head", r"對賽往績"),
        ("data_comparison", r"數據對比"),
        ("referee_stats", r"裁判統計"),
        ("league_trend", r"聯賽盤路走勢"),
        ("same_trend", r"相同盤路"),
        ("goal_distribution", r"入球數/上下半場入球分布"),
        ("halftime_fulltime", r"半全場"),
        ("goal_count", r"進球數/單雙"),
        ("goal_time", r"進球時間"),
        ("future_matches", r"未來五場"),
        ("pre_match_brief", r"賽前簡報"),
        ("season_stats_comparison", r"本賽季數據統計比較"),
        ("recent_form", r"近期戰績"),
        ("last_match_player_ratings", r"球員上一場出場評分"),
        ("lineup_and_injuries", r"陣容情況"),
        ("goal_count_odd_even", r"進球數/單雙"),  # alias
    ]

    def extract_table_data(tbl: Tag) -> List[Dict[str, str]]:
        rows = tbl.find_all("tr")
        if len(rows) < 2:
            return []
        headers = [re.sub(r"\s+", " ", c.get_text(strip=True)) or f"col_{i}" for i, c in enumerate(rows[0].find_all(["th", "td"]))]
        data = []
        for row in rows[1:]:
            cells = row.find_all(["td", "th"])
            if not cells:
                continue
            row_data = {}
            for i, cell in enumerate(cells[: len(headers)]):
                txt = re.sub(r"\s+", " ", cell.get_text(strip=True))
                if txt:
                    row_data[headers[i]] = txt
            if row_data:
                data.append(row_data)
        return data

    def extract_section_by_regex(sp: BeautifulSoup, regex: str) -> Tuple[Optional[Any], bool]:
        header = sp.find(string=re.compile(regex))
        if not header:
            return None, False
        next_table = header.find_next("table")
        if next_table:
            parsed = extract_table_data(next_table)
            if parsed and len(parsed[0].keys()) >= 1:
                return parsed, True
        for tbl in header.find_all_next("table", limit=3):
            parsed = extract_table_data(tbl)
            if parsed and len(parsed[0].keys()) >= 1:
                return parsed, True
        parent = header.find_parent() if hasattr(header, "find_parent") else getattr(header, "parent", None)
        if parent:
            txt = parent.get_text(separator=" | ", strip=True)
            if txt and len(txt) > 30:
                return [{"text_content": txt}], False
        return None, False

    sections_found = 0
    for key, regex in sections_to_try:
        sec, has_tbl = extract_section_by_regex(soup, regex)
        if sec:
            out["sections"][key] = sec
            if has_tbl:
                sections_found += 1

    formation_header = soup.find(string=re.compile(r"陣容情況"))
    if formation_header:
        parent = formation_header.find_parent() if hasattr(formation_header, "find_parent") else getattr(formation_header, "parent", None)
        if parent:
            out["sections"]["team_formation"] = parent.get_text(separator=" | ", strip=True)
            sections_found += 1

    out["stats_available"] = sections_found > 0
    if save_html:
        out["html"] = html
    return out

# ---------------- Discovery ----------------
async def discover_match_ids(limit: Optional[int]) -> List[str]:
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(INDEX_URL, wait_until="domcontentloaded", timeout=60000)
        await asyncio.sleep(4.0)  # allow JS to populate globals

        ids = await page.evaluate(
            """
            () => {
              const out = new Set();
              const globals = ['A','B','C','arr','arrData','Match','matchList'];
              for (const k of globals) {
                const v = window[k];
                if (Array.isArray(v)) {
                  for (const row of v) {
                    const cand = Array.isArray(row)
                      ? row[0]
                      : (row && (row.matchid || row.MatchID || row.matchId || row.id));
                    if (cand) out.add(String(cand));
                  }
                }
              }
              document.querySelectorAll('a[href*="analysis/"][href$=".htm"]').forEach(a => {
                const m = a.href.match(/analysis\\/(\\d+)\\.htm/);
                if (m) out.add(m[1]);
              });
              return Array.from(out);
            }
            """
        )

        await browser.close()

    seen = set()
    uniq = []
    for mid in ids:
        if mid not in seen:
            seen.add(mid)
            uniq.append(mid)
    if limit:
        uniq = uniq[:limit]
    logger.info("Discovered %d match ids from live page (limit=%s)", len(uniq), limit)
    return uniq

# ---------------- Gate: is_full ----------------
def load_required_keys_from_template(template_path: Optional[str]) -> List[str]:
    if not template_path:
        return []
    try:
        tmpl = json.loads(Path(template_path).read_text(encoding="utf-8"))
        if isinstance(tmpl, dict):
            return list((tmpl.get("sections") or {}).keys())
    except Exception:
        pass
    return []

def _has_content(v: Any) -> bool:
    if isinstance(v, list):
        return len(v) > 0
    if isinstance(v, str):
        return bool(v.strip())
    return bool(v)

def is_full_stats(data: Dict[str, Any], min_sections: int, required_keys: List[str]) -> bool:
    sections = data.get("sections") or {}
    if not sections:
        return False

    non_empty = sum(1 for v in sections.values() if _has_content(v))
    # Require at least min_sections non-empty sections (default 1); never less than 1
    if non_empty < max(1, min_sections):
        return False

    # Enforce required keys only if provided, and require they have content
    for rk in required_keys:
        if rk:
            v = sections.get(rk)
            if v is None or not _has_content(v):
                return False

    return True

# ---------------- Output helpers ----------------
def get_attempts(existing_path: Path) -> int:
    if not existing_path.exists():
        return 0
    try:
        data = json.loads(existing_path.read_text(encoding="utf-8"))
        return int(data.get("attempts", 0))
    except Exception:
        return 0

async def handle_result(
    match_id: str,
    data: Dict[str, Any],
    base: Path,
    save_html: bool,
    min_sections: int,
    required_keys: List[str],
    prev_attempts: int,
) -> None:
    full_dir = base / "full"
    incomplete_dir = base / "incomplete"
    missing_dir = base / "completelymissing"
    html_dir = base.parent / "html"

    full_dir.mkdir(parents=True, exist_ok=True)
    incomplete_dir.mkdir(parents=True, exist_ok=True)
    missing_dir.mkdir(parents=True, exist_ok=True)
    if save_html:
        html_dir.mkdir(parents=True, exist_ok=True)

    attempts = prev_attempts + 1
    data["attempts"] = attempts

    is_full = is_full_stats(data, min_sections=min_sections, required_keys=required_keys)
    is_empty = (not data.get("sections")) and (not data.get("stats_available"))

    # Gate debug log
    sections = data.get("sections") or {}
    non_empty = sum(1 for v in sections.values() if _has_content(v))
    missing_req = [rk for rk in required_keys if rk and (rk not in sections or not _has_content(sections.get(rk)))]
    logger.info(
        "Gate check for %s: sections=%d non_empty=%d min_sections=%d required=%s missing_required=%s is_full=%s",
        match_id, len(sections), non_empty, min_sections, required_keys, missing_req, is_full
    )

    if is_full:
        target_dir = full_dir
        status = "FULL"
    elif is_empty and attempts >= 3:
        target_dir = missing_dir
        status = "COMPLETELY MISSING (stop after 3)"
    else:
        target_dir = incomplete_dir
        status = "INCOMPLETE"

    out_path = target_dir / f"{match_id}.json"
    out_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
    logger.info("%s stats: %s (attempt %d)", status, out_path, attempts)

    if save_html and "html" in data:
        (html_dir / f"{match_id}.html").write_text(data["html"], encoding="utf-8")

# ---------------- Bounded scraping ----------------
async def scrape_all(ids: List[str], save_html: bool, min_sections: int, required_keys: List[str], base: Path, max_concurrent: int = 3):
    sem = asyncio.Semaphore(max_concurrent)
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)

        async def scrape_one(mid: str):
            async with sem:
                page = await browser.new_page()
                try:
                    data = await titan_scrape_stats(mid, save_html=save_html, page=page)
                    prev_attempts = 0
                    for folder in ["incomplete", "completelymissing"]:
                        existing = base / folder / f"{mid}.json"
                        prev_attempts = max(prev_attempts, get_attempts(existing))
                    await handle_result(mid, data, base, save_html, min_sections, required_keys, prev_attempts)
                finally:
                    await page.close()

        await asyncio.gather(*(scrape_one(mid) for mid in ids))
        await browser.close()

# ---------------- Main ----------------
async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--ids", nargs="*", help="Explicit match IDs (skip discovery)")
    parser.add_argument("--file", help="Text file with match IDs (one per line)")
    parser.add_argument("--limit", type=int, help="Limit how many IDs from discovery or provided list")
    parser.add_argument("--save-html", action="store_true", help="Save raw HTML snapshots")
    parser.add_argument("--min-sections", type=int, default=1, help="Minimum non-empty sections to consider stats usable")
    parser.add_argument("--required-keys", nargs="*", default=[], help="Section keys required (e.g., data_comparison head_to_head)")
    parser.add_argument("--template", help="Path to a good sample JSON (e.g., 2784627.json) to derive required keys")
    parser.add_argument("--base", default="titan/stats", help="Base stats directory (flat layout)")
    parser.add_argument("--max-concurrent", type=int, default=3, help="Max pages scraping in parallel")
    args = parser.parse_args()

    ids: List[str] = []
    if args.file:
        ids.extend([line.strip() for line in Path(args.file).read_text(encoding="utf-8").splitlines() if line.strip()])
    if args.ids:
        ids.extend(args.ids)

    if not ids:
        ids = await discover_match_ids(args.limit)
    elif args.limit:
        ids = ids[:args.limit]

    template_keys = load_required_keys_from_template(args.template)
    required_keys = args.required_keys if args.required_keys else template_keys

    base = Path(args.base)
    full_dir = base / "full"

    filtered_ids = []
    for mid in ids:
        # skip if already full
        if (full_dir / f"{mid}.json").exists():
            logger.info("Skip (already full): %s", mid)
            continue
        # check attempts from incomplete and completelymissing
        prev_attempts = 0
        for folder in ["incomplete", "completelymissing"]:
            existing = base / folder / f"{mid}.json"
            prev_attempts = max(prev_attempts, get_attempts(existing))
        if prev_attempts >= 3:
            logger.info("Skip (attempts >= 3): %s", mid)
            continue
        filtered_ids.append(mid)

    if not filtered_ids:
        logger.info("No matches to scrape (all full or max attempts reached).")
        return

    await scrape_all(filtered_ids, args.save_html, args.min_sections, required_keys, base, max_concurrent=args.max_concurrent)

if __name__ == "__main__":
    asyncio.run(main())
